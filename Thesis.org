#+TITLE: Computational trinitarianism and Linear types
#+AUTHOR: Vojtěch Štěpančík
#+OPTIONS: toc:nil ':t

#+latex_header: \usepackage{fontspec}
#+latex_header: \usepackage{prftree}
#+latex_header: \usepackage{apacite}
#+latex_header: \usepackage{framed}

#+begin_export latex
% Binary or
\newcommand{\binor}{\mathbin{|}}

% Introduction rule
\newcommand{\Intro}[1]{#1\mathrm{I}}
% Elimination rule
\newcommand{\Elim}[1]{#1\mathrm{E}}

% Lambda calculus
\newcommand{\stl}{\lambda^{\to}_{\ProdTypeCon}}

% Product type
\newcommand{\ProdTypeCon}{\land}
\newcommand{\ProdType}[2]{#1 \ProdTypeCon #2}
\newcommand{\ProdTypeFst}[1]{fst(#1)}
\newcommand{\ProdTypeSnd}[1]{snd(#1)}

% Tuple
\newcommand{\tuple}[2]{(#1, #2)}
#+end_export

* COMMENT Topic

Computational trinitarianism describes the intimate relationship between logic, category theory and type theory. This relationship identifies propositions of a logic with a type of a corresponding type system, and also establishes a correspondence between a proof of a proposition, a term (program) of a given type, and a generalized element of an object in a category.
A linear type system is a special kind of a substructural type system with important applications in computer science. An advantage of a linear type system resides in its ability to place constraints on the usage of (or access to) variables (resources).
The aim of the bachelor thesis is to describe linear logic as an example of a substructural logic, to construct a linear type system stemming from that logic, and to give their categorical semantics via categories with structure.
The style and presentation of the thesis will be theoretical.

* TODO Introduction

* Logic

Mathematical logic is logic treated by mathematical methods. However, such studies of different kinds of logic often use logical and deductive thinking themselves. To separate the logic observed from the logic used to make the observations, we consider them to be two separate systems. The language of the logic studied is referred to as the *object language*, while the language of the logic used for doing the observing is called the *metalanguage* \cite{Kleene1966}.

These metalanguages are then used to reason about formal composition of proofs \mdash therefore we call them *proof systems*, or *proof calculi*.

The proof system used in this paper stems from Gentzen's natural deduction \cite{Gentzen1935}. Natural deduction builds proofs on *judgements* and *propositions*.

A proposition is a formula of the object language, and a judgement is a knowable fact. For example in traditional logic (that is to say, a /truth-oriented/ logic), one might take "It is raining today" for a proposition $A$, and a judgement is the statement /$A$ is true/, or /$A$ true/ for short.

Another judgment that often arises in various logics is identifying propositions themselves \mdash one can only make judgments about a proposition $A$ if $A$ is a proposition, which is represented by the judgment /$A$ is a proposition/, abbreviated to /$A$ prop/.

We will later see that, without delving into the philosophy of mathematics, the exact nature of propositions and judgements depends on the object language.

The basis for the metalanguage is the *deduction rules*. A deduction rule consists of a collection of judgements, called the *premises*, and a single judgement, called the *conclusion*. To be able to refer to the rule in proofs, it is assigned a semantically significant name. Graphically, it is represented by drawing a horizontal line (the *derivation line*), placing the premises above it, the conclusion below, and writing the name of the rule to the right.

To illustrate, if we wanted to show the rule expressing that given two propositions, $A$ and $B$, and the judgements /$A$ true/ and /$B$ true/, one can obtain the judgement /$A \land B$ true/, we could write it as

$$
\prftree[r]{$\Intro{\land}$}
 {A\;true}
 {A\;prop}
 {B\;true}
 {B\;prop}
 {(A \land B)\;true}
$$


\noindent where the label $\Intro{\land}$ is an abbreviation for "conjunction introduction".

Gentzen used the concept of assumptions to formulate implication. If, given that /$A$ true/, we could sequence the deduction rules in such a way that we get the judgement /$B$ true/, we can abstract this dependency on a hypothetical $A$ into an implication. Gentzen used $[A]$ to denote the *assumption* of the judgement /$A$ true/, and this assumption needs to be later *discharged* by abstracting it into an appropriate implication via a corresponding implication introduction. The formulation of the $\Intro{\to}$ rule can be seen in Figure\nbsp[[fig:localized_hyp]]\nbsp(left). The $\vdots$ stands for a sequence of deduction rules that can derive the judgement /$B$ true/ from the judgement /$A$ true/.

A proof in natural deduction is tree-like, with the judgement to be proven at the root, assumptions at the leaves, and deduction rules between the nodes. It is not a proper tree, because it needs to keep track of which implication introductions discharge which assumptions, so additional structure to manage backreferences is necessary.

In this notation, assumptions are /global/ to the proof. We can change the notation to be able to reason about assumptions locally, allowing us to degenerate the proof structure to a proper tree. We say that a *contextualized judgement*[fn:1] has the form $\Gamma \vdash J$, where \Gamma is a sequence of zero or more assumptions, called the *context*, and $J$ is the judgement. An example of rewriting a proof from Gentzen's notation to the context notation is shown in Figure\nbsp[[fig:localized_hyp]]. Note that the context can also be empty. Assumptions can be added to the context via a comma: $\Gamma, S$ is a new context, which includes all the assumptions from \Gamma, and the assumption $S$. This concatenation is intuitively extended to merging of two contexts, so $\Gamma, \Delta$ is a context that includes all the assumptions from \Gamma, and all the assumptions from \Delta \cite{Pfenning2004}. In this new notation, deduction rules have contextualized judgements for premises and conclusion.

#+begin_figure
#+name: fig:localized_hyp
#+caption: Gentzen's assumption notation (left) and notation for localized assumptions (right)
#+begin_framed
$$
\prftree[r]{$(\Intro{\to})_{\prfref<A>}$}
 {\prfsummary
   {\prfboundedassumption<A>{A}}
   {B}}
 {A \to B}
\hspace{2em}
\prftree[r]{$\Intro{\to}$}
 {A\;prop, A\;true, B\;prop \vdash B\;true}
 {A\;prop, B\;prop \vdash (A \to B)\;true}
$$
#+end_framed
#+end_figure

The behaviour of the context is specified in the metalanguage, using deduction rules. These rules are called *structural rules*, and usually include Weakening, Contraction, and Exchange, which are listed in Figure\nbsp[[fig:structural]]. These three rules encode semantics similar to those of a finite set.

Weakening allows one to add arbitrary assumptions to the context without invalidating the derived judgement. Contraction states that assumptions may be used multiple times. Exchange asserts that the order in which assumptions appear in the context is irrelevant.

A logic which constrains one or more of these structural rules is called *substructural* \cite{Paoli2013}.

#+begin_figure
#+name: fig:structural
#+caption: Structural rules
#+begin_framed
$$
\prftree[r]{Weakening}
 {\Gamma \vdash A\;true}
 {\Gamma, B\;true \vdash A\;true}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, A\;true, A\;true \vdash B\;true}
 {\Gamma, A\;true \vdash B\;true}
$$

$$
\prftree[r]{Exchange}
 {\Gamma, A\;true, B\;true, \Delta \vdash C\;true}
 {\Gamma, B\;true, A\;true, \Delta \vdash C\;true}
$$
#+end_framed
#+end_figure

Just as there can be zero assumptions in a contextualized judgement, there can be zero premises in a deduction rule. Such rules are called *axioms*, and the judgments in their conclusions are always derivable.

A proof in this updated notation is now a proper tree, with a contextualized judgement at the root, contextualized judgements in the inner nodes, axioms at the leaves, and deduction rules connecting the nodes.

When composing existing proofs together, we sometimes produce redundancies. Namely when a rule for introducing a connective is immediately followed by a rule for eliminating it, the proof can be simplified via rewriting rules called *proof-reductions*, specific to the object language.

** Intuitionistic logic

Intuitionistic logic is the logic of constructive mathematics \mdash the only axiom in the system is $A\;true \vdash A\;true$, in other words, any judgement can be made assuming itself. This is in contrast with classical logic, which also axiomatizes the law of excluded middle, $\vdash (A \lor \lnot A)\;true$. The philosophical difference between classical and intuitionistic logic is that classical logic is content with knowing whether a formula is true or whether it is false. After all, those are the only options. Intuitionistic logic, on the other hand, requires a constructive proof \mdash a "recipe", turning the assumptions into the conclusion. Without the law of excluded middle, one cannot derive a proposition to be true based on its double negation alone, which is in accord with the constructive requirements, because showing that an object cannot /not/ exist is not the same as constructing an object. In intuitionistic logic, the judgement /$(A \lor \lnot A)$ true/ can still be made, but it needs to be accompanied with either a proof of /$A$ true/ or /$\lnot A$ true/ \cite{Sorensen2006}.

Since intuitionistic logic is an example of a traditional logic, the basic judgement that can be made about a proposition stays the same, /$A$ true/. Because this is the only judgment we will be using in the proofs[fn:2], we define a shorthand notation, $\Gamma \vdash_T A$, where \Gamma is a list of /propositions/, and $A$ is a proposition, and we take it to mean the contextualized judgment where the context is a list of judgments /$P$ true/ for every proposition $P$ in \Gamma, and where the conclusion is the judgment /$A$ true/ (the index $T$ stands for "truth").

The logic studied in this section is the meet-implicative fragment of propositional intuitionistic logic \mdash that is to say, we only concern ourselves with propositions created using the connectives $\land$ and $\to$. The formulas of this fragment can be described by the following Backus-Naur form:

$$
A, B ::= X \binor (A \to B) \binor (A \land B)
$$

\noindent for X ranging over atomic formulas. The rules of this fragment are given in Figure\nbsp[[fig:intuit_deduct]].

#+begin_figure
#+name: fig:intuit_deduct
#+caption: Deduction rules for the meet-implicative fragment of propositional intuitionistic logic
#+begin_framed
$$
\prftree[r]{Id}
 {A \vdash_T A}
\hspace{2em}
\prftree[r]{Weakening}
 {\Gamma \vdash_T A}
 {\Gamma, B \vdash_T A}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, A, A \vdash_T B}
 {\Gamma, A \vdash_T B}
\hspace{2em}
\prftree[r]{Exchange}
 {\Gamma, A, B, \Delta \vdash_T C}
 {\Gamma, B, A, \Delta \vdash_T C}
$$

$$
\prftree[r]{$\Intro{\land}$}
 {\Gamma \vdash_T A}
 {}
 {\Gamma \vdash_T B}
 {\Gamma \vdash_T A \land B}
$$

$$
\prftree[r]{$\Elim{\land}_1$}
 {\Gamma \vdash_T A \land B}
 {\Gamma \vdash_T A}
\hspace{2em}
\prftree[r]{$\Elim{\land}_2$}
 {\Gamma \vdash_T A \land B}
 {\Gamma \vdash_T B}
$$

$$
\prftree[r]{$\Intro{\to}$}
 {\Gamma, A \vdash_T B}
 {\Gamma \vdash_T A \to B}
\hspace{2em}
\prftree[r]{$\Elim{\to}$}
 {\Gamma \vdash_T A \to B}
 {}
 {\Delta \vdash_T A}
 {\Gamma, \Delta \vdash_T B}
$$
#+end_framed
#+end_figure

The rules consist of the one axiom Id mentioned above, the three structural rules, Weakening, Contraction, and Exchange, and introduction and elimination rules for the two connectives, $\Intro{\land}$, $\Elim{\land}_1$, $\Elim{\land}_2$, $\Intro{\to}$ and $\Elim{\to}$.

/Conjunction introduction/, labeled $\Intro{\land}$ in the deduction rules, states that given a proof of /$A$ true/ and a proof of /$B$ true/, the two proofs combined give a proof of /$(A \land B$) true/. The respective elimination rules allow one to extract one of the proofs of /$A$ true/ or /$B$ true/ from /$(A \land B)$ true/, even after they were combined.

When formulating the proof reduction rule for a particular connective, one needs to look at a generic example of a reducible proof. For sequencing a conjunction introduction and a conjunction elimination, we need to represent generic proofs of the premises, then apply the two rules in succession, and finally justify an alternative path to reach the conclusion. We can represent the generic proofs with the symbol $\vdots$, much like how Gentzen formulated assumptions. For the conjunction reduction, the generic schema would look like the following tree, with the subproofs labeled $s$ and $t$.

$$
\prftree[r]{$\Elim{\land}_1$}
 {\prftree[r]{$\Intro{\land}$}
   {\prfsummary[s]{\Gamma \vdash_T A}}
   {}
   {\prfsummary[t]{\Gamma \vdash_T B}}
   {\Gamma \vdash_T A \land B}}
 {\Gamma \vdash_T A}
$$

\noindent It is easy to see that the conclusion $\Gamma \vdash A\;true$ could have been reached earlier with the $s$ subproof. The full rule is shown in Figure\nbsp[[fig:intuit_conj_red]]. The rule for the other elimination rule is not shown, as it is trivially symmetrical.

#+begin_figure
#+name: fig:intuit_conj_red
#+caption: Conjunction proof reduction
#+begin_framed
$$
\prftree[r]{$\Elim{\land}_1$}
 {\prftree[r]{$\Intro{\land}$}
   {\prfsummary[s]{\Gamma \vdash_T A}}
   {}
   {\prfsummary[t]{\Gamma \vdash_T B}}
   {\Gamma \vdash_T A \land B}}
 {\Gamma \vdash_T A}
\hspace{1em}\Rightarrow\hspace{1em}
\prfsummary[s]{\Gamma \vdash_T A}
$$
#+end_framed
#+end_figure

/Implication introduction/, labeled $\Intro{\to}$, once again builds on abstracting away an assumption. If a judgement can be made under an assumption, then the proof tree can be seen as a way of turning a proof of /$A$ true/ into a proof of /$B$ true/. The implication elimination is then a method for providing such a proof of $A$. The respective proof reduction is shown in Figure\nbsp[[fig:intuit_impl_red]].

#+begin_figure
#+name: fig:intuit_impl_red
#+caption: Implication proof reduction
#+begin_framed
$$
\prftree[r]{$\Elim{\to}$}
 {\prftree[r]{$\Intro{\to}$}
   {\prftree[r,double]{}
     {\prfsummary[s]{A \vdash_T A \cdots}
       {\Gamma, A \cdots \vdash_T B}}
     {\Gamma, A \vdash_T B}}
   {\Gamma \vdash_T (A \to B)}}
 {\prfsummary[t]{\Delta \vdash_T A}}
 {\Gamma, \Delta \vdash_T B}
\hspace{1em}\Rightarrow\hspace{1em}
\prftree[r,double]{}
 {\prfStackPremises
   {\prfsummary[t $\cdots$]{\Delta \vdash_T A \cdots}}
   {\prfsummary[s]{\Gamma, \Delta \cdots \vdash_T B}}}
 {\Gamma, \Delta \vdash_T B}
$$
#+end_framed
#+end_figure

** TODO Linear logic

#+begin_figure
#+name: fig:intuit_duplic
#+caption: Duplication and discard of truth
#+begin_framed
$$
\prftree[r]{Contr}
 {\prftree[r]{$\Intro{\land}$}
   {\prftree[r]{Id}
     {A \vdash_T A}}
   {\prftree[r]{Id}
     {A \vdash_T A}}
   {A, A \vdash_T A \land A}}
 {A \vdash_T A \land A}
\hspace{2em}
\prftree[r]{$\Intro{\to}$}
 {\prftree[r]{Weak}
   {\prftree[r]{Id}
     {A \vdash_T B}}
   {A, B \vdash_T \to A}}
 {A \vdash_T B \to A}
$$
#+end_framed
#+end_figure

As can be seen from Figure\nbsp[[fig:intuit_duplic]], intuitionistic logic has no problem with "duplicating" propositions \mdash it considers truth and proofs to be "free". Linear logic attempts to formalize a system where such mathematical objects are /not/ free, and resources that can be freely duplicated or discarded have to be annotated. This approach was chosen so that intuitionistic logic can be fully embedded in linear logic, therefore adding expressivity instead of limiting it, which would be the case if Contraction and Weakening were simply not allowed.

To this extent, the intuition behind several connectives, as well as their nomenclature, must be adjusted.

The intuitionistic implication $A \to B$ becomes linear $A \multimap B$, and it carries the idea of "consuming" A to "produce" B.

* TODO Type theory

Type theory is the study of formal systems in which terms have an associated label called /type/, and rules for constructing the terms include the description of their behavior on the types. For more information on the subject, see \cite{Thompson1991} and \cite{PerLof1980}.

More precisely, in constructive mathematics, a mathematical object is created by construction, and the type of an object is the type of construction used to create it \cite{Bauer2018}.

One such type system is the simply typed \lambda-calculus, or STLC, which extends the untyped \lambda-calculus by introducing a set of /base types/, and inductively generates all its types with the $\to$ binary type operator, where the type $A \to B$ is the type of functions from type $A$ to type $B$. A term $t$ of type $A$ is expressed as $t: A$.

The STLC recognizes three forms for its terms, very much like the untyped \lambda-calculus. These are /variables/, of the form $x: A$, where $x$ is an atom and $A$ is a type, then /abstractions/, which represent functions, and have the form $\lambda x.t: A \to B$, where $x: A$, $t: B$, and $x$ is a free variable in $t$, becoming bound by the abstraction. Finally, abstractions can be used in an /application/, which, given the terms $f: A \to B$ and $t: A$, yields the term $f(t): B$. Application forms can be further simplified by performing /\(\beta\)-reduction/, defined using term substitution as $(\lambda x.t)(s) \to t[s/x]$, where free occurrences of $x$ in $t$ are rewritten to $s$. Performing a reduction is synonymous with /evaluating/ a program.

We define an extension of the simply typed \lambda-calculus by introducing the binary product type operator $\ProdTypeCon$, producing types of the form $\ProdType{A}{B}$, which represent tuples of one object of type $A$ and one object of type $B$. We call this extension the \(\stl\)-calculus, and the construction rules are listed in Figure\nbsp[[fig:type_deduction]].

#+begin_figure
#+name: fig:type_deduction
#+caption: Deduction rules for the \(\stl\)-calculus
#+begin_framed
$$
\prftree[r]{Id}
{x: A \vdash x: A}
$$

$$
\prftree[r]{Weakening}
{\Gamma \vdash t: A}
{\Gamma, x: B \vdash t: A}
\hspace{2em}
\prftree[r]{Contraction}
{\Gamma, x: A, y: A \vdash t: B}
{\Gamma, z: A \vdash t[z/x][z/y]: B}
$$

$$
\prftree[r]{Exchange}
{\Gamma, x: A, y: B, \Delta \vdash t: C}
{\Gamma, y: B, x: A, \Delta \vdash t: C}
$$

$$
\prftree[r]{$\Intro{\land}$}
{\Gamma \vdash x: A}
{}
{\Delta \vdash y: B}
{\Gamma, \Delta \vdash \tuple{x}{y}: \ProdType{A}{B}}
$$

$$
\prftree[r]{$\Elim{\ProdTypeCon}_1$}
{\Gamma \vdash t: \ProdType{A}{B}}
{\Gamma \vdash \ProdTypeFst{t}: A}
\hspace{2em}
\prftree[r]{$\Elim{\ProdTypeCon}_2$}
{\Gamma \vdash t: \ProdType{A}{B}}
{\Gamma \vdash \ProdTypeSnd{t}: B}
$$

$$
\prftree[r]{$\Intro{\to}$}
{\Gamma, x: A \vdash t: B}
{\Gamma \vdash \lambda x.t: A \to B}
\hspace{2em}
\prftree[r]{$\Elim{\to}$}
{\Gamma \vdash f: A \to B}
{\Delta \vdash t: A}
{\Gamma, \Delta \vdash f(t): B}
$$
#+end_framed
#+end_figure

The new forms introduced are /tuples/, written as $\tuple{x}{y}: \ProdType{A}{B}$, which represent a pair of terms, and left and right /projections/, written as $\ProdTypeFst{t}: A$ and $\ProdTypeSnd{t}: B$, respectively, assuming a term $t: \ProdType{A}{B}$. This new syntax allows for more redundant forms of terms, which can be simplified using /\(\pi\)-reduction/ via the evaluation steps $\ProdTypeFst{\tuple{x}{y}} \to x$ and $\ProdTypeSnd{\tuple{x}{y}} \to y$.

The language is once again described with deduction rules, with zero or more premises above and one conclusion below the line. The context in a judgment now stands for a collection of typed variables, and contains the variables that are free in the term on the right side of the turnstile. In this way, the $\Intro{\to}$ rule can be intuitively interpreted by taking a variable $x: A$, and instead of treating it as free, we remove it from the context and bind it with an abstraction.
#+begin_export latex
\bibliography{ComputationalTrinitarianism}
\bibliographystyle{apacite}
#+end_export

* Footnotes

[fn:2] The judgment /$A$ prop/ is used more frequently in the field of dependent type theories, which are out of scope for this thesis. The valid formulas of the relevant fragment can be described more easily with a simple grammar.

[fn:1] The notation is borrowed from Gentzen's other proof calculus, the sequent calculus. To prevent confusion of the two systems, we prefer the term /contextualized judgment/ to Gentzen's /sequent/.
