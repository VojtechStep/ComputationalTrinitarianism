#+OPTIONS: toc:nil ':t

#+latex_class: book
#+latex_class_options: [11pt,twoside,a4paper]
#+latex_header: \usepackage{fontspec}
#+latex_header: \usepackage{prftree}
#+latex_header: \usepackage{stmaryrd}
#+latex_header: \usepackage{apacite}
#+latex_header: \usepackage{fancyhdr}
#+latex_header: \usepackage[english]{babel}
#+latex_header: \usepackage{./thesis_template/k336_thesis_macros}

# Binary or
#+latex_header: \newcommand{\binor}{\mathbin{|}}

# Introduction rule
#+latex_header: \newcommand{\Intro}[1]{#1\mathrm{I}}
# Elimination rule
#+latex_header: \newcommand{\Elim}[1]{#1\mathrm{E}}

# Proof reduction
#+latex_header: \newcommand{\prfRed}[1][1em]{\hspace{#1}\Rightarrow\hspace{#1}}
#+latex_header: \newcommand{\prfEq}[1][1em]{\hspace{#1}=\hspace{#1}}

# Linear assumption
#+latex_header: \newcommand{\Lin}[1]{\langle#1\rangle}
# Intuitionistic assumption
#+latex_header: \newcommand{\Int}[1]{[#1]}

# Tensor
#+latex_header: \newcommand{\tens}{\mathbin{\otimes}}
# With
#+latex_header: \newcommand{\with}{\mathbin{\&}}
# Lollipop
#+latex_header: \newcommand{\loli}{\multimap}
# Linear translation
#+latex_header: \newcommand{\LinTrans}[1]{\left\llbracket #1 \right\rrbracket_L}

# Lambda calculus
#+latex_header: \newcommand{\stl}{\lambda^{\to}_{\ProdTypeCon}}

# Product type
#+latex_header: \newcommand{\ProdTypeCon}{\land}
#+latex_header: \newcommand{\ProdType}[2]{#1 \ProdTypeCon #2}
#+latex_header: \newcommand{\ProdTypeFst}[1]{fst(#1)}
#+latex_header: \newcommand{\ProdTypeSnd}[1]{snd(#1)}

# Tuple
#+latex_header: \newcommand{\tuple}[2]{(#1, #2)}

# Tensor Tuple
#+latex_header: \newcommand{\tenstup}[2]{\langle #1, #2 \rangle}
# With Tuple
#+latex_header: \newcommand{\withtup}[2]{\langle \langle #1, #2 \rangle \rangle}
#+latex_header: \newcommand{\withfst}[1]{fst \langle #1 \rangle}
#+latex_header: \newcommand{\withsnd}[1]{snd \langle #1 \rangle}
#+latex_header: \newcommand{\bang}{{!}}

#+latex_header: \newcommand\WorkTitle{Computational trinitarianism and Linear types}
#+latex_header: \newcommand\FirstandFamilyName{Vojtěch Štěpančík}
#+latex_header: \newcommand\Supervisor{Ing. Matěj Dostál, Ph.D.}
#+latex_header: \newcommand\TypeOfWork{Bachelor's Thesis}
#+latex_header: \newcommand\StudProgram{Otevřená informatika, Bakalářský}
#+latex_header: \newcommand\StudBranch{Software}

#+begin_export latex
\graphicspath{{thesis_template/}}
\selectlanguage{english}
\translate
\coverpagestarts
\acknowledgements
...
\declaration{In Prague on ... ... 2021}
\abstractpage
\vglue60mm
\noindent{\Huge \textbf{Abstrakt}}

\tableofcontents
\listoffigures
\mainbodystarts
#+end_export

* COMMENT Topic

Computational trinitarianism describes the intimate relationship between logic, category theory and type theory. This relationship identifies propositions of a logic with a type of a corresponding type system, and also establishes a correspondence between a proof of a proposition, a term (program) of a given type, and a generalized element of an object in a category.
A linear type system is a special kind of a substructural type system with important applications in computer science. An advantage of a linear type system resides in its ability to place constraints on the usage of (or access to) variables (resources).
The aim of the bachelor thesis is to describe linear logic as an example of a substructural logic, to construct a linear type system stemming from that logic, and to give their categorical semantics via categories with structure.
The style and presentation of the thesis will be theoretical.

* TODO Introduction

* Logic

Mathematical logic is logic treated by mathematical methods. However, such studies of different kinds of logic often use logical and deductive thinking themselves. To separate the logic observed from the logic used to make the observations, we consider them to be two separate systems. The language of the logic studied is referred to as the *object language*, while the language of the logic used for doing the observing is called the *metalanguage* \cite{Kleene1966}.

These metalanguages are then used to reason about formal composition of proofs \mdash therefore we call them *proof systems*, or *proof calculi*.

The proof system used in this paper stems from Gentzen's natural deduction \cite{Gentzen1935}. Natural deduction builds proofs on *judgements* and *propositions*.

A proposition is a formula of the object language, and a judgement is a knowable fact. For example in traditional logic (that is to say, a /truth-oriented/ logic), one might take "It is raining today" for a proposition $A$, and a judgement is the statement /$A$ is true/, or /$A$ true/ for short.

Another judgment that often arises in various logics is identifying propositions themselves \mdash one can only make judgments about a proposition $A$ if $A$ is a proposition, which is represented by the judgment /$A$ is a proposition/, abbreviated to /$A$ prop/.

We will later see that, without delving into the philosophy of mathematics, the exact nature of propositions and judgements depends on the object language.

The basis for the metalanguage is the *deduction rules*. A deduction rule consists of a collection of judgements, called the *premises*, and a single judgement, called the *conclusion*. To be able to refer to the rule in proofs, it is assigned a semantically significant name. Graphically, it is represented by drawing a horizontal line (the *derivation line*), placing the premises above it, the conclusion below, and writing the name of the rule to the right.

To illustrate, if we wanted to show the rule expressing that given two propositions, $A$ and $B$, and the judgements /$A$ true/ and /$B$ true/, one can obtain the judgement /$A \land B$ true/, we could write it as

$$
\prftree[r]{$\Intro{\land}$}
 {A\;true}
 {A\;prop}
 {B\;true}
 {B\;prop}
 {(A \land B)\;true}
$$
where the label $\Intro{\land}$ is an abbreviation for "conjunction introduction".

Gentzen used the concept of assumptions to formulate the rules for implication. If, given that /$A$ true/, we could sequence the deduction rules in such a way that we get the judgement /$B$ true/, we can abstract this dependency on a hypothetical $A$ into an implication. Gentzen used $[A]$ to denote the *assumption* of the judgement /$A$ true/, and this assumption needs to be later *discharged* by abstracting it into an appropriate implication via a corresponding implication introduction. The formulation of the $\Intro{\to}$ rule can be seen in Figure\nbsp[[fig:localized_hyp]]\nbsp(left). The symbol $\vdots$ stands for a sequence of deduction rules that can derive the judgement /$B$ true/ from the judgement /$A$ true/.

A proof in natural deduction is tree-like, with the judgement to be proven at the root, assumptions at the leaves, and deduction rules between the nodes. It is not a proper tree, because it needs to keep track of which implication introductions discharge which assumptions, so additional structure to manage backreferences is necessary.

In this notation, assumptions are /global/ to the proof. We can change the notation to be able to reason about assumptions locally, allowing us to degenerate the proof structure to a proper tree. We say that a *contextualized judgement*[fn:1] has the form $\Gamma \vdash J$, where \Gamma is a sequence of zero or more assumptions, called the *context*, and $J$ is the judgement. An example of rewriting a proof from Gentzen's notation to the context notation is shown in Figure\nbsp[[fig:localized_hyp]]. Note that the context can also be empty. Assumptions can be added to the context via a comma: $\Gamma, S$ is a new context, which includes all the assumptions from \Gamma, and the assumption $S$. This concatenation is intuitively extended to merging of two contexts, so $\Gamma, \Delta$ is a context that includes all the assumptions from \Gamma, and all the assumptions from \Delta \cite{Pfenning2004}. In this new notation, deduction rules have contextualized judgements for premises and conclusion.

#+name: fig:localized_hyp
#+caption: Gentzen's assumption notation (left) and notation for localized assumptions (right)
#+begin_figure
$$
\prftree[r]{$(\Intro{\to})_{\prfref<A>}$}
 {\prfsummary
   {\prfboundedassumption<A>{A}}
   {B}}
 {A \to B}
\hspace{2em}
\prftree[r]{$\Intro{\to}$}
 {A\;prop, A\;true, B\;prop \vdash B\;true}
 {A\;prop, B\;prop \vdash (A \to B)\;true}
$$
#+end_figure

The behaviour of the context is specified in the metalanguage, using deduction rules. These rules are called *structural rules*, and usually include Weakening, Contraction, and Exchange, which are listed in Figure\nbsp[[fig:structural]]. These three rules encode semantics similar to those of a finite set.

Weakening allows one to add arbitrary assumptions to the context without invalidating the derived judgement. Contraction states that assumptions may be used multiple times. Exchange asserts that the order in which assumptions appear in the context is irrelevant.

A logic which constrains one or more of these structural rules is called *substructural* \cite{Paoli2013}.

#+name: fig:structural
#+caption: Structural rules
#+begin_figure
$$
\prftree[r]{Weakening}
 {\Gamma \vdash A\;true}
 {\Gamma, B\;true \vdash A\;true}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, A\;true, A\;true \vdash B\;true}
 {\Gamma, A\;true \vdash B\;true}
$$

$$
\prftree[r]{Exchange}
 {\Gamma, A\;true, B\;true, \Delta \vdash C\;true}
 {\Gamma, B\;true, A\;true, \Delta \vdash C\;true}
$$
#+end_figure

Apart from the structural rules, the logic also specifies *logical rules*. These describe how the logical connectives participate in derivations. Conventionally, they come in pairs of introduction and elimination, the former defining how a proposition containing the connective is created, and the latter defining how such a proposition is "used" and split apart.

Just as there can be zero assumptions in a contextualized judgement, there can be zero premises in a deduction rule. Such rules are called *axioms*, and the judgments in their conclusions are always derivable.

A proof in this updated notation is now a proper tree, with a contextualized judgement at the root, contextualized judgements in the inner nodes, axioms at the leaves, and deduction rules connecting the nodes.

When composing deductions, we sometimes produce redundancies. Namely when a rule for introducing a connective is immediately followed by a rule for eliminating it, the proof can be simplified via rewriting rules called *proof-reductions*. These rules must preserve the validity of the proof, meaning that the proof after a reduction must still consist only of derivations specified for the logic. This condition is called /local soundness/ \cite{Pfenning2004}, and we will revisit it when talking about linear logic.

** Intuitionistic logic

Intuitionistic logic is the logic of constructive mathematics \mdash the only axiom in the system is $A\;true \vdash A\;true$, in other words, any judgement can be made assuming itself. This is in contrast with classical logic, which also axiomatizes the law of excluded middle, $\vdash (A \lor \lnot A)\;true$. The philosophical difference between classical and intuitionistic logic is that classical logic is content with knowing whether a proposition is true or whether it is false. After all, those are the only options. Intuitionistic logic, on the other hand, requires a constructive proof \mdash a "recipe", turning the assumptions into the conclusion. The law of excluded middle allows for proofs where one judges a proposition to be true, just because it cannot be false. This goes against the intuitionistic line of reasoning, because merely showing that something has to exists doesn't provide the mathematician with a way to construct it. In intuitionistic logic, the judgement /$(A \lor \lnot A)$ true/ can still be made, but it needs to be accompanied with either a proof of /$A$ true/ or /$\lnot A$ true/ \cite{Sorensen2006}.

Since intuitionistic logic is an example of a traditional logic, the basic judgement that can be made about a proposition stays the same, /$A$ true/. Because this is the only judgment we will be using in the proofs[fn:2], we define a shorthand notation, $\Gamma \vdash_T A$, where \Gamma is a list of /propositions/, and $A$ is a proposition, and we take it to mean the contextualized judgment where the context is a list of judgments /$P$ true/ for every proposition $P$ in \Gamma, and where the conclusion is the judgment /$A$ true/ (the index $T$ stands for "truth"). For example, the formula $A, B \vdash_T C$ is short for $A\;true, B\;true \vdash C\;true$. This notation will be used exclusively in the diagrams to prevent them from spreading too wide, and we will use the full form in the body of the thesis.

The logic studied in this section is the meet-implicative fragment of propositional intuitionistic logic \mdash that is to say, we only concern ourselves with propositions created using the connectives $\land$ and $\to$. The propositions of this fragment can be described by the following Backus-Naur form:
$$
A, B ::= X \binor (A \to B) \binor (A \land B)
$$
for X ranging over atomic propositions. The rules of this fragment are given in Figure\nbsp[[fig:intuit_deduct]].

#+name: fig:intuit_deduct
#+caption: Deduction rules for the meet-implicative fragment of propositional intuitionistic logic
#+begin_figure
$$
\prftree[r]{Id}
 {A \vdash_T A}
\hspace{2em}
\prftree[r]{Weakening}
 {\Gamma \vdash_T A}
 {\Gamma, B \vdash_T A}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, A, A \vdash_T B}
 {\Gamma, A \vdash_T B}
\hspace{2em}
\prftree[r]{Exchange}
 {\Gamma, A, B, \Delta \vdash_T C}
 {\Gamma, B, A, \Delta \vdash_T C}
$$

$$
\prftree[r]{$\Intro{\land}$}
 {\Gamma \vdash_T A}
 {}
 {\Gamma \vdash_T B}
 {\Gamma \vdash_T A \land B}
$$

$$
\prftree[r]{$\Elim{\land}_1$}
 {\Gamma \vdash_T A \land B}
 {\Gamma \vdash_T A}
\hspace{2em}
\prftree[r]{$\Elim{\land}_2$}
 {\Gamma \vdash_T A \land B}
 {\Gamma \vdash_T B}
$$

$$
\prftree[r]{$\Intro{\to}$}
 {\Gamma, A \vdash_T B}
 {\Gamma \vdash_T A \to B}
\hspace{2em}
\prftree[r]{$\Elim{\to}$}
 {\Gamma \vdash_T A \to B}
 {}
 {\Delta \vdash_T A}
 {\Gamma, \Delta \vdash_T B}
$$
#+end_figure

The rules consist of the one axiom Id mentioned above, the three structural rules, Weakening, Contraction, and Exchange, and introduction and elimination rules for the two connectives, $\Intro{\land}$, $\Elim{\land}_1$, $\Elim{\land}_2$, $\Intro{\to}$ and $\Elim{\to}$.

/Conjunction introduction/, labeled $\Intro{\land}$ in the deduction rules, states that given a proof of /$A$ true/ and a proof of /$B$ true/, the two proofs combined give a proof of /$(A \land B$) true/. The respective elimination rules allow one to extract one of the proofs of /$A$ true/ or /$B$ true/ from /$(A \land B)$ true/, even after they were combined.

When formulating the proof reduction rule for a particular connective, one needs to look at a generic example of a reducible proof. For sequencing a conjunction introduction and a conjunction elimination, we need to represent generic proofs of the premises, then apply the two rules in succession, and finally justify an alternative path to reach the conclusion. We can represent the generic proofs with the symbol $\vdots$, much like how Gentzen formulated assumptions. For the conjunction reduction, the generic schema would look like the following tree, with the subproofs labeled $s$ and $t$.
$$
\prftree[r]{$\Elim{\land}_1$}
 {\prftree[r]{$\Intro{\land}$}
   {\prfsummary[s]{\Gamma \vdash_T A}}
   {}
   {\prfsummary[t]{\Gamma \vdash_T B}}
   {\Gamma \vdash_T A \land B}}
 {\Gamma \vdash_T A}
$$

It is easy to see that the conclusion $\Gamma \vdash A\;true$ could have been reached earlier with the $s$ subproof. The full rule is shown in Figure\nbsp[[fig:intuit_conj_red]]. The rule for the other elimination rule is not shown, as it is trivially symmetrical.

#+name: fig:intuit_conj_red
#+caption: Conjunction proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\land}_1$}
 {\prftree[r]{$\Intro{\land}$}
   {\prfsummary[s]{\Gamma \vdash_T A}}
   {}
   {\prfsummary[t]{\Gamma \vdash_T B}}
   {\Gamma \vdash_T A \land B}}
 {\Gamma \vdash_T A}}
\prfRed
\vcenter{\prfsummary[s]{\Gamma \vdash_T A}}
$$
#+end_figure

/Implication introduction/, labeled $\Intro{\to}$, once again builds on abstracting away an assumption. If a judgement /$B$ true/ can be made under an assumption /$A$ true/, then the proof tree can be seen as a way of turning a proof of /$A$ true/ (or multiple proofs of /$A$ true/) into a proof of /$B$ true/. The implication elimination is then a method for providing such a proof of $A$.

The proof reduction rule must take into account that the judgment /\(A\)\nbsp{}true/ might have been assumed zero or multiple times in the proof of /$B$ true/, and the context later modified with contractions or weakenings to reach the contextualized judgment $\Gamma, A\;true \vdash B\;true$. Every assumption of /$A$ true/ that is used in the proof must have been introduced by the identity rule, and the ones that aren't used were introduced by weakening. As shown in \cite{Wadler1993}, applications of structural and logic rules commute, so for every proof where contraction and weakening are used, there is an equivalent proof with all the contractions and weakenings pushed to the root of the proof tree. In other words, for every proof of $\Gamma, J_1 \vdash J_2$, where $J_1$ and $J_2$ stand for arbitrary judgments, there is an equivalent proof which consists of a contraction- and weakening-less subproof of $\Gamma, J_1 \cdots \vdash J_2$, followed by applications of contraction and weakening to accommodate the context, where the ellipsis indicate zero of more assumptions of $J_1$. The final applications of contraction and weakening are represented by a doubled derivation line, to indicate that it's multiple steps shown as one.

The role of the proof reduction is then to take the proof of $\Delta \vdash A\;true$, and replace with it the instances of $A\;true \vdash A\;true$ in the proof of $\Gamma, A\;true \vdash B\;true$. The full proof reduction rule is shown in Figure\nbsp[[fig:intuit_impl_red]].

#+name: fig:intuit_impl_red
#+caption: Implication proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\to}$}
 {\prftree[r]{$\Intro{\to}$}
   {\prftree[r,double]{}
     {\prfsummary[s]
       {\prftree[r]{Id}
         {A \vdash_T A} \cdots}
       {\Gamma, A \cdots \vdash_T B}}
     {\Gamma, A \vdash_T B}}
   {\Gamma \vdash_T (A \to B)}}
 {\prfsummary[t]{\Delta \vdash_T A}}
 {\Gamma, \Delta \vdash_T B}}
\prfRed
\vcenter{\prftree[double]
 {\prfStackPremises
   {\prfsummary[t $\cdots$]{\Delta \vdash_T A} \cdots}
   {\prfsummary[s]{\Gamma, \Delta \cdots \vdash_T B}}}
 {\Gamma, \Delta \vdash_T B}}
$$
#+end_figure

** Linear logic

In contrast to intuitionistic logic, linear logic considers propositions to be a form of resource - they should not be subject to duplication or discard. When looking at intuitionistic proofs, such as the ones listed in Figure [[fig:intuit_duplic]], we can see that intuitionistic logic has no problem with duplicating propositions (from a single $A$ one might obtain multiple \(A\)'s) or discarding propositions (the $B$ is unnecessary in the proof of $A$, so it is thrown away).

#+name: fig:intuit_duplic
#+caption: Duplication and discard of truth
#+begin_figure
$$
\prftree[r]{$\Intro{\to}$}
 {\prftree[r]{Contr}
   {\prftree[r]{$\Intro{\land}$}
     {\prftree[r]{Id}
       {A \vdash_T A}}
     {\prftree[r]{Id}
       {A \vdash_T A}}
     {A, A \vdash_T A \land A}}
   {A \vdash_T A \land A}}
 {\vdash_T A \to (A \land A)}
\hspace{2em}
\prftree[r]{$\Intro{\to}$}
 {\prftree[r]{$\Intro{\to}$}
  {\prftree[r]{Weak}
    {\prftree[r]{Id}
      {A \vdash_T B}}
    {A, B \vdash_T \to A}}
  {A \vdash_T B \to A}}
 {\vdash_T A \to (B \to A)}
$$
#+end_figure

In intuitionistic logic, we judged a proposition to be true, and the judgment had the form /$A$ true/. In linear logic, we focus on /availability/. We can judge a proposition $A$ to be available, written /$A$ avail/, if there is a proof that "consumes" some assumptions, "producing" the proposition $A$. The semantics of consumption are embedded in the deduction rules, explained below.

One simple way to prevent "invalid" usage of resources is to remove the contraction and weakening rules altogether. However, this approach severely limits the expressivity of the language. We might still want to model "free" resources, meaning resources that can be used any number of times, even zero, but conveying this information would not be possible in such a system. Instead, we introduce an annotation for unbound resources, and limit contraction and weakening so that they can only be used on these "intuitionistic" resources. This alternative gives us strictly greater expressivity than intuitionistic logic, as we will see that every intuitionistic proof can be translated to a linear proof.

The introduction of unbound resources necessitates differentiating between two kinds of assumptions in contextualized judgments \mdash a /linear/ assumption of the judgment /$A$ avail/ is written $\Lin{A\;avail}$, and indicates that the conclusion uses the fact that $A$ is available /exactly once/. An /intuitionistic/ assumption of the judgment /$A$ avail/, written $\Int{A\;avail}$, makes no guarantees about its usage in the conclusion \mdash it may be used zero, one, or even more times. It is important to emphasize that these glyphs are not a part of the object language \mdash neither $\Lin{A}$ nor $\Int{A}$ are valid propositions, and the bracket notation can only appear on the left side of a turnstile.

Contraction and weakening are now limited to only intuitionistic assumptions, meaning that judgments can be linearly assumed multiple times. These new rules lead to a general context $\Gamma$ behaving like a multiset. Every intuitionistic judgment can be made to have a multiplicity of one (using the new contraction and weakening), and multiplicity of linear assumptions is given by their usage in the conclusion.

Similarly to the intuitionistic case, a shorthand notation for contextualized judgments is used \mdash writing $\Gamma \vdash_R A$, the context \Gamma is a list of /propositions/ in square or angle brackets, such as $\Lin{B}$ or $\Int{C \loli D}$, and $A$ is a proposition. This is shorthand for a contextualized judgment whose context is a list containing one occurrence of the judgment $\Lin{B\;avail}$ for every proposition $B$ in angle brackets in \Gamma, and one occurrence of the judgment $\Int{C\;avail}$ for every proposition $C$ in square brackets in \Gamma. The conclusion of this contextualized judgment is the judgment $A\;avail$, where $A$ is the proposition on the right of the turnstile in the shorthand.

A general context \Gamma can contain assumptions of both kinds, linear and intuitionistic, but an /intuitionistic context/, denoted by $\Int{\Gamma}$, is a context that only contains intuitionistic assumptions, if any.

The focus of this chapter is a fragment of propositional intuitionistic linear logic. It bears similarity to the intuitionistic logic described in the last chapter, specifically it provides tools for representing implication and conjunction, in addition to the linear-logic-specific exponentiation.

The new implication connective is historically called "lollipop", and it's written $A \loli B$. The proposition is read "produce $B$ consuming $A$".

Interestingly, there are two conjunction connectives \mdash the "tensor", written $A \tens B$, and the "with", written $A \with B$. The tensor represents a conjunction "containing" /both/ resources $A$ and $B$, while the "with" lists two resources that are both available, but not at the same time \mdash the recipient of such a resource needs to choose either $A$ or $B$.

The last connective is a new concept entirely. The exponential operator $!A$, pronounced "of course", allows one to represent an infinite amount of a resource. We will see how this connective differs from the intuitionistic assumption $\Int{A\;avail}$ and why they are both necessary once we take a look at the proof reduction rules.

The propositions of this logic can also be described by the simple grammar
$$
A, B ::= X \binor (A \loli B) \binor (A \tens B) \binor (A \with B) \; \binor \; !A
$$
for X ranging over atomic propositions. The deduction rules are listed in Figure [[fig:linear_deduct]].

#+name: fig:linear_deduct
#+caption: Deduction rules for the fragment of intuitionistic linear logic
#+begin_figure
$$
\prftree[r]{$\Lin{\text{Id}}$}
 {\Lin{A} \vdash_R A}
\hspace{2em}
\prftree[r]{$\Int{\text{Id}}$}
 {\Int{A} \vdash_R A}
$$

$$
\prftree[r]{Exchange}
 {\Gamma, S, T, \Delta \vdash_R A}
 {\Gamma, T, S, \Delta \vdash_R A}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, \Int{A}, \Int{A} \vdash_R B}
 {\Gamma, \Int{A}, \vdash_R B}
\hspace{2em}
\prftree[r]{Weakening}
 {\Gamma \vdash_R B}
 {\Gamma, \Int{A} \vdash_R B}
$$

$$
\prftree[r]{$\Intro{\loli}$}
 {\Gamma, \Lin{A} \vdash_R B}
 {\Gamma \vdash_R (A \loli B)}
\hspace{2em}
\prftree[r]{$\Elim{\loli}$}
 {\Gamma \vdash_R (A \loli B)}
 {}
 {\Delta \vdash_R A}
 {\Gamma, \Delta \vdash_R B}
$$

$$
\prftree[r]{$\Intro{\with}$}
 {\Gamma \vdash_R A}
 {}
 {\Gamma \vdash_R B}
 {\Gamma \vdash_R A \with B}
$$

$$
\prftree[r]{$\Elim{\with}_1$}
 {\Gamma \vdash_R A \with B}
 {\Gamma \vdash_R A}
\hspace{2em}
\prftree[r]{$\Elim{\with}_2$}
 {\Gamma \vdash_R A \with B}
 {\Gamma \vdash_R B}
$$

$$
\prftree[r]{$\Intro{\tens}$}
 {\Gamma \vdash_R A}
 {}
 {\Delta \vdash_R B}
 {\Gamma, \Delta \vdash_R A \tens B}
\hspace{2em}
\prftree[r]{$\Elim{\tens}$}
 {\Gamma, \Lin{A}, \Lin{B} \vdash_R C}
 {}
 {\Delta \vdash_R A \tens B}
 {\Gamma, \Delta \vdash_R C}
$$

$$
\prftree[r]{$\Intro{!}$}
 {\Int{\Gamma} \vdash_R A}
 {\Int{\Gamma} \vdash_R !A}
\hspace{2em}
\prftree[r]{$\Elim{!}$}
 {\Gamma, \Int{A} \vdash_R B}
 {}
 {\Delta \vdash_R !A}
 {\Gamma, \Delta \vdash_R B}
$$
#+end_figure

There are now two axioms, one for each kind of assumption. The /linear identity/ $\Lin{\text{Id}}$ says that one can conclude the availability of a resource if one such resource is available. The /intuitionistic identity/ expresses the very same concept, except with one caveat \mdash the proof says nothing about how many times the resource was used in the reasoning.

The exchange rule stays unchanged, only $S$ and $T$ stand for any two propositions with square or angle brackets \mdash we are free to rearrange and intermix linear and intuitionistic assumptions.

The contraction and weakening rules are limited to intuitionistic assumptions, as mentioned in the introduction.

The $\loli$ ("lollipop") introduction rule in linear logic also abstracts an assumption, but it is limited only to linear ones. The proposition $A \loli B$ represents an action of "consuming" a resource $A$ to "produce" a resource $B$. We choose the word "consuming", because when introducing the lollipop, the resource $A$ is removed from the context. In other words, the subsequent deductions loose access to it. Because the deduction sequence leading to the judgment /\(B\)\nbsp{}avail/  was using the assumption $\Lin{A\;avail}$, we can imagine a proof of the judgment /$(A \loli B)$ avail/ to contain a hole, waiting for an $A$.

The corresponding elimination rule fills such a hole with a resource obtained from a different context. Emphasis is put on the contexts being different \mdash the context \Gamma contains other resources that are also consumed during the process of turning an $A$ into a $B$, therefore the resources cannot be shared with the context used for filling the hole.

Proof reduction for the lollipop is similar in spirit to the intuitionistic implication, except there is no need to worry about the assumption /$A$ avail/ being used multiple times. This is apparent from the fact that linear assumptions cannot be contracted. Therefore, the resulting reduction rule is simpler, as shown in Figure [[fig:lin_impl_red]].

#+name: fig:lin_impl_red
#+caption: Lollipop proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\loli}$}
 {\prftree[r]{$\Intro{\loli}$}
   {\prfsummary[s]
     {\prftree[r]{$\Lin{\text{Id}}$}
       {\Lin{A} \vdash_R A}}
     {\Gamma, \Lin{A} \vdash_R B}}
   {\Gamma \vdash_R A \loli B}}
 {\prfsummary[t]{\Delta \vdash_R A}}
 {\Gamma, \Delta \vdash_R B}}
\prfRed
\vcenter{\prfStackPremises
 {\prfsummary[t]{\Delta \vdash_R A}}
 {\prfsummary[s]{\Gamma, \Delta \vdash_R B}}}
$$
#+end_figure

The $\with$ ("with") deduction rules exactly mirror the intuitionistic conjunction rules. This connective is also called the /additive conjunction/, because the introduction rule shares the resources used for producing the individual components. This sharing of resources prevents a consumer from extracting both of the components \mdash the resources are all used once one of the components is extracted. The proof reduction is also analogous, and presented in Figure [[fig:lin_with_red]].

#+name: fig:lin_with_red
#+caption: With conjunction proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\with}_1$}
 {\prftree[r]{$\Intro{\with}$}
   {\prfsummary[s]{\Gamma \vdash_R A}}
   {}
   {\prfsummary[t]{\Gamma \vdash_R B}}
   {\Gamma \vdash_T A \with B}}
 {\Gamma \vdash_T A}}
\prfRed
\vcenter{\prfsummary[s]{\Gamma \vdash_R A}}
$$
#+end_figure

The $\tens$ ("tensor") conjunction represents a pair of resources, both of which have to be consumed, due to the requirement of not discarding resources. The introduction rule looks almost exactly the same as the one for the $\with$ conjunction, however in this case, the two parts of the tensor conjunction are produced in different contexts. It is this difference that makes the two connectives have different semantics \mdash while the $\with$ conjunction offers two different possible results from the same resources, the $\tens$ conjunction combines two sets of resources into a pair of two results, and provides both for later consumption.

The elimination rule says that a $\tens$ resource can be used to complete a proof that contains a linear assumption of each of its constituents. In other words, to consume a $\tens$ resource, one must consume both of its parts.

The reduction rule, shown in Figure [[fig:lin_tens_red]], describes how to perform such a completion. If the conjunction is constructed using two proofs $t$ and $u$ of the judgments /$A$ avail/ and /$B$ avail/, respectively, then these proofs can replace the assumptions $\Lin{A\;avail}$ and $\Lin{B\;avail}$ in another proof $s$.

#+name: fig:lin_tens_red
#+caption: Tensor conjunction proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\tens}$}
 {\prfsummary[s]
   {\prftree[r]{$\Lin{\text{Id}}$}
     {\Lin{A} \vdash_R A}}
   {}
   {\prftree[r]{$\Lin{\text{Id}}$}
     {\Lin{B} \vdash_R B}}
   {\Gamma, \Lin{A}, \Lin{B} \vdash_R C}}
 {\prftree[r]{$\Intro{\tens}$}
   {\prfsummary[t]{\Delta \vdash_R A}}
   {}
   {\prfsummary[u]{\Theta \vdash_R B}}
   {\Delta, \Theta \vdash_R A \tens B}}
 {\Gamma, \Delta, \Theta \vdash_R C}}
\prfRed
\vcenter{\prfsummary[s]
 {\prfsummary[t]{\Delta \vdash_R A}}
 {}
 {\prfsummary[u]{\Theta \vdash_R B}}
 {\Gamma, \Delta, \Theta \vdash_R C}}
$$

#+end_figure

The $\bang$ ("of course") connective is supposed to extend the expressive power of linear logic to reason about free resources. A judgment of the form /$!A$ avail/ does not represent an instance of the resource $A$, but rather /a source of/[fn:3] these resources. The idea is that a resource $A$ can be pulled out from this source at any time, or even never at all, allowing us to model free resources \mdash the judgment /$!A$ avail/ serves as a statement that $A$ is a free resource.

To produce one of these sources, the introduction rule provides us with a way of extending proofs based on only intuitionistic assumptions. Intuitionistic assumptions are another way of modeling free resources, so the essence of the introduction rule is an observation that, given a recipe of creating one unit of a resource $A$ from free ingredients $\Int{\Gamma}$, we can duplicate those free ingredients however many times is necessary to supply more instances of the resource, and that we don't mind throwing the ingredients away in the case that there is no demand for it.

Dually to the introduction rule, which relays how to create a source from free ingredients, the elimination rule describes how a source can satiate an undisclosed demand. A proof built on an intuitionistic assumption gives no guarantees about the number of times it uses the associated resource $A$. To satisfy this assumption, we can provide the proof with a source $!A$, which can adapt to its requirements.

Reducing a sequence of $\bang$ introduction and elimination looks similar to reducing an implication in intuitionistic logic, because it operates on the same principle \mdash replacing assumptions with auxiliary proofs, while acknowledging the fact that the assumptions might appear zero or more times. In the Figure [[fig:lin_exp_red]], the expression $\Int{A} \cdots$ represents zero or more intuitionistic assumptions of the judgment /$A$ avail/, and the proof tree $s$ is devoid of contraction and weakening on the judgment /$A$ avail/. Instead, these are all applied in the step represented by the double derivation line. The reduction then replaces each instance of the intuitionistic assumption /$A$ avail/ with a derivation tree $t$, which produces a resource $A$ from other intuitionistic assumptions. The double line in the reduced proof signifies applications of contraction and weakening to the assumptions $\Int{\Delta}$, corresponding to the double line in the non-reduced proof.

#+name: fig:lin_exp_red
#+caption: Exponential proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\bang}$}
 {\prftree[double]
   {\prfsummary[s]
     {\prftree[r]{$\Int{\text{Id}} \cdots$}
       {\Int{A} \vdash_R A}}
     {\Gamma, \Int{A} \cdots \vdash_R B}}
   {\Gamma, \Int{A} \vdash_R B}}
 {\prftree[r]{$\Intro{\bang}$}
   {\prfsummary[t]{\Int{\Delta} \vdash_R A}}
   {\Int{\Delta} \vdash_R !A}}
 {\Gamma, \Int{\Delta} \vdash_R B}}
\prfRed
\vcenter{\prftree[double]
 {\prfStackPremises
   {\prfsummary[t $\cdots$]{\Int{\Delta} \vdash_R A} \cdots}
   {\prfsummary[s]{\Gamma, \Int{\Delta} \cdots \vdash_R B}}}
 {\Gamma, \Int{\Delta} \vdash_R B}}
$$
#+end_figure

** TODO Intuitionistic embedding

We claimed that every intuitionistic proof can be translated to an equivalent linear proof. To verify this statement, two steps are necessary. First, we need to show how to translate the three primitive constructs: propositions, judgments, and contextualized judgments. Secondly, we need to show that this translation preserves deduction rules and proof reductions. That is to say, for every intuitionistic deduction rule or proof reduction, there is a corresponding linear deduction or reduction taking the translated premises to the translated conclusion.

The intuitionistic propositions come in three flavors: base propositions, conjunctions and implications. We define a translation operator $\LinTrans{\_}$, and its action on propositions is given by the equations
\begin{align*}
  \LinTrans{X} & = X \\
  \LinTrans{A \land B} & = \LinTrans{A} \with \LinTrans{B} \\
  \LinTrans{A \to B} & = \bang{}\LinTrans{A} \loli \LinTrans{B} \\
\end{align*}
where X stands for an atomic proposition, and A and B stand for arbitrary intuitionistic propositions.

On a formal level, this mapping is justified by showing that it preserves deduction and reduction, which is done later in the chapter. On an intuitive level, we appeal to the interpretation of the connectives. When looking at an atomic proposition in isolation, the intuitionistic and linear interpretation is the same, because differences arise only when talking about more complex propositions, and how they relate to each other, for example how are the two sides of a conjunction used, or how is the input to an implication used. The intuitionistic conjunction gives access to each of its constituents, but only to one can be extracted, behaving the same as the $\with$ conjunction. Finally, the intuitionistic implication gives no guarantees about the use of its hypothesis, therefore it is necessary to mark the hypothesis with a bang, and promote it to a source in the linear interpretation.

There are only two judgments in intuitionistic logic, and these are /$A$ type/ and /$A$ true/ for an intuitionistic proposition $A$. These are interpreted as /$A$ type/ and /$A$ avail/, respectively, defining the action of the translation operator on judgments.
\begin{align*}
  \LinTrans{A\;type} &= A\;type \\
  \LinTrans{A\;true} &= A\;avail
\end{align*}

To give a translation of a contextualized judgment, we need to describe how to translate the context. This action is defined with an equation for the empty context, labeled '$\cdot$', and an equation for a concatenation of an arbitrary context $\Gamma$ with an arbitrary assumption $J$.
\begin{align*}
  \LinTrans{\cdot} &= \cdot \\
  \LinTrans{\Gamma, J} &= \LinTrans{\Gamma}, \Int{\LinTrans{J}}
\end{align*}

Verbally, the translation preserves the empty context, and it maps every judgment $J$ in $\Gamma$ (since assumptions in intuitionistic logic are simply judgments) to an intuitionistic assumption of the translation of the judgment. As a consequence, all the assumptions in a translated context are intuitionistic. The contextualized judgment translation is then given by the equation
$$
\LinTrans{\Gamma \vdash J} = \LinTrans{\Gamma} \vdash \LinTrans{J}
$$

It is easy to see that by also defining the action of the translation on lists of propositions as $\LinTrans{(\Gamma_i)_{i=0}^n} = (\Int{\LinTrans{\Gamma_i}})_{i=0}^n$, we can recover a relationship between the shorthand notations:
$$
\LinTrans{\Gamma \vdash_T A} = \LinTrans{\Gamma} \vdash_R \LinTrans{A}
$$

$$
\LinTrans{\vcenter{\prftree[r]{Id}
    {A \vdash_T A}}}
\prfEq
\vcenter{\prftree[r]{$\Int{\text{Id}}$}{\Int{\LinTrans{A}} \vdash_R \LinTrans{A}}}
$$

$$
\LinTrans{\vcenter{\prftree[r]{Weakening}
    {\Gamma \vdash_T A}
    {\Gamma, B \vdash_T A}}}
\prfEq
\vcenter{\prftree[r]{Weakening}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A}}
  {\LinTrans{\Gamma}, \Int{\LinTrans{B}} \vdash_R \LinTrans{A}}}
$$

$$
\LinTrans{\vcenter{\prftree[r]{Contraction}
  {\Gamma, A, A \vdash_T B}
  {\Gamma, A \vdash_T B}}}
\prfEq
\vcenter{\prftree[r]{Contraction}
  {\LinTrans{\Gamma}, \Int{\LinTrans{A}}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}
  {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}}
$$

$$
\LinTrans{\vcenter{\prftree[r]{Exchange}
    {\Gamma, A, B, \Delta \vdash_T C}
    {\Gamma, B, A, \Delta \vdash_T C}}}
\prfEq
\vcenter{\prftree[r]{Exchange}
  {\LinTrans{\Gamma}, \Int{\LinTrans{A}}, \Int{\LinTrans{B}}, \LinTrans{\Delta} \vdash_R \LinTrans{C}}
  {\LinTrans{\Gamma}, \Int{\LinTrans{B}}, \Int{\LinTrans{A}}, \LinTrans{\Delta} \vdash_R \LinTrans{C}}}
$$

$$
\LinTrans{\vcenter{\prftree[r]{$\Intro{\land}$}
    {\Gamma \vdash_T A}
    {\Gamma \vdash_T B}
    {\Gamma \vdash_T A \land B}}}
\prfEq
\vcenter{\prftree[r]{$\Intro{\with}$}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A}}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{B}}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A} \with \LinTrans{B}}}
$$

$$
\LinTrans{\vcenter{\prftree[r]{$\Elim{\land}_1$}
    {\Gamma \vdash_T A \land B}
    {\Gamma \vdash_T A}}}
\prfEq
\vcenter{\prftree[r]{$\Elim{\with}_1$}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A} \with \LinTrans{B}}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A}}}
$$

$$
\LinTrans{\vcenter{\prftree[r]{$\Intro{\to}$}
    {\Gamma, A \vdash_T B}
    {\Gamma \vdash_T A \to B}}}
\prfEq
\vcenter{\prftree[r]{$\Intro{\loli}$}
  {\prftree[r]{$\Elim{\bang}$}
    {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}
    {\prftree[r]{$\Lin{\text{Id}}$}
      {\Lin{!\!\LinTrans{A}} \vdash_R !\!\LinTrans{A}}}
    {\LinTrans{\Gamma}, \Lin{!\!\LinTrans{A}} \vdash_T \LinTrans{B}}}
  {\LinTrans{\Gamma} \vdash_R !\!\LinTrans{A} \loli \LinTrans{B}}}
$$

$$
\LinTrans{\vcenter{\prftree[r]{$\Elim{\to}$}
    {\Gamma \vdash_T A \to B}
    {\Delta \vdash_T A}
    {\Gamma, \Delta \vdash_T B}}}
\prfEq
\vcenter{\prftree[r]{$\Elim{\loli}$}
  {\LinTrans{\Gamma} \vdash_R !\!\LinTrans{A} \loli \LinTrans{B}}
  {\prftree[r]{$\Intro{\bang}$}
    {\LinTrans{\Delta} \vdash_R \LinTrans{A}}
    {\LinTrans{\Delta} \vdash_R !\!\LinTrans{A}}}
  {\LinTrans{\Gamma}, \LinTrans{\Delta} \vdash_R \LinTrans{B}}}
$$


#+caption: Deducing $\Gamma, \Lin{!A} \vdash_R B$ from $\Gamma, \Int{A} \vdash_R B$ (left) and vice versa (right)
#+begin_figure
$$
\prftree[r]{$\Elim{!}$}
 {\Gamma, \Int{A} \vdash_R B}
 {\prftree[r]{$\Lin{\text{Id}}$}
   {\Lin{!A} \vdash_R !A}}
 {\Gamma, \Lin{!A} \vdash_R B}
\hspace{2em}
\prftree[r]{$\Elim{\loli}$}
 {\prftree[r]{$\Intro{\loli}$}
   {\Gamma, \Lin{!A} \vdash_R B}
   {\Gamma \vdash_R !A \loli B}}
 {\prftree[r]{$\Intro{!}$}
   {\prftree[r]{$\Int{\text{Id}}$}
     {\Int{A} \vdash_R A}}
   {\Int{A} \vdash_R !A}}
 {\Gamma, \Int{A} \vdash_R B}
$$
#+end_figure

* TODO Type theory

Type theory is the study of formal systems in which terms have an associated label called /type/, and rules for constructing the terms include the description of their behavior on the types. For more information on the subject, see \cite{Thompson1991} and \cite{PerLof1980}.

More precisely, in constructive mathematics, a mathematical object is created by construction, and the type of an object is the type of construction used to create it \cite{Bauer2018}.

One such type system is the simply typed \lambda-calculus, or STLC, which extends the untyped \lambda-calculus by introducing a set of /base types/, and inductively generates all its types with the $\to$ binary type operator, where the type $A \to B$ is the type of functions from type $A$ to type $B$. A term $t$ of type $A$ is expressed as $t: A$.

The STLC recognizes three forms for its terms, very much like the untyped \lambda-calculus. These are /variables/, of the form $x: A$, where $x$ is an atom and $A$ is a type, then /abstractions/, which represent functions, and have the form $\lambda x.t: A \to B$, where $x: A$, $t: B$, and $x$ is a free variable in $t$, becoming bound by the abstraction. Finally, abstractions can be used in an /application/, which, given the terms $f: A \to B$ and $t: A$, yields the term $f(t): B$. Application forms can be further simplified by performing /\(\beta\)-reduction/, defined using term substitution as $(\lambda x.t)(s) \to t[s/x]$, where free occurrences of $x$ in $t$ are rewritten to $s$. Performing a reduction is synonymous with /evaluating/ a program.

We define an extension of the simply typed \lambda-calculus by introducing the binary product type operator $\ProdTypeCon$, producing types of the form $\ProdType{A}{B}$, which represent tuples of one object of type $A$ and one object of type $B$. We call this extension the \(\stl\)-calculus, and the construction rules are listed in Figure\nbsp[[fig:type_deduction]].

#+name: fig:type_deduction
#+caption: Deduction rules for the \(\stl\)-calculus
#+begin_figure
$$
\prftree[r]{Id}
{x: A \vdash x: A}
$$

$$
\prftree[r]{Weakening}
{\Gamma \vdash t: A}
{\Gamma, x: B \vdash t: A}
\hspace{2em}
\prftree[r]{Contraction}
{\Gamma, x: A, y: A \vdash t: B}
{\Gamma, z: A \vdash t[z/x][z/y]: B}
$$

$$
\prftree[r]{Exchange}
{\Gamma, x: A, y: B, \Delta \vdash t: C}
{\Gamma, y: B, x: A, \Delta \vdash t: C}
$$

$$
\prftree[r]{$\Intro{\land}$}
{\Gamma \vdash x: A}
{}
{\Delta \vdash y: B}
{\Gamma, \Delta \vdash \tuple{x}{y}: \ProdType{A}{B}}
$$

$$
\prftree[r]{$\Elim{\ProdTypeCon}_1$}
{\Gamma \vdash t: \ProdType{A}{B}}
{\Gamma \vdash \ProdTypeFst{t}: A}
\hspace{2em}
\prftree[r]{$\Elim{\ProdTypeCon}_2$}
{\Gamma \vdash t: \ProdType{A}{B}}
{\Gamma \vdash \ProdTypeSnd{t}: B}
$$

$$
\prftree[r]{$\Intro{\to}$}
{\Gamma, x: A \vdash t: B}
{\Gamma \vdash \lambda x.t: A \to B}
\hspace{2em}
\prftree[r]{$\Elim{\to}$}
{\Gamma \vdash f: A \to B}
{\Delta \vdash t: A}
{\Gamma, \Delta \vdash f(t): B}
$$
#+end_figure

The new forms introduced are /tuples/, written as $\tuple{x}{y}: \ProdType{A}{B}$, which represent a pair of terms, and left and right /projections/, written as $\ProdTypeFst{t}: A$ and $\ProdTypeSnd{t}: B$, respectively, assuming a term $t: \ProdType{A}{B}$. This new syntax allows for more redundant forms of terms, which can be simplified using /\(\pi\)-reduction/ via the evaluation steps $\ProdTypeFst{\tuple{x}{y}} \to x$ and $\ProdTypeSnd{\tuple{x}{y}} \to y$.

The language is once again described with deduction rules, with zero or more premises above and one conclusion below the line. The context in a judgment now stands for a collection of typed variables, and contains the variables that are free in the term on the right side of the turnstile. In this way, the $\Intro{\to}$ rule can be intuitively interpreted by taking a variable $x: A$, and instead of treating it as free, we remove it from the context and bind it with an abstraction.

** TODO Intuitionistic embedding, revisited

#+begin_export latex
\bibliography{ComputationalTrinitarianism}
\bibliographystyle{apacite}
#+end_export

* Footnotes

[fn:3] Or /a generator of/, or /an infinite pocket of/

[fn:2] The judgment /$A$ prop/ (and subsequently /$A$ type/) is used more frequently in predicate logic and dependent type theories, which are out of scope for this thesis. The valid propositions of the relevant fragment can be described more easily with a simple grammar.

[fn:1] The notation is borrowed from Gentzen's other proof calculus, the sequent calculus. To prevent confusion of the two systems, we prefer the term /contextualized judgment/ to Gentzen's /sequent/.
