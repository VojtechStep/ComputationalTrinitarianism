#+OPTIONS: toc:nil ':t

#+latex_class: book
#+latex_header: \usepackage{fontspec}
#+latex_header: \usepackage{prftree}
#+latex_header: \usepackage{stmaryrd}
#+latex_header: \usepackage{mathtools}
#+latex_header: \usepackage{tikz-cd}
#+latex_header: \usepackage{rotating}
#+latex_header: \tikzcdset{every label/.append style={font=\small}}
#+latex_header: \tikzcdset{every diagram/.append style={row sep={4em}, column sep={4em}}}
#+latex_header: \usepackage{apacite}
#+latex_header: \usepackage{fancyhdr}
#+latex_header: \usepackage[english]{babel}
#+latex_header: \usepackage{./thesis_template/k336_thesis_macros}

# Binary or
#+latex_header: \newcommand{\binor}{\mathbin{|}}

# Introduction rule
#+latex_header: \newcommand{\Intro}[1]{#1\mathrm{I}}
# Elimination rule
#+latex_header: \newcommand{\Elim}[1]{#1\mathrm{E}}

# Proof reduction
#+latex_header: \newcommand{\prfRed}[1][1em]{\hspace{#1}\Rightarrow\hspace{#1}}
#+latex_header: \newcommand{\prfEq}[1][1em]{\hspace{#1}=\hspace{#1}}

# Linear assumption
#+latex_header: \newcommand{\Lin}[1]{\langle#1\rangle}
# Intuitionistic assumption
#+latex_header: \newcommand{\Int}[1]{[#1]}

# Tensor
#+latex_header: \newcommand{\tens}{\mathbin{\otimes}}
# With
#+latex_header: \newcommand{\with}{\mathbin{\&}}
# Lollipop
#+latex_header: \newcommand{\loli}{\multimap}
# Linear translation
#+latex_header: \newcommand{\LinTrans}[1]{\left\llbracket #1 \right\rrbracket_L}

# Product type
#+latex_header: \newcommand{\tuple}[2]{\left( #1, #2 \right)}
#+latex_header: \DeclareMathOperator{\Fst}{\mathsf{fst}}
#+latex_header: \DeclareMathOperator{\Snd}{\mathsf{snd}}
#+latex_header: \newcommand{\fst}[1]{\left(\Fst #1 \right)}
#+latex_header: \newcommand{\snd}[1]{\left(\Snd #1 \right)}

# Tensor Tuple
#+latex_header: \newcommand{\tenstup}[2]{\left| #1, #2 \right|}
#+latex_header: \DeclareMathOperator{\Case}{\mathsf{case}}
#+latex_header: \DeclareMathOperator{\Of}{\mathsf{of}}
#+latex_header: \DeclareMathOperator{\In}{\mathsf{in}}
#+latex_header: \newcommand{\tenscase}[4]{\left(\Case #1 \Of \tenstup{#2}{#3} \In #4 \right)}
# With Tuple
#+latex_header: \newcommand{\bang}{{!}}
#+latex_header: \newcommand{\bangcase}[3]{\left(\Case #1 \Of \bang{#2} \In #3 \right)}

# Categories
# Axioms
#+latex_header: \DeclareMathOperator{\Src}{\mathsf{src}}
#+latex_header: \DeclareMathOperator{\Tar}{\mathsf{tar}}
#+latex_header: \DeclareMathOperator{\Hom}{\mathsf{hom}}
#+latex_header: \DeclareMathOperator{\Id}{\mathsf{id}}
#+latex_header: \DeclareMathOperator{\Comp}{\circ}
#+latex_header: \newcommand{\src}[1]{\Src(#1)}
#+latex_header: \newcommand{\tar}[1]{\Tar(#1)}
#+latex_header: \renewcommand{\hom}[2]{\Hom(#1, #2)}
#+latex_header: \newcommand{\id}[0]{\Id}
#+latex_header: \newcommand{\comp}[0]{\Comp}
#+latex_header: \newcommand{\cat}[1]{\mathcal{#1}}

# Examples
#+latex_header: \newcommand{\Set}{\mathbf{Set}}
#+latex_header: \newcommand{\Grp}{\mathbf{Grp}}
#+latex_header: \newcommand{\op}[1]{#1^{\mathsf{op}}}

# Natural transformations
#+latex_header: \newcommand{\nat}[2]{#1 \Rightarrow #2}

# Interpretation
#+latex_header: \newcommand{\CatTrans}[1]{\left\llbracket #1 \right\rrbracket_C}

# Products
#+latex_header: \newcommand{\prodar}[2]{\left\langle #1, #2 \right\rangle}

# Monoidal structure
#+latex_header: \newcommand{\One}{\mathsf{1}}
#+latex_header: \newcommand{\assoc}[3]{\alpha_{#1, #2, #3}}

#+latex_header: \newcommand\WorkTitle{Computational trinitarianism and Linear types}
#+latex_header: \newcommand\FirstandFamilyName{Vojtěch Štěpančík}
#+latex_header: \newcommand\Supervisor{Ing. Matěj Dostál, Ph.D.}
#+latex_header: \newcommand\TypeOfWork{Bachelor's Thesis}
#+latex_header: \newcommand\StudProgram{Otevřená informatika, Bakalářský}
#+latex_header: \newcommand\StudBranch{Software}

#+begin_export latex
\graphicspath{{thesis_template/}}
\selectlanguage{english}
\translate
\coverpagestarts
\acknowledgements
...
\declaration{In Prague on ... ... 2021}
\abstractpage

This thesis focuses on extending the Curry-Howard correspondence into a linear setting. Instead of the traditional equivalence of intuitionistic logic and simply typed $\lambda$-calculus, we present a formulation of linear logic, which corresponds to a language referred to as \textit{linear $\lambda$-calculus}. We show the correspondence on three levels --- types as propositions, programs as proofs and computation as reduction.

Further, we show an embedding of intuitionistic logic into linear logic, and an analogous embedding of intuitionistic programs into linear programs. The last chapter describes a class of categories with structure, which reflect the behavior of linear programs.
\\[10pt]
\textit{Keywords:
\parbox[t]{0.8\linewidth}{natural deduction, linear logic, linear types, type theory, \\ Curry-Howard correspondence, categorical semantics}}

\vglue40mm
\noindent{\Huge \textbf{Abstrakt}}
\vskip2.75\baselineskip

\noindent
Tato práce se zabývá rozšířením Curryovy-Howardovy korespondence do lineárního prostředí. Místo tradiční ekvivalence intuicionistické logiky a jednoduše typovaného $\lambda$-kalkulu zavádíme formulaci lineární logiky, která odpovídá jazyku nazývanému \textit{lineární $\lambda$-kalkulus}. Korespondenci ukazujeme na třech úrovních --- typy jako výroky, programy jako důkazy a výpočet jako redukce.

Dále popisujeme vnoření intuicionistické logiky do lineární logiky, a analogické vnoření intuicionistických programů do lineárních programů. Poslední kapitola se věnuje třídě kategorií se strukturou, které odrážejí chování lineárních programů.
\\[10pt]
\textit{Klíčová slova:
\parbox[t]{0.8\linewidth}{přirozená dedukce, lineární logika, lineární typy, teorie typů, \\ Curryova-Howardova korespondence, kategoriální semantika}}

\tableofcontents
\listoffigures
\mainbodystarts
#+end_export

* Introduction
:PROPERTIES:
:UNNUMBERED: t
:END:

* Logic

Mathematical logic is logic treated by mathematical methods. However, such studies of different kinds of logic often use logical and deductive thinking themselves. To separate the logic observed from the logic used to make the observations, we consider them to be two separate systems. The language of the logic studied is referred to as the *object language*, while the language of the logic used for doing the observing is called the *metalanguage* \cite{Kleene1966}.

These metalanguages are then used to reason about formal composition of proofs \mdash therefore we call them *proof systems*, or *proof calculi*.

The proof system used in this paper stems from Gentzen's natural deduction \cite{Gentzen1935}. Natural deduction builds proofs on *judgements* and *propositions*.

A proposition is a formula of the object language, and a judgement is a knowable fact. For example in traditional logic (that is to say, a /truth-oriented/ logic), one might take "It is raining today" for a proposition $A$, and a judgement is the statement /$A$ is true/, or /$A$ true/ for short.

Another judgment that often arises in various logics is identifying propositions themselves \mdash one can only make judgments about a proposition $A$ if $A$ is a proposition, which is represented by the judgment /$A$ is a proposition/, abbreviated to /$A$ prop/.

We will later see that, without delving into the philosophy of mathematics, the exact nature of propositions and judgements depends on the object language.

The basis for the metalanguage is the *deduction rules*. A deduction rule consists of a collection of judgements, called the *premises*, and a single judgement, called the *conclusion*. To be able to refer to the rule in proofs, it is assigned a semantically significant name. Graphically, it is represented by drawing a horizontal line (the *derivation line*), placing the premises above it, the conclusion below, and writing the name of the rule to the right.

To illustrate, if we wanted to show the rule expressing that given two propositions, $A$ and $B$, and the judgements /$A$ true/ and /$B$ true/, one can obtain the judgement /$A \land B$ true/, we could write it as

$$
\prftree[r]{$\Intro{\land}$}
 {A\ true}
 {A\ prop}
 {B\ true}
 {B\ prop}
 {(A \land B)\ true}
$$
where the label $\Intro{\land}$ is an abbreviation for "conjunction introduction".

Gentzen used the concept of assumptions to formulate the rules for implication. If, given that /$A$ true/, we could sequence the deduction rules in such a way that we get the judgement /$B$ true/, we can abstract this dependency on a hypothetical $A$ into an implication. Gentzen used $[A]$ to denote the *assumption* of the judgement /$A$ true/, and this assumption needs to be later *discharged* by abstracting it into an appropriate implication via a corresponding implication introduction. The formulation of the $\Intro{\to}$ rule can be seen in Figure\nbsp[[fig:localized_hyp]]\nbsp(left). The symbol $\vdots$ stands for a sequence of deduction rules that can derive the judgement /$B$ true/ from the judgement /$A$ true/.
#+name: fig:localized_hyp
#+caption: Gentzen's assumption notation (left) and notation for localized assumptions (right)
#+attr_latex: :options [!h]
#+begin_figure
$$
\prftree[r]{$(\Intro{\to})_{\prfref<A>}$}
 {\prfsummary
   {\prfboundedassumption<A>{A}}
   {B}}
 {A \to B}
\hspace{2em}
\prftree[r]{$\Intro{\to}$}
 {A\ prop, A\ true, B\ prop \vdash B\ true}
 {A\ prop, B\ prop \vdash (A \to B)\ true}
$$
#+end_figure

A proof in natural deduction is tree-like, with the judgement to be proven at the root, assumptions at the leaves, and deduction rules between the nodes. It is not a proper tree, because it needs to keep track of which implication introductions discharge which assumptions, so additional structure to manage backreferences is necessary.

In this notation, assumptions are /global/ to the proof. We can change the notation to be able to reason about assumptions locally, allowing us to degenerate the proof structure to a proper tree. We say that a *contextualized judgement*[fn:1] has the form $\Gamma \vdash J$, where \Gamma is a sequence of zero or more assumptions, called the *context*, and $J$ is the judgement. An example of rewriting a proof from Gentzen's notation to the context notation is shown in Figure\nbsp[[fig:localized_hyp]]. Note that the context can also be empty. Assumptions can be added to the context via a comma: $\Gamma, S$ is a new context, which includes all the assumptions from \Gamma, and the assumption $S$. This concatenation is extended in the obvious way to merging of two contexts, so $\Gamma, \Delta$ is a context that includes all the assumptions from \Gamma, and all the assumptions from \Delta \cite{Pfenning2004}. In this new notation, deduction rules have contextualized judgements for premises and conclusion.

The behaviour of the context is specified in the metalanguage, using deduction rules. (TODO: deduction rules are not structural rules) These rules are called *structural rules*, and usually include Weakening, Contraction, and Exchange, which are listed in Figure\nbsp[[fig:structural]]. These three rules encode semantics similar to those of a finite set.

Weakening allows one to add arbitrary assumptions to the context without invalidating the derived judgement. Contraction states that assumptions may be used multiple times. Exchange asserts that the order in which assumptions appear in the context is irrelevant.

A logic which constrains one or more of these structural rules is called *substructural* \cite{Paoli2013}.

#+name: fig:structural
#+caption: Structural rules
#+begin_figure
$$
\prftree[r]{Weakening}
 {\Gamma \vdash A\ true}
 {\Gamma, B\ true \vdash A\ true}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, A\ true, A\ true \vdash B\ true}
 {\Gamma, A\ true \vdash B\ true}
$$

$$
\prftree[r]{Exchange}
 {\Gamma, A\ true, B\ true, \Delta \vdash C\ true}
 {\Gamma, B\ true, A\ true, \Delta \vdash C\ true}
$$
#+end_figure

Apart from the structural rules, the logic also specifies *logical rules*. These describe how the logical connectives participate in derivations. (TODO: wording) Conventionally, they come in pairs of introduction and elimination, the former defining how a proposition containing the connective is created, and the latter defining how such a proposition is "used" and split apart.

Just as there can be zero assumptions in a contextualized judgement, there can be zero premises in a deduction rule. Such rules are called *axioms*, and the judgments in their conclusions are always derivable.

A proof in this updated notation is now a proper tree, with a contextualized judgement at the root, contextualized judgements in the inner nodes, axioms at the leaves, and deduction rules connecting the nodes.

When composing deductions, we sometimes produce (TODO: proofs with?) redundancies. Namely when a rule for introducing a connective is immediately followed by a rule for eliminating it, the proof can be simplified via rewriting rules called *proof-reductions*. These rules must preserve well-formedness of the proof, meaning that the proof after a reduction must still consist only of derivations specified for the logic. This condition is called /local soundness/ \cite{Pfenning2004}.

(TODO: "material is standard, my contribution can be found...")

** Intuitionistic logic

Intuitionistic logic is the logic of constructive mathematics \mdash the only axiom in the system is $A\ true \vdash A\ true$, in other words, any judgement can be made assuming itself. This is in contrast with classical logic, which also axiomatizes the law of excluded middle, $\vdash (A \lor \lnot A)\ true$. The philosophical difference between classical and intuitionistic logic is that classical logic is content with knowing whether a proposition is true or whether it is false. After all, those are the only options. Intuitionistic logic, on the other hand, requires a constructive proof \mdash a "recipe", turning the assumptions into the conclusion. The law of excluded middle allows for proofs where one judges a proposition to be true, just because it cannot be false. This goes against the intuitionistic line of reasoning, because merely showing that something has to exists does not provide the mathematician with a way to construct it. In intuitionistic logic, the judgement /$(A \lor \lnot A)$ true/ can still be made, but it needs to be accompanied with either a proof of /$A$ true/ or /$\lnot A$ true/ \cite{Sorensen2006}.

Since intuitionistic logic is an example of a (TODO: is it?) traditional logic, the basic judgement that can be made about a proposition stays the same, /$A$ true/. Because this is the only judgment we will be using in the proofs[fn:2], we define a shorthand notation, $\Gamma \vdash_T A$, where \Gamma is a list of /propositions/, and $A$ is a proposition, and we take it to mean the contextualized judgment where the context is a list of judgments /$P$ true/ for every proposition $P$ in \Gamma, and where the conclusion is the judgment /$A$ true/ (the index $T$ stands for "truth"). For example, the formula $A, B \vdash_T C$ is short for $A\ true, B\ true \vdash C\ true$. This notation will be used exclusively in the diagrams to prevent them from spreading too wide, and we will use the full form in the body of the thesis.

The logic studied in this section is the meet-implicative fragment of propositional intuitionistic logic \mdash that is to say, we only concern ourselves with propositions created using the connectives $\land$ and $\to$. The propositions of this fragment can be described by the following Backus-Naur form:
$$
A, B ::= X \binor (A \to B) \binor (A \land B)
$$
for X ranging over atomic propositions. The rules (TODO: for?) of this fragment are given in Figure\nbsp[[fig:intuit_deduct]].

#+name: fig:intuit_deduct
#+caption: Deduction rules for the meet-implicative fragment of propositional intuitionistic logic
#+begin_figure
$$
\prftree[r]{Id}
 {A \vdash_T A}
\hspace{2em}
\prftree[r]{Weakening}
 {\Gamma \vdash_T A}
 {\Gamma, B \vdash_T A}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, A, A \vdash_T B}
 {\Gamma, A \vdash_T B}
\hspace{2em}
\prftree[r]{Exchange}
 {\Gamma, A, B, \Delta \vdash_T C}
 {\Gamma, B, A, \Delta \vdash_T C}
$$

$$
\prftree[r]{$\Intro{\land}$}
 {\Gamma \vdash_T A}
 {}
 {\Gamma \vdash_T B}
 {\Gamma \vdash_T A \land B}
$$

$$
\prftree[r]{$\Elim{\land}_1$}
 {\Gamma \vdash_T A \land B}
 {\Gamma \vdash_T A}
\hspace{2em}
\prftree[r]{$\Elim{\land}_2$}
 {\Gamma \vdash_T A \land B}
 {\Gamma \vdash_T B}
$$

$$
\prftree[r]{$\Intro{\to}$}
 {\Gamma, A \vdash_T B}
 {\Gamma \vdash_T A \to B}
\hspace{2em}
\prftree[r]{$\Elim{\to}$}
 {\Gamma \vdash_T A \to B}
 {}
 {\Delta \vdash_T A}
 {\Gamma, \Delta \vdash_T B}
$$
#+end_figure

The rules consist of the one axiom Id mentioned above, the three structural rules, Weakening, Contraction, and Exchange, and introduction and elimination rules for the two connectives, $\Intro{\land}$, $\Elim{\land}_1$, $\Elim{\land}_2$, $\Intro{\to}$ and $\Elim{\to}$.

/Conjunction introduction/, labeled $\Intro{\land}$ in the deduction rules, states that given a proof of /$A$ true/ and a proof of /$B$ true/, the two proofs combined give a proof of /$(A \land B$) true/. The respective elimination rules allow one to extract one of the proofs of /$A$ true/ or /$B$ true/ from /$(A \land B)$ true/, even after they were combined.

When formulating the proof reduction rule for a particular connective, one needs to look at a generic example of a reducible proof. For sequencing a conjunction introduction and a conjunction elimination, we need to represent generic proofs of the premises, then apply the two rules in succession, and finally justify an alternative path to reach the conclusion. We can represent the generic proofs with the symbol $\vdots$, (TODO: wording) much like how Gentzen formulated assumptions. For the conjunction reduction, the generic schema looks like the following tree, with the subproofs labeled $s$ and $t$.
$$
\prftree[r]{$\Elim{\land}_1$}
 {\prftree[r]{$\Intro{\land}$}
   {\prfsummary[$s$]{\Gamma \vdash_T A}}
   {}
   {\prfsummary[$t$]{\Gamma \vdash_T B}}
   {\Gamma \vdash_T A \land B}}
 {\Gamma \vdash_T A}
$$

It is easy to see that the conclusion $\Gamma \vdash A\ true$ could have been reached earlier with the $s$ subproof. The full rule is shown in Figure\nbsp[[fig:intuit_conj_red]]. The rule for the other elimination rule is not shown, as it is trivially symmetrical.

#+name: fig:intuit_conj_red
#+caption: Conjunction proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\land}_1$}
 {\prftree[r]{$\Intro{\land}$}
   {\prfsummary[$s$]{\Gamma \vdash_T A}}
   {}
   {\prfsummary[$t$]{\Gamma \vdash_T B}}
   {\Gamma \vdash_T A \land B}}
 {\Gamma \vdash_T A}}
\prfRed
\vcenter{\prfsummary[$s$]{\Gamma \vdash_T A}}
$$
#+end_figure

/Implication introduction/, labeled $\Intro{\to}$, once again builds on abstracting away an assumption. If a judgement /$B$ true/ can be made under an assumption /$A$ true/, then the proof tree can be seen as a way of turning a proof of /$A$ true/ (or multiple proofs of /$A$ true/) into a proof of /$B$ true/. The implication elimination is then a method for providing such a proof of $A$.

The proof reduction rule must take into account that the judgment /\(A\)\nbsp{}true/ might have been assumed zero or multiple times in the proof of /$B$ true/, and the context later modified with contractions or weakenings to reach the contextualized judgment $\Gamma, A\ true \vdash B\ true$. Every assumption of /$A$ true/ that is used in the proof must have been introduced by the identity rule, and the ones that are not used were introduced by weakening. As shown in \cite{Wadler1993}, applications of structural and logic rules commute, so for every proof where contraction and weakening are used, there is an equivalent proof with all the contractions and weakenings pushed to the root of the proof tree. In other words, for every proof of $\Gamma, J_1 \vdash J_2$, where $J_1$ and $J_2$ stand for arbitrary judgments, there is an equivalent proof which consists of a contraction- and weakening-less subproof of $\Gamma, J_1 \cdots \vdash J_2$, followed by applications of contraction and weakening to accommodate the context, where the ellipsis indicates zero or more assumptions of $J_1$. The final applications of contraction and weakening are represented by a doubled derivation line, to indicate that it's multiple steps shown as one.

(TODO: wording) The role of the proof reduction is then to take the proof of $\Delta \vdash A\ true$, and replace with it the instances of $A\ true \vdash A\ true$ in the proof of $\Gamma, A\ true \vdash B\ true$. The full proof reduction rule is shown in Figure\nbsp[[fig:intuit_impl_red]].

#+name: fig:intuit_impl_red
#+caption: Implication proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\to}$}
 {\prftree[r]{$\Intro{\to}$}
   {\prftree[double]
     {\prfsummary[$s$]
       {\left(\vcenter{\prftree[r]{Id}{A \vdash_T A}}\right) \cdots}
       {\Gamma, A \cdots \vdash_T B}}
     {\Gamma, A \vdash_T B}}
   {\Gamma \vdash_T A \to B}}
 {\prfsummary[$t$]{\Delta \vdash_T A}}
 {\Gamma, \Delta \vdash_T B}}
\prfRed
\vcenter{\prftree[double]
 {\prfStackPremises
   {\left(\vcenter{\prfsummary[$t$]{\Delta \vdash_T A}}\right) \cdots}
   {\prfsummary[$s$]{\Gamma, \Delta \cdots \vdash_T B}}}
 {\Gamma, \Delta \vdash_T B}}
$$
#+end_figure

** Linear logic

In contrast to intuitionistic logic, linear logic considers propositions to be a form of resource \mdash they should not be subject to duplication or discard. When looking at intuitionistic proofs, such as the ones listed in Figure [[fig:intuit_duplic]], we can see that intuitionistic logic has no problem with duplicating propositions (from a single $A$ one might obtain multiple \(A\)'s) or discarding propositions (the $B$ is unnecessary in the proof of $A$, so it is thrown away).

#+name: fig:intuit_duplic
#+caption: Duplication and discard of truth
#+begin_figure
$$
\prftree[r]{$\Intro{\to}$}
 {\prftree[r]{Contr}
   {\prftree[r]{$\Intro{\land}$}
     {\prftree[r]{Id}
       {A \vdash_T A}}
     {\prftree[r]{Id}
       {A \vdash_T A}}
     {A, A \vdash_T A \land A}}
   {A \vdash_T A \land A}}
 {\vdash_T A \to (A \land A)}
\hspace{2em}
\prftree[r]{$\Intro{\to}$}
 {\prftree[r]{$\Intro{\to}$}
  {\prftree[r]{Weak}
    {\prftree[r]{Id}
      {A \vdash_T A}}
    {A, B \vdash_T A}}
  {A \vdash_T B \to A}}
 {\vdash_T A \to (B \to A)}
$$
#+end_figure

In intuitionistic logic, we judged a proposition to be true, and the judgment had the form /$A$ true/. In linear logic, we focus on /availability/. We can judge a proposition $A$ to be available, written /$A$ avail/, if there is a proof that "consumes" some assumptions, "producing" the proposition $A$. The semantics of consumption are embedded in the deduction rules, explained below.

One simple way to prevent "invalid" usage of resources is to remove the contraction and weakening rules altogether. However, this approach severely limits the expressivity of the language. We might still want to model "free" resources, meaning resources that can be used any number of times, even zero, but conveying this information would not be possible in such a system. Instead, we introduce an annotation for unbound resources, and limit contraction and weakening so that they can only be used on these "intuitionistic" resources. This alternative gives us strictly greater expressivity than intuitionistic logic, as we will see that every intuitionistic proof can be translated to an equivalent linear proof.

The introduction of unbound resources necessitates differentiating between two kinds of assumptions in contextualized judgments \mdash a /linear/ assumption of the judgment /$A$ avail/ is written $\Lin{A\ avail}$, and indicates that the conclusion uses the fact that $A$ is available /exactly once/. An /intuitionistic/ assumption of the judgment /$A$ avail/, written $\Int{A\ avail}$, makes no guarantees about its usage in the conclusion \mdash it may be used zero, one, or even more times. It is important to emphasize that these glyphs are not a part of the object language \mdash neither $\Lin{A}$ nor $\Int{A}$ are well-formed propositions, and the bracket notation can only appear on the left side of a turnstile.

Contraction and weakening are now limited to only intuitionistic assumptions, meaning that judgments can be linearly assumed multiple times. These new rules lead to a general context $\Gamma$ behaving like a multiset. Every intuitionistic judgment can be made to have a multiplicity of one (using the new contraction and weakening), and multiplicity of linear assumptions is given by their usage in the conclusion.

Similarly to the intuitionistic case, a shorthand notation for contextualized judgments is used \mdash writing $\Gamma \vdash_R A$, the context \Gamma is a list of /propositions/ in square or angle brackets, such as $\Lin{B}$ or $\Int{C \loli D}$, and $A$ is a proposition (the index $R$ indicates that we make judgments about resources). This is shorthand for a contextualized judgment whose context is a list containing one occurrence of the judgment $\Lin{B\ avail}$ for every proposition $B$ in angle brackets in \Gamma, and one occurrence of the judgment $\Int{C\ avail}$ for every proposition $C$ in square brackets in \Gamma. The conclusion of this contextualized judgment is the judgment $A\ avail$, where $A$ is the proposition on the right of the turnstile in the shorthand.

A general context \Gamma can contain assumptions of both kinds, linear and intuitionistic, but an /intuitionistic context/, denoted by $\Int{\Gamma}$, is a context that only contains intuitionistic assumptions, if any.

The focus of this chapter is a fragment of propositional intuitionistic linear logic. It bears similarity to the intuitionistic logic described in the last chapter, specifically it provides tools for representing implication and conjunction, in addition to the linear-logic-specific exponentiation.

The new implication connective is historically called "lollipop", and it's written $A \loli B$. The proposition is read "produce $B$ consuming $A$".

Interestingly, there are two conjunction connectives \mdash the "tensor", written $A \tens B$, and the "with", written $A \with B$. The tensor represents a conjunction "containing" /both/ resources $A$ and $B$, while the "with" lists two resources that are both available, but not at the same time \mdash the recipient of such a resource needs to choose either $A$ or $B$.

The last connective is a new concept entirely. The exponential operator $\bang{A}$, pronounced "bang", allows one to represent an infinite amount of a resource. We will see how this connective differs from the intuitionistic assumption $\Int{A\ avail}$ and why they are both necessary once we take a look at program evaluation in [[*Type theory][Part III]].

The propositions of this logic can also be described by the simple grammar
$$
A, B ::= X \binor (A \loli B) \binor (A \tens B) \binor (A \with B) \  \binor \  \bang{A}
$$
for X ranging over atomic propositions. The deduction rules are listed in Figure [[fig:linear_deduct]].

#+name: fig:linear_deduct
#+caption: Deduction rules for the fragment of intuitionistic linear logic
#+begin_figure
$$
\prftree[r]{$\Lin{\text{Id}}$}
 {\Lin{A} \vdash_R A}
\hspace{2em}
\prftree[r]{$\Int{\text{Id}}$}
 {\Int{A} \vdash_R A}
$$

$$
\prftree[r]{Exchange}
 {\Gamma, S, T, \Delta \vdash_R A}
 {\Gamma, T, S, \Delta \vdash_R A}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, \Int{A}, \Int{A} \vdash_R B}
 {\Gamma, \Int{A} \vdash_R B}
\hspace{2em}
\prftree[r]{Weakening}
 {\Gamma \vdash_R B}
 {\Gamma, \Int{A} \vdash_R B}
$$

$$
\prftree[r]{$\Intro{\loli}$}
 {\Gamma, \Lin{A} \vdash_R B}
 {\Gamma \vdash_R (A \loli B)}
\hspace{2em}
\prftree[r]{$\Elim{\loli}$}
 {\Gamma \vdash_R (A \loli B)}
 {}
 {\Delta \vdash_R A}
 {\Gamma, \Delta \vdash_R B}
$$

$$
\prftree[r]{$\Intro{\with}$}
 {\Gamma \vdash_R A}
 {}
 {\Gamma \vdash_R B}
 {\Gamma \vdash_R A \with B}
$$

$$
\prftree[r]{$\Elim{\with}_1$}
 {\Gamma \vdash_R A \with B}
 {\Gamma \vdash_R A}
\hspace{2em}
\prftree[r]{$\Elim{\with}_2$}
 {\Gamma \vdash_R A \with B}
 {\Gamma \vdash_R B}
$$

$$
\prftree[r]{$\Intro{\tens}$}
 {\Gamma \vdash_R A}
 {}
 {\Delta \vdash_R B}
 {\Gamma, \Delta \vdash_R A \tens B}
\hspace{2em}
\prftree[r]{$\Elim{\tens}$}
 {\Gamma, \Lin{A}, \Lin{B} \vdash_R C}
 {}
 {\Delta \vdash_R A \tens B}
 {\Gamma, \Delta \vdash_R C}
$$

$$
\prftree[r]{$\Intro{\bang}$}
 {\Int{\Gamma} \vdash_R A}
 {\Int{\Gamma} \vdash_R \bang{A}}
\hspace{2em}
\prftree[r]{$\Elim{\bang}$}
 {\Gamma, \Int{A} \vdash_R B}
 {}
 {\Delta \vdash_R \bang{A}}
 {\Gamma, \Delta \vdash_R B}
$$
#+end_figure

There are now two axioms, one for each kind of assumption. The /linear identity/ $\Lin{\text{Id}}$ says that one can conclude the availability of a resource if one such resource is available. The /intuitionistic identity/ expresses the very same concept, except with one caveat \mdash the proof says nothing about how many times the resource was used in the reasoning.

The exchange rule stays unchanged, only $S$ and $T$ stand for any two propositions with square or angle brackets \mdash we are free to rearrange and intermix linear and intuitionistic assumptions.

The contraction and weakening rules are limited to intuitionistic assumptions, as mentioned in the introduction.

The $\loli$ ("lollipop") introduction rule in linear logic also abstracts an assumption, but it is limited only to linear ones. The proposition $A \loli B$ represents an action of "consuming" a resource $A$ to "produce" a resource $B$. We choose the word "consuming", because when introducing the lollipop, the resource $A$ is removed from the context. In other words, the subsequent deductions loose access to it. Because the deduction sequence leading to the judgment /\(B\)\nbsp{}avail/  was using the assumption $\Lin{A\ avail}$, we can imagine a proof of the judgment /$(A \loli B)$ avail/ to contain a hole, waiting for an $A$.

The corresponding elimination rule fills such a hole with a resource obtained from a different context. Emphasis is put on the contexts being different \mdash the context \Gamma contains other resources that are also consumed during the process of turning an $A$ into a $B$, therefore the resources cannot be shared with the context used for filling the hole.

Proof reduction for the lollipop is similar in spirit to the intuitionistic implication, except there is no need to worry about the assumption /$A$ avail/ being used multiple times. This is apparent from the fact that linear assumptions cannot be contracted. Therefore, the resulting reduction rule is simpler, as shown in Figure [[fig:lin_impl_red]].

#+name: fig:lin_impl_red
#+caption: Lollipop proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\loli}$}
 {\prftree[r]{$\Intro{\loli}$}
   {\prfsummary[$s$]
     {\prftree[r]{$\Lin{\text{Id}}$}
       {\Lin{A} \vdash_R A}}
     {\Gamma, \Lin{A} \vdash_R B}}
   {\Gamma \vdash_R A \loli B}}
 {\prfsummary[$t$]{\Delta \vdash_R A}}
 {\Gamma, \Delta \vdash_R B}}
\prfRed
\vcenter{\prfStackPremises
 {\prfsummary[$t$]{\Delta \vdash_R A}}
 {\prfsummary[$s$]{\Gamma, \Delta \vdash_R B}}}
$$
#+end_figure

The $\with$ ("with") deduction rules exactly mirror the intuitionistic conjunction rules. This connective is also called the /additive conjunction/, because the introduction rule shares the resources used for producing the individual components. This sharing of resources prevents a consumer from extracting both of the components \mdash the resources are all used once one of the components is extracted. The proof reduction is also analogous, and presented in Figure [[fig:lin_with_red]].

#+name: fig:lin_with_red
#+caption: With conjunction proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\with}_1$}
 {\prftree[r]{$\Intro{\with}$}
   {\prfsummary[$s$]{\Gamma \vdash_R A}}
   {}
   {\prfsummary[$t$]{\Gamma \vdash_R B}}
   {\Gamma \vdash_R A \with B}}
 {\Gamma \vdash_R A}}
\prfRed
\vcenter{\prfsummary[$s$]{\Gamma \vdash_R A}}
$$
#+end_figure

The $\tens$ ("tensor") conjunction represents a pair of resources, both of which have to be consumed, due to the requirement of not discarding resources. The introduction rule looks almost exactly the same as the one for the $\with$ conjunction, however in this case, the two parts of the tensor conjunction are produced in different contexts. It is this difference that makes the two connectives have different semantics \mdash while the $\with$ conjunction offers two different possible results from the same resources, the $\tens$ conjunction combines two sets of resources into a pair of two results, and provides both for later consumption.

The elimination rule says that a $\tens$ resource can be used to complete a proof that contains a linear assumption of each of its constituents. In other words, to consume a $\tens$ resource, one must consume both of its parts.

The reduction rule, shown in Figure [[fig:lin_tens_red]], describes how to perform such a completion. If the conjunction is constructed using two proofs $t$ and $u$ of the judgments /$A$ avail/ and /$B$ avail/, respectively, then these proofs can replace the assumptions $\Lin{A\ avail}$ and $\Lin{B\ avail}$ in another proof $s$.

#+name: fig:lin_tens_red
#+caption: Tensor conjunction proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\tens}$}
 {\prfsummary[$s$]
   {\prftree[r]{$\Lin{\text{Id}}$}
     {\Lin{A} \vdash_R A}}
   {}
   {\prftree[r]{$\Lin{\text{Id}}$}
     {\Lin{B} \vdash_R B}}
   {\Gamma, \Lin{A}, \Lin{B} \vdash_R C}}
 {\prftree[r]{$\Intro{\tens}$}
   {\prfsummary[$t$]{\Delta \vdash_R A}}
   {}
   {\prfsummary[$u$]{\Theta \vdash_R B}}
   {\Delta, \Theta \vdash_R A \tens B}}
 {\Gamma, \Delta, \Theta \vdash_R C}}
\prfRed
\vcenter{\prfsummary[$s$]
 {\prfsummary[$t$]{\Delta \vdash_R A}}
 {}
 {\prfsummary[$u$]{\Theta \vdash_R B}}
 {\Gamma, \Delta, \Theta \vdash_R C}}
$$

#+end_figure

The $\bang$ ("bang") connective is supposed to extend the expressive power of linear logic to reason about free resources. A judgment of the form /$\bang{A}$ avail/ does not represent an instance of the resource $A$, but rather /a source of/[fn:3] these resources. The idea is that a resource $A$ can be pulled out from this source at any time, or even never at all, allowing us to model free resources \mdash the judgment /$\bang{A}$ avail/ serves as a statement that $A$ is a free resource.

To produce one of these sources, the introduction rule provides us with a way of extending proofs based on only intuitionistic assumptions. Intuitionistic assumptions are another way of modeling free resources, so the essence of the introduction rule is an observation that, given a recipe of creating one unit of a resource $A$ from free ingredients $\Int{\Gamma}$, we can duplicate those free ingredients however many times is necessary to supply more instances of the resource, and that we do not mind throwing the ingredients away in the case that there is no demand for it.

Dually to the introduction rule, which relays how to create a source from free ingredients, the elimination rule describes how a source can satiate an undisclosed demand. A proof built on an intuitionistic assumption gives no guarantees about the number of times it uses the associated resource $A$. To satisfy this assumption, we can provide the proof with a source $\bang{A}$, which can adapt to its requirements.

Reducing a sequence of $\bang$ introduction and elimination looks similar to reducing an implication in intuitionistic logic, because it operates on the same principle \mdash replacing assumptions with auxiliary proofs, while acknowledging the fact that the assumptions might appear zero or more times. In the Figure [[fig:lin_exp_red]], the expression $\Int{A} \cdots$ represents zero or more intuitionistic assumptions of the judgment /$A$ avail/, and the proof tree $s$ is devoid of contraction and weakening on the judgment /$A$ avail/. Instead, these are all applied in the step represented by the double derivation line. The reduction then replaces each instance of the intuitionistic assumption /$A$ avail/ with a derivation tree $t$, which produces a resource $A$ from other intuitionistic assumptions. The double line in the reduced proof signifies applications of contraction and weakening to the assumptions $\Int{\Delta}$, corresponding to the double line in the non-reduced proof.

#+name: fig:lin_exp_red
#+caption: Exponential proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\bang}$}
 {\prftree[double]
   {\prfsummary[$s$]
     {\left(\vcenter{\prftree[r]{$\Int{\text{Id}}$}{\Int{A} \vdash_R A}}\right) \cdots}
     {\Gamma, \Int{A} \cdots \vdash_R B}}
   {\Gamma, \Int{A} \vdash_R B}}
 {\prftree[r]{$\Intro{\bang}$}
   {\prfsummary[$t$]{\Int{\Delta} \vdash_R A}}
   {\Int{\Delta} \vdash_R \bang{A}}}
 {\Gamma, \Int{\Delta} \vdash_R B}}
\prfRed
\vcenter{\prftree[double]
 {\prfStackPremises
   {\left(\vcenter{\prfsummary[$t$]{\Int{\Delta} \vdash_R A}}\right) \cdots}
   {\prfsummary[s]{\Gamma, \Int{\Delta} \cdots \vdash_R B}}}
 {\Gamma, \Int{\Delta} \vdash_R B}}
$$
#+end_figure

** Intuitionistic embedding

We claimed that every intuitionistic proof can be translated to an equivalent linear proof. To verify this statement, two steps are necessary. First, we need to show how to translate the three primitive constructs: propositions, judgments, and contextualized judgments. Secondly, we need to show that this translation preserves deduction rules and proof reductions. That is to say, for every intuitionistic deduction rule or proof reduction, there is a corresponding linear deduction or reduction taking the translated premises to the translated conclusion.

The intuitionistic propositions come in three flavors: base propositions, conjunctions and implications. We define a translation operator $\LinTrans{\_}$, and its action on propositions is given by the equations
\begin{align*}
  \LinTrans{X} & = X \\
  \LinTrans{A \land B} & = \LinTrans{A} \with \LinTrans{B} \\
  \LinTrans{A \to B} & = \bang{\LinTrans{A}} \loli \LinTrans{B} \\
\end{align*}
where $X$ stands for an atomic proposition, and $A$ and $B$ stand for arbitrary intuitionistic propositions.

On a formal level, this mapping is justified by showing that it preserves deduction and reduction, which is done later in the chapter. On an intuitive level, we appeal to the interpretation of the connectives. (TODO: wording) When looking at an atomic proposition in isolation, the intuitionistic and linear interpretation is the same, because differences arise only when talking about more complex propositions, and how they relate to each other, for example how are the two sides of a conjunction used, or how is the input to an implication used. The intuitionistic conjunction gives access to each of its constituents, but only one can be extracted, behaving the same as the $\with$ conjunction. Finally, the intuitionistic implication gives no guarantees about the use of its hypothesis, therefore it is necessary to mark the hypothesis with a bang, and promote it to a source in the linear interpretation.

There are only two judgments in intuitionistic logic, and these are /$A$ prop/ and /$A$ true/ for an intuitionistic proposition $A$. These are interpreted as /$A$ prop/ and /$A$ avail/, respectively, defining the action of the translation operator on judgments.
\begin{align*}
  \LinTrans{A\ prop} &= A\ prop \\
  \LinTrans{A\ true} &= A\ avail
\end{align*}

To give a translation of a contextualized judgment, we need to describe how to translate the context. This action is defined with an equation for the empty context, labeled '$\cdot$', and an equation for a concatenation of an arbitrary context $\Gamma$ with an arbitrary assumption $J$.
\begin{align*}
  \LinTrans{\cdot} &= \cdot \\
  \LinTrans{\Gamma, J} &= \LinTrans{\Gamma}, \Int{\LinTrans{J}}
\end{align*}

Verbally, the translation preserves the empty context, and it maps every judgment $J$ in $\Gamma$ (since assumptions in intuitionistic logic are simply judgments) to an intuitionistic assumption of the translation of the judgment. As a consequence, all the assumptions in a translated context are intuitionistic. The contextualized judgment translation is then given by the equation
$$
\LinTrans{\Gamma \vdash J} = \LinTrans{\Gamma} \vdash \LinTrans{J}
$$

It is easy to see that by also defining the action of the translation on lists of propositions as $\LinTrans{(\Gamma_i)_{i=0}^n} = (\Int{\LinTrans{\Gamma_i}})_{i=0}^n$, we can recover a relationship between the shorthand notations:
$$
\LinTrans{\Gamma \vdash_T A} = \LinTrans{\Gamma} \vdash_R \LinTrans{A}
$$

Having defined the translation of contextualized judgments, we continue by defining how their relationships are translated \mdash that is, how to translate deduction rules.

The axiom of intuitionistic logic is translated into the intuitionistic axiom of linear logic, and the structural rules correspond to their respective counterparts, as shown in the following equations:
\begin{align*}
\LinTrans{\vcenter{\prftree[r]{Id}
    {A \vdash_T A}}}
&\prfEq
\vcenter{\prftree[r]{$\Int{\text{Id}}$}{\Int{\LinTrans{A}} \vdash_R \LinTrans{A}}}
\\[1ex]
\LinTrans{\vcenter{\prftree[r]{Weakening}
    {\Gamma \vdash_T A}
    {\Gamma, B \vdash_T A}}}
&\prfEq
\vcenter{\prftree[r]{Weakening}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A}}
  {\LinTrans{\Gamma}, \Int{\LinTrans{B}} \vdash_R \LinTrans{A}}}
\\[1ex]
\LinTrans{\vcenter{\prftree[r]{Contraction}
  {\Gamma, A, A \vdash_T B}
  {\Gamma, A \vdash_T B}}}
&\prfEq
\vcenter{\prftree[r]{Contraction}
  {\LinTrans{\Gamma}, \Int{\LinTrans{A}}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}
  {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}}
\\[1ex]
\LinTrans{\vcenter{\prftree[r]{Exchange}
    {\Gamma, A, B, \Delta \vdash_T C}
    {\Gamma, B, A, \Delta \vdash_T C}}}
&\prfEq
\vcenter{\prftree[r]{Exchange}
  {\LinTrans{\Gamma}, \Int{\LinTrans{A}}, \Int{\LinTrans{B}}, \LinTrans{\Delta} \vdash_R \LinTrans{C}}
  {\LinTrans{\Gamma}, \Int{\LinTrans{B}}, \Int{\LinTrans{A}}, \LinTrans{\Delta} \vdash_R \LinTrans{C}}}
\end{align*}
\newpage

Translation of the intuitionistic conjunction is defined in terms of the $\with$ conjunction, so it is expected that the deduction rules of one will correspond to the deduction rules of the other.(TODO: wording) That is indeed the case, as the translation is given below. It uses the equality $\LinTrans{A \land B} = \LinTrans{A} \with \LinTrans{B}$.
\begin{align*}
\LinTrans{\vcenter{\prftree[r]{$\Intro{\land}$}
    {\Gamma \vdash_T A}
    {\Gamma \vdash_T B}
    {\Gamma \vdash_T A \land B}}}
&\prfEq
\vcenter{\prftree[r]{$\Intro{\with}$}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A}}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{B}}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A} \with \LinTrans{B}}}
\\[1ex]
\LinTrans{\vcenter{\prftree[r]{$\Elim{\land}_1$}
    {\Gamma \vdash_T A \land B}
    {\Gamma \vdash_T A}}}
&\prfEq
\vcenter{\prftree[r]{$\Elim{\with}_1$}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A} \with \LinTrans{B}}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A}}}
\end{align*}

The intuitionistic implication is translated with the $\bang$ and $\loli$ connectives, and the translation of the $\Intro{\to}$ rule, stated below, demonstrates why. The linear implication cannot be introduced from an intuitionistic assumption, so it necessitates an intermediary step which replaces it with a linear assumption, through $\bang$ elimination.
$$
\LinTrans{\vcenter{\prftree[r]{$\Intro{\to}$}
    {\Gamma, A \vdash_T B}
    {\Gamma \vdash_T A \to B}}}
\prfEq
\vcenter{\prftree[r]{$\Intro{\loli}$}
  {\prftree[r]{$\Elim{\bang}$}
    {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}
    {\prftree[r]{$\Lin{\text{Id}}$}
      {\Lin{\bang{\LinTrans{A}}} \vdash_R \bang{\LinTrans{A}}}}
    {\LinTrans{\Gamma}, \Lin{\bang{\LinTrans{A}}} \vdash_T \LinTrans{B}}}
  {\LinTrans{\Gamma} \vdash_R \bang{\LinTrans{A}} \loli \LinTrans{B}}}
$$

The translation for the $\Elim{\to}$ rule takes advantage of the fact that for any intuitionistic context $\Delta$, its translation $\LinTrans{\Delta}$ consists only of intuitionistic assumptions, therefore it is a valid target for applying the $\Intro{\bang}$ rule. Producing a $\bang$ proposition is required for the input of the translated implication proposition.
$$
\LinTrans{\vcenter{\prftree[r]{$\Elim{\to}$}
    {\Gamma \vdash_T A \to B}
    {\Delta \vdash_T A}
    {\Gamma, \Delta \vdash_T B}}}
\prfEq
\vcenter{\prftree[r]{$\Elim{\loli}$}
  {\LinTrans{\Gamma} \vdash_R \bang{\LinTrans{A}} \loli \LinTrans{B}}
  {\prftree[r]{$\Intro{\bang}$}
    {\LinTrans{\Delta} \vdash_R \LinTrans{A}}
    {\LinTrans{\Delta} \vdash_R \bang{\LinTrans{A}}}}
  {\LinTrans{\Gamma}, \LinTrans{\Delta} \vdash_R \LinTrans{B}}}
$$

We can extend the notion of translating deduction rules into translating entire proof trees. The linear translation of an intuitionistic proof tree $p$ is denoted $\LinTrans{p}$, and it is constructed by replacing the intuitionistic contextualized judgments and deduction rules by their linear translations. Because the deduction rules are translated into well-formed linear deductions, and because the premises and conclusions are consistently translated, we can be certain that the new deduction tree is correctly constructed and represents a well-formed linear proof.

Finally, we need to show that the translation commutes with reductions. That is, given an intuitionistic proof $p$ and its reduction $p \prfRed[0em] p'$, there is an equivalent reduction $\LinTrans{p} \LinTrans{\prfRed[0em]} \LinTrans{p}'$ such that its result is the same as translating $p'$. This condition is represented by the following diagram:
#+begin_center
\begin{tikzcd}
p \arrow[r, maps to, "\Rightarrow"] \arrow[d, maps to, "\LinTrans{\_}"] & p' \arrow[d, maps to, "\LinTrans{\_}"] \\
\LinTrans{p} \arrow[r, maps to, "\LinTrans{\Rightarrow}"] & \LinTrans{p}' = \LinTrans{p'}
\end{tikzcd}
#+end_center

To prove this commutativity, it suffices to prove it for the two intuitionistic reductions individually.

For conjunction reduction, we take a general reducible proof $p$
$$
p \prfEq \vcenter{\prftree[r]{$\Elim{\land}_1$}
  {\prftree[r]{$\Intro{\land}$}
    {\prfsummary[$s$]{\Gamma \vdash_T A}}
    {\prfsummary[$t$]{\Gamma \vdash_T B}}
    {\Gamma \vdash_T A \land B}}
  {\Gamma \vdash_T A}}
$$
its reduced form $p'$
$$
p' \prfEq \vcenter{\prfsummary[$s$]{\Gamma \vdash_T A}}
$$
and the translation $\LinTrans{p}$
$$
\LinTrans{p} \prfEq \vcenter{\prftree[r]{$\Elim{\with}_1$}
  {\prftree[r]{$\Intro{\with}$}
    {\prfsummary[$\LinTrans{s}$]{\LinTrans{\Gamma} \vdash_R \LinTrans{A}}}
    {\prfsummary[$\LinTrans{t}$]{\LinTrans{\Gamma} \vdash_R \LinTrans{B}}}
    {\LinTrans{\Gamma} \vdash_R \LinTrans{A} \with \LinTrans{B}}}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A}}}
$$

It is easily verifiable that the commutativity holds
\begin{align*}
\LinTrans{p}'
&\prfEq \vcenter{\prfsummary[$\LinTrans{s}$]{\LinTrans{\Gamma} \vdash_R \LinTrans{A}}}
\\[1ex]
&\prfEq \LinTrans{p'}
\end{align*}

The proof for implication reduction involves both lollipop and exponential reductions. First, take a general reducible proof $p$
$$
p \prfEq \vcenter{\prftree[r]{$\Elim{\to}$}
  {\prftree[r]{$\Intro{\to}$}
    {\prftree[double]
      {\prfsummary[$s$]
        {\left(\vcenter{\prftree[r]{Id}{A \vdash_T A}}\right) \cdots}
        {\Gamma, A \cdots \vdash_T B}}
      {\Gamma, A \vdash_T B}}
    {\Gamma \vdash_T A \to B}}
  {\prfsummary[$t$]{\Delta \vdash_T A}}
  {\Gamma, \Delta \vdash_T B}}
$$
its reduced form $p'$
$$
p' \prfEq \vcenter{\prftree[double]
  {\prfStackPremises
    {\left(\vcenter{\prfsummary[$t$]{\Delta \vdash_T A}}\right) \cdots}
    {\prfsummary[$s$]{\Gamma, \Delta \cdots \vdash_T B}}}
  {\Gamma, \Delta \vdash_T B}}
$$
and the translation $\LinTrans{p}$
$$
\LinTrans{p} \prfEq \vcenter{\prftree[r]{$\Elim{\loli}$}
  {\prftree[r]{$\Intro{\loli}$}
    {\prftree[r]{$\Elim{\bang}$}
      {\prftree[double]
        {\prfsummary[$\LinTrans{s}$]
          {\left(\vcenter{\prftree[r]{$\Int{\text{Id}}$}{\Int{\LinTrans{A}} \vdash_R \LinTrans{A}}}\right) \cdots}
          {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \cdots \vdash_R \LinTrans{B}}}
        {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}}
      {\prftree[r]{$\Lin{\text{Id}}$}{\Lin{\bang{\LinTrans{A}}} \vdash_R \bang{\LinTrans{A}}}}
      {\LinTrans{\Gamma}, \Lin{\bang{\LinTrans{A}}} \vdash_R \LinTrans{B}}}
    {\LinTrans{\Gamma} \vdash_R \bang{\LinTrans{A}} \loli \LinTrans{B}}}
  {\prftree[r]{$\Intro{\bang}$}
    {\prfsummary[$\LinTrans{t}$]{\LinTrans{\Delta} \vdash_R \LinTrans{A}}}
    {\LinTrans{\Delta} \vdash_R \bang{\LinTrans{A}}}}
  {\LinTrans{\Gamma}, \LinTrans{\Delta} \vdash_R \LinTrans{B}}}
$$

We can define the translation of the implication reduction as first reducing the lollipop, and subsequently reducing the exponential, as in the sequence
\begin{align*}
\LinTrans{p}
&\underset{\loli}{\prfRed}
\vcenter{\prftree[r]{$\Elim{\bang}$}
  {\prftree[double]
    {\prfsummary[$\LinTrans{s}$]
      {\left(\vcenter{\prftree[r]{$\Int{\text{Id}}$}{\Int{\LinTrans{A}} \vdash_R \LinTrans{A}}}\right) \cdots}
      {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \cdots \vdash_R \LinTrans{B}}}
    {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}}
  {\prftree[r]{$\Intro{\bang}$}
    {\prfsummary[$\LinTrans{t}$]{\LinTrans{\Delta} \vdash_R \LinTrans{A}}}
    {\LinTrans{\Delta} \vdash_R \bang{\LinTrans{A}}}}
  {\LinTrans{\Gamma}, \LinTrans{\Delta} \vdash_R \LinTrans{B}}}
\\[2ex]
&\underset{\bang}{\prfRed}
\vcenter{\prftree[double]
  {\prfStackPremises
    {\left(\vcenter{\prfsummary[$\LinTrans{t}$]{\LinTrans{\Delta} \vdash_R \LinTrans{A}}}\right) \cdots}
    {\prfsummary[$\LinTrans{s}$]{\LinTrans{\Gamma}, \LinTrans{\Delta} \cdots \vdash_R \LinTrans{B}}}}
  {\LinTrans{\Gamma}, \LinTrans{\Delta} \vdash_R \LinTrans{B}}}
\\[2ex]
&\prfEq
\LinTrans{p}'
\end{align*}

Writing down the translation of $p'$, we can see that the two conclusions are equal.
\begin{align*}
  \LinTrans{p'}
  &\prfEq
  \vcenter{\prftree[double]
    {\prfStackPremises
      {\left(\vcenter{\prfsummary[$\LinTrans{t}$]{\LinTrans{\Delta} \vdash_R \LinTrans{A}}}\right) \cdots}
      {\prfsummary[$\LinTrans{s}$]{\LinTrans{\Gamma}, \LinTrans{\Delta} \cdots \vdash_R \LinTrans{B}}}}
    {\LinTrans{\Gamma}, \LinTrans{\Delta} \vdash_R \LinTrans{B}}}
  \\[2ex]
  &\prfEq \LinTrans{p}'
\end{align*}

Because all proof reductions are composed of sequenced implication and conjunction reductions, it follows that the defined translation commutes with every proof reduction.

* Type theory

Type theory is the study of types, and it serves as a constructive way of organizing mathematical objects. Types are descriptions of constructions, and in a constructive system, every existing object needs a recipe for how it can be constructed. It follows that every mathematical object has an associated type.

To assert that a mathematical object $a$ is of a certain type $T$, we write $a : T$, and this statement is called a *typing judgment*, or sometimes simply a *typing*. Analogously to judgments in logic, a typing judgment might be valid only in a certain context, so we introduce a notion of *contextualized typing judgments*, which have the form $\Gamma \vdash a : T$, meaning that $a$ is of type $T$ in the context $\Gamma$.

Traditionally, defining a type is a procedure consisting of fours steps \cite{Bauer2019}. First, the *formation* rules are given, which describe the conditions for a mathematical object $T$ to be called a type. Then, the *introduction* rules specify how objects of this type are constructed. After an object is constructed, the *elimination* rules give ways of taking it apart. Lastly, objects that have type $T$ may relate to each other in some ways, and these relationships are described by additional *equations*.

A collection of types is called a *type system*. One such type system is the /simply typed \lambda-calculus/, or STLC for short. It uses syntax of the untyped \lambda-calculus, and a metalanguage similar to natural deduction to describe its types. The version of STLC used in this thesis is the traditional simply typed \lambda-calculus, extended with product types.

Given a collection of base types, an STLC system is generated by introducing function and product types.

The formation rules of function types and product types are almost identical, so we present them both at the same time.

/If $A$ and $B$ are types, then $(A \to B)$ is a type, and $(A \times B)$ is a type./

\noindent Types in STLC are then described by the grammar
$$
A, B ::= X \binor (A \to B) \binor (A \times B)
$$
for X ranging over base types.

Objects of STLC are *well-typed* terms of the untyped \lambda-calculus. A well-typed term is a term that is obtainable by deductions of the type system. A well-typed term is also called a *program*. A context of a contextualized typing judgment in STLC is a list of typing judgments, where the terms being typed are variables, and every variable appears in the context at most once. When concatenating contexts, it is implicitly assumed that they do not share any variables.

Analogues to the structural rules from intuitionistic logic exist for STLC. The only difference is that the type-theoretical variants provide additional information on their action on terms. All three rules are listed in Figure [[fig:stlc_struct]], along with the identity axiom.

The exchange rule remains mostly unchanged. It asserts that changing the order of variable typings in the context has no effect on either the typed term or its type.

The weakening rule plays the same role as logical weakening, but it also states that the conclusion deduces the same term of the same type as the premise. As stated above, there is an implicit assumption that the variable $x$ is not contained in the context $\Gamma$.

The contraction rule expresses that the type of an expression does not depend on specific values of its free variables, only their types. That is to say, any two variables $x$ and $y$ of the same type may be replaced by a new variable $z$ without changing the resulting type. It employs capture-avoiding variable substitution as defined in \cite{Sorensen2006}, which is a metaoperation \mdash the symbols '$[$', '$:=$' and '$]$' are not part of the language of lambda calculus. The metaterm $s[x:=z]$ stands for the term $s$ with free occurrences of the variable $x$ replaced by the term $z$.

The identity axiom claims that every variable from the context can be derived.

#+name: fig:stlc_struct
#+caption: Structural rules and the identity axiom for STLC
#+begin_figure
$$
\prftree[r]{Exchange}
 {\Gamma, x: A, y: B, \Delta \vdash s: C}
 {\Gamma, y: B, x: A, \Delta \vdash s: C}
$$

$$
\prftree[r]{Weakening}
 {\Gamma \vdash s: A}
 {\Gamma, x: B \vdash s: A}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, x: A, y: A \vdash s: B}
 {\Gamma, z: A \vdash s[x:=z][y:=z]: B}
$$

$$
\prftree[r]{Id}
 {x: A \vdash x: A}
$$
#+end_figure

The introduction and elimination rules for function types mirror the structure of implication deduction rules in intuitionistic logic. Whereas the logical interpretation relied on hypotheses, the type-theoretical interpretation is given in terms of binding variables and applying abstractions. The premise of the introduction rule presents a term $s$, and among its free variables might be the variable $x$ ($x$ is free in $s$ if it was derived using the identity axiom, or it might not be referenced in $s$ if it was derived using weakening). The conclusion then produces a \lambda-term which explicitly binds this variable.

The elimination rule introduces an application term, and together with \beta-reduction it gives a notion of "computation", which corresponds to the implication proof reduction rule. As a consequence of the \eta-conversion, we know that every object of the function type is equivalent to a \lambda-term. The rules and equations are listed in Figure [[fig:stlc_fun]].

The condition of $x$ not being free in $f$ for the \eta-conversion can be justified by looking at the expanded form of the equality, which is obtained by annotating the terms with their proof trees.
$$
\vcenter{\prfsummary{\Gamma \vdash f: (A \to B)}}
\hspace{2em}\equiv\hspace{2em}
\vcenter{\prftree[r]{$\Intro{\to}$}
 {\prftree[r]{$\Elim{\to}$}
   {\prfsummary{\Gamma \vdash f: (A \to B)}}
   {\prftree[r]{Id}{x: A \vdash x: A}}
   {\Gamma, x: A \vdash (f\ x): B}}
 {\Gamma \vdash (\lambda x. (f\ x)): (A \to B)}}
$$
We see that the tree contains a typing in the context $\Gamma, x: A$. If $x$ was free in $f$, then the list $\Gamma$ would already contain a typing of the variable $x$, leading to a proof that is not well-formed.

#+name: fig:stlc_fun
#+caption: Rules and equations of the function type
#+begin_figure
$$
\prftree[r]{$\Intro{\to}$}
 {\Gamma, x: A \vdash s: B}
 {\Gamma \vdash (\lambda x.s): (A \to B)}
\hspace{2em}
\prftree[r]{$\Elim{\to}$}
 {\Gamma \vdash f: (A \to B)}
 {\Delta \vdash s: A}
 {\Gamma, \Delta \vdash (f\ s): B}
$$
\begin{align*}
&\beta\text{-reduction: } ((\lambda x. s)\ t) \equiv s[x := t]
\\
&\eta\text{-conversion: } (\lambda x. (f\ x)) \equiv f \text{ when $x$ is not free in $f$}
\end{align*}
#+end_figure

On the other hand, the introduction and elimination rules for product types looks exactly like the ones for logical conjunction. Previously, we saw that conjunction in intuitionistic logic encodes the availability of proofs of both of its constituents, and this notion is made explicit as the product involves storing both terms. The elimination rules with \beta-reduction say that either of the two original terms may be recovered, and the reductions correspond to proof reduction of intuitionistic conjunction. The \eta-conversion for product types fulfills the same role as the one for function types \mdash we see that every object of a product type is equivalent to a term constructed with the $\Intro{\times}$ rule. The rules and equations are listed in Figure [[fig:stlc_prod]].

#+name: fig:stlc_prod
#+caption: Rules and equations of the product type
#+begin_figure
$$
\prftree[r]{$\Intro{\times}$}
 {\Gamma \vdash s: A}
 {\Gamma \vdash t: B}
 {\Gamma \vdash \tuple{s}{t}: (A \times B)}
\hspace{2em}
\prftree[r]{$\Elim{\times}_1$}
 {\Gamma \vdash s: (A \times B)}
 {\Gamma \vdash \fst{s} : A}
\hspace{2em}
\prftree[r]{$\Elim{\times}_2$}
 {\Gamma \vdash s: (A \times B)}
 {\Gamma \vdash \snd{s} : B}
$$
\begin{align*}
&\beta\text{-reduction: } \fst{\tuple{s}{t}} \equiv s
\\
&\beta\text{-reduction: } \snd{\tuple{s}{t}} \equiv t
\\
&\eta\text{-conversion: } \tuple{\fst{s}}{\snd{s}} \equiv s
\end{align*}
#+end_figure

\pagebreak
The syntax of the terms of STLC is generated by the following grammar:
\begin{alignat*}{3}
s, t &::=\ &&x \\
&\binor &&(\lambda x. s) &&\binor (s\ t) \\
&\binor &&\tuple{s}{t} &&\binor \fst{s} \binor \snd{s}
\end{alignat*}
for $x$ ranging over variables.

The resemblance between STLC and intuitionistic logic is striking, and it has a name: the /Curry-Howard correspondence/. We can see a correspondence on three different levels.

First, the propositions from intuitionistic logic correspond to types. The judgment /$A$ true/ amounts to having an appropriate term $s$ for which we can make the typing judgment $s: A$.

Second, every logical rule in intuitionistic logic has an equivalent in STLC, and every rule has an associated syntactic construct. Consequently, the term encodes the deduction tree that led to its construction, up to commuting structural rules. In other words, programs are proofs.

Lastly, the \beta-reduction rules, which are computational in nature, correspond to proof reductions. Therefore, computation is proof reduction.

** Linear types

Given the correspondence between intuitionistic logic and the simply typed \lambda-calculus, we might wonder if there is a programming language corresponding to linear logic, and indeed there is.

In this section, we introduce a programming language called /linear \lambda-calculus/, or LLC. Its form is given by assigning terms to the logical deduction rules of linear logic. The syntax was influenced by \cite{Wadler1993} and \cite{Barber1996}.

The context of contextualized judgments in LLC is a list of type judgments, each of which is enclosed in either square brackets $\Int{\_}$, indicating an intuitionistic assumption, or angle brackets $\Lin{\_}$, indicating a linear assumption. As with STLC, the terms typed in assumptions must be variables, and each variable can appear in the context at most once.

The type system includes two axioms, one for every kind of assumption, and they are used for introducing variables. The structural rules are almost identical to the ones of STLC, with the exception that they only act on intuitionistic assumptions. The axioms and structural rules are listed in Figure [[fig:llc_struct]]. The Exchange rule does not show any brackets around its assumptions, which is done to indicate that any two assumptions can be exchanged. This syntactic deviation is made in the name of not having to specify four separate exchange rules, one for each combination of an intuitionistic/linear pair.

We can see how the contraction rule allows an intuitionistic variable to be used more than once in a proof \mdash instances of two separate intuitionistic assumptions of the same type can be replaced by one variable.

#+name: fig:llc_struct
#+caption: Structural rules and identity axioms for LLC
#+begin_figure
$$
\prftree[r]{$\Lin{\text{Id}}$}
 {\Lin{x: A} \vdash x: A}
\hspace{2em}
\prftree[r]{$\Int{\text{Id}}$}
 {\Int{x: A} \vdash x: A}
$$

$$
\prftree[r]{Exchange}
 {\Gamma, x: A, y: B, \Delta \vdash s: C}
 {\Gamma, y: B, x: A, \Delta \vdash s: C}
\hspace{2em}
\prftree[r]{Weakening}
 {\Gamma \vdash s: B}
 {\Gamma, \Int{x: A} \vdash s: B}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, \Int{x: A}, \Int{y: A} \vdash s: B}
 {\Gamma, \Int{z: A} \vdash s[x:=z][y:=z]: B}
$$
#+end_figure

The rules of linear functions assign terms to $\loli$ introduction and elimination, producing linear abstraction and linear application. As a consequence of $\lambda$-terms being formed strictly by binding linear variables, we can conclude that every variable bound by a $\lambda$-term is used exactly once in its body. Therefore, the \beta-reduction is correct, meaning that the term $t$ being substituted will end up being used exactly once. Both rules and equations for linear functions are listed in Figure [[fig:llc_fun]].

#+name: fig:llc_fun
#+caption: Rules and equations for the $\loli$ function type
#+begin_figure
$$
\prftree[r]{$\Intro{\loli}$}
 {\Gamma, \Lin{x: A} \vdash s: B}
 {\Gamma \vdash (\lambda x. s): (A \loli B)}
\hspace{2em}
\prftree[r]{$\Elim{\loli}$}
 {\Gamma \vdash f: (A \loli B)}
 {\Delta \vdash s: A}
 {\Gamma, \Delta \vdash (f\ s): B}
$$
\begin{align*}
&\beta\text{-reduction: } ((\lambda x. s)\ t) \equiv s[x:=t]
\\
&\eta\text{-conversion: } (\lambda x. (f\ x)) \equiv f \text{ when $x$ is not free in $f$}
\end{align*}
#+end_figure

The $\with$ product's terms and equations correspond to the $\times$ product from STLC. The introduction rule is used for forming a tuple of two resources, each of which references the exact same context, and the elimination rules allow the consumer to pick which component they want. The reduction equations identify components of a tuple with the objects extracted using the eliminators, and the conversion equation identifies every object of a $\with$ type with one constructed using the introduction rule. The rules and equations are listed in Figure [[fig:llc_with]].

The $\with$ product is the reason why we differentiate between "using" a variable exactly once in a program, and having the variable "appear" exactly once in a program. In the contextualized typing judgment
$$
x: A \vdash \tuple{x}{x}: (A \with A)
$$
the variable $x$ is /used/ exactly once, because a $\with$ product can only be used by extracting one of its components, even though $x$ /appears/ twice in the program.

#+name: fig:llc_with
#+caption: Rules and equations for the $\with$ product type
#+begin_figure
$$
\prftree[r]{$\Intro{\with}$}
 {\Gamma \vdash s: A}
 {\Gamma \vdash t: B}
 {\Gamma \vdash \tuple{s}{t}: (A \with B)}
\hspace{2em}
\prftree[r]{$\Elim{\with}_1$}
 {\Gamma \vdash s: (A \with B)}
 {\Gamma \vdash \fst{s}: A}
\hspace{2em}
\prftree[r]{$\Elim{\with}_2$}
 {\Gamma \vdash s: (A \with B)}
 {\Gamma \vdash \snd{s}: B}
$$
\begin{align*}
&\beta\text{-reduction: } \fst{\tuple{s}{t}} \equiv s
\\
&\beta\text{-reduction: } \snd{\tuple{s}{t}} \equiv t
\\
&\eta\text{-conversion: } \tuple{\fst{s}}{\snd{s}} \equiv s
\end{align*}
#+end_figure

A tuple of two independent resources $s$ and $t$ is an instance of a $\tens$ product, and it is written $\tenstup{x}{y}$. This syntax was chosen to indicate that the two resources exist "in parallel", in contrast to the $\with$ product. The elimination rule specifies a syntactic construction known in functional languages as /pattern matching/ \mdash if the term $s$ assumes two linear variables $x$ and $y$, then a value of the appropriate $\tens$ type can be deconstructed into its parts, and the construct binds each part to the corresponding variable. Since they are both linear variables, the \beta-reduction once again preserves the property of using the components of a $\tens$ product exactly once. The rules and equations are listed in Figure [[fig:llc_tens]].

The \eta-conversion for the $\tens$ product differs from what we have seen so far \mdash past conversion were, in some sense, direct opposites of the corresponding \beta-reductions. Where \beta-reductions allowed to remove an introduction followed by an elimination, the \eta-conversions allowed wrapping a proof into an elimination followed by an introduction. On the other hand, annotating the terms of the \eta-conversion for $\tens$ products gives the following diagram.
$$
\vcenter{\prfsummary{\Gamma \vdash s: (A \tens B)}}
\hspace{2em}\equiv\hspace{2em}
\vcenter{\prftree[r]{$\Elim{\tens}$}
 {\prfsummary{\Gamma \vdash s: (A \tens B)}}
 {\prftree[r]{$\Intro{\tens}$}
   {\prftree[r]{$\Lin{\text{Id}}$}{\Lin{x: A} \vdash x: A}}
   {\prftree[r]{$\Lin{\text{Id}}$}{\Lin{y: B} \vdash y: B}}
   {\Lin{x: A}, \Lin{y: B} \vdash \tenstup{x}{y}: (A \tens B)}}
 {\Gamma \vdash \tenscase{s}{x}{y}{\tenstup{x}{y}}: (A \tens B)}}
$$
We can see that the order of introduction/elimination is reversed for this conversion. Note, however, that this proof tree is not subject to \beta-reduction, because the introduction and elimination rules act on different instances of the connective \mdash we introduce the term $\tenstup{x}{y}$, but eliminate the term $s$. The reason for this pattern change is that the $\tens$ product has a different /polarity/. While all the previous types were /negative types/, the $\tens$ product is /positive/. Intuitively, positive types encode structure, while negative types encode behavior. Exploration of type polarity is, however, out of scope for this thesis, therefore the interested reader may consult \cite{Zeilberger2009}.

#+name: fig:llc_tens
#+caption: Rules and equations for the $\tens$ product type
#+begin_figure
$$
\prftree[r]{$\Intro{\tens}$}
 {\Gamma \vdash s: A}
 {\Delta \vdash t: B}
 {\Gamma, \Delta \vdash \tenstup{s}{t}: (A \tens B)}
\hspace{2em}
\prftree[r]{$\Elim{\tens}$}
 {\Gamma, \Lin{x: A}, \Lin{y: B} \vdash s: C}
 {\Delta \vdash t: (A \tens B)}
 {\Gamma, \Delta \vdash \tenscase{t}{x}{y}{s}: C}
$$
\begin{align*}
&\beta\text{-reduction: } \tenscase{\tenstup{s}{t}}{x}{y}{u} \equiv u[x:=s][y:=t]
\\
&\eta\text{-conversion: } \tenscase{s}{x}{y}{\tenstup{x}{y}} \equiv s
\end{align*}
#+end_figure

The $\bang$ exponential is another example of a positive type. The pattern matching term $\bangcase{t}{x}{s}$ decomposes a source $t$ into its "template" $x$, which can then be used however many times is necessary in the program $s$. The rules and equations are listed in Figure [[fig:llc_bang]].

At first sight, it might not be obvious why the \beta-reduction holds. After all, the substitution might result in $t$ being used non-linearly. Upon further inspection, we see that since $t$ was promoted to $\bang{t}$ using the $\Intro{\bang}$ rule, it can only use intuitionistic variables. Therefore, the substitution cannot break any linearity contracts.

#+name: fig:llc_bang
#+caption: Rules and equations for the $\bang$ exponential type
#+begin_figure
$$
\prftree[r]{$\Intro{\bang}$}
 {\Int{\Gamma} \vdash s: A}
 {\Int{\Gamma} \vdash \bang{s}: \bang{A}}
\hspace{2em}
\prftree[r]{$\Elim{\bang}$}
 {\Gamma, \Int{x: A} \vdash s: B}
 {\Delta \vdash t: \bang{A}}
 {\Gamma, \Delta \vdash \bangcase{t}{x}{s}}
$$
\begin{align*}
&\beta\text{-reduction: } \bangcase{\bang{t}}{x}{s} \equiv s[x:=t]
\\
&\eta\text{-conversion: } \bangcase{s}{x}{\bang{x}} \equiv s
\end{align*}
#+end_figure
\newpage

The syntax of LLC is generated by the following grammar:
\begin{alignat*}{3}
s, t &::=\ &&x \\
&\binor &&(\lambda x. s) &&\binor (s\ t) \\
&\binor &&\tuple{s}{t} &&\binor \fst{s} \binor \snd{s} \\
&\binor &&\tenstup{s}{t} &&\binor \tenscase{s}{x}{y}{t} \\
&\binor &&\bang{s} &&\binor \bangcase{s}{x}{t}
\end{alignat*}
for $x$ and $y$ ranging over variables.

Substitution of LLC terms is defined analogously to substitution of STLC terms. The rules are listed in Figure [[fig:llc_subst]], and they are defined for distinct variables $x$, $y$ and $z$, and for LLC terms $s$, $t$ and $u$. The substitution avoids variable capture \mdash if a free variable in $s$ would become bound following the substitution, then the substitution is not defined, and renaming variables must precede.

#+name: fig:llc_subst
#+caption: Substitution of LLC terms
#+begin_figure
\begin{align*}
x[x:=s] &= s \\
y[x:=s] &= y \\
(\lambda x. t)[x:=s] &= (\lambda x. t) \\
(\lambda y. t)[x:=s] &= (\lambda y. t[x:=s]) \\
(t\ u)[x:=s] &= (t[x:=s]\ u[x:=s]) \\
\tuple{t}{u}[x:=s] &= \tuple{t[x:=s]}{u[x:=s]} \\
\fst{t}[x:=s] &= \fst{t[x:=s]} \\
\snd{t}[x:=s] &= \snd{t[x:=s]} \\
\tenstup{t}{u}[x:=s] &= \tenstup{t[x:=s]}{u[x:=s]} \\
\tenscase{t}{x}{y}{u}[x:=s] &= \tenscase{t[x:=s]}{x}{y}{u} \\
\tenscase{t}{y}{x}{u}[x:=s] &= \tenscase{t[x:=s]}{y}{x}{u} \\
\tenscase{t}{y}{z}{u}[x:=s] &= \tenscase{t[x:=s]}{y}{z}{u[x:=s]} \\
(\bang{t})[x:=s] &= \bang{(t[x:=s])} \\
\bangcase{t}{x}{u}[x:=s] &= \bangcase{t[x:=s]}{x}{u} \\
\bangcase{t}{y}{u}[x:=s] &= \bangcase{t[x:=s]}{y}{u[x:=s]}
\end{align*}
#+end_figure

** Commuting conversions
In addition to equations describing relationships between introductions and eliminations of the same type, there are also equations relating rules of different types. These equations, called *commuting conversions*, describe valid ways of moving pattern matching terms through the program. For example, when given the program
$$
\tenscase{s}{x}{y}{(f\ {\tenstup{x}{y}})}
$$
one might feel that it ought to be equivalent to the program
$$
(f\ s)
$$
because the object $s$ is being deconstructed only to be reconstructed in the same manner later, exactly like in the \eta-conversion rule for the $\tens$ product. Alas, \eta-conversion is not applicable in this case, because the $\tens$ elimination-introduction pair is interleaved with function application. Commuting conversions give us a framework for "tunneling" pattern matching, so that the first program can be rewritten to
$$
(f\ \tenscase{s}{x}{y}{\tenstup{x}{y}})
$$
where \eta-conversion is applicable.

We describe commuting conversions using terms-with-holes à la Barber[fn:4] \cite{Barber1996}. A term-with-holes is a mathematical object defined by the grammar
\begin{alignat*}{4}
C[\_], D[\_] &::=\ &&\_ \\
&\binor &&(\lambda x. C[\_]) &&\binor (C[\_]\ s) &&\binor (s\ C[\_]) \\
&\binor &&\tuple{C[\_]}{D[\_]} &&\binor \fst{C[\_]} &&\binor \snd{C[\_]} \\
&\binor &&\tenstup{C[\_]}{s} &&\binor \tenstup{s}{C[\_]} &&\binor \tenscase{C[\_]}{x}{y}{s} \binor \tenscase{s}{x}{y}{C[\_]} \\
&\binor &&\mathrlap{\bangcase{C[\_]}{x}{s}} && &&\binor \bangcase{s}{x}{C[\_]}
\end{alignat*}
for $x$ and $y$ ranging over variables and $s$ ranging over terms of LLC.

In effect, a term-with-holes is a program constructed without the use of the $\Intro{\bang}$ rule, with a subterm replaced by '_', called a hole. Note that a term-with-holes $C[\_]$ uses exactly one hole. Emphasis is once again on the terminology "uses", because multiple holes may appear in a term-with-holes, if it was constructed using the $\tuple{C[\_]}{D[\_]}$ rule. Next, we provide a way to fill the hole \mdash $C[s]$ is defined as the term $C[\_]$ with the hole replaced by the term $s$.

The commuting conversions are listed in Figure [[fig:comm_conv]]. The motivating example mentioned above is an application of the first commuting conversion, with the term-with-holes $C[\_]$ being equal to $(f\ \_)$, and the term $s$ being equal to $\tenstup{x}{y}$.

The requirements on the bindings of variables arise naturally when one writes down the proof trees for the terms on the two sides of the equations. When considering the first commuting conversion, the proof tree of the left side has the form
$$
\prftree[r]{$\Elim{\tens}$}
 {\prfsummary
   {\Theta, \Lin{x: A}, \Lin{y: B} \vdash t: E}
   {\Gamma, \Theta, \Lin{x: A}, \Lin{y: B} \vdash C[t]: D}}
 {}
 {\Delta \vdash s: (A \tens B)}
 {\Gamma, \Theta, \Delta \vdash \tenscase{s}{x}{y}{C[t]}: D}
$$
and the right side has the form
$$
\prfsummary
 {\prftree[r]{$\Elim{\tens}$}
   {\Theta, \Lin{x: A}, \Lin{y: B} \vdash t: E}
   {}
   {\Delta \vdash s: (A \tens B)}
   {\Theta, \Delta \vdash \tenscase{s}{x}{y}{t}: E}}
 {\Gamma, \Theta, \Delta \vdash C[\tenscase{s}{x}{y}{t}]: D}
$$
Since the first derivation contains the context $\Gamma, \Theta, \Lin{x: A}, \Lin{y: B}$, we know that neither $x$ nor $y$ may appear in $\Gamma$, which contains the free variables of $C[\_]$. The second condition specifies that the $x$ and $y$ that are free in $t$ are the same $x$ and $y$ that are bound by the pattern matching in the final term. The only way to make them differ would be if the proof tree of $C[\_]$ first bound them for $t$, and then introduced them as new variables. For example, the term-with-hole
$$
(((\lambda y. (\lambda x. \_))\ x)\ y)
$$
is not eligible for the commuting conversion, because moving the pattern matching into the hole would swap the values of $x$ and $y$.

Similar reasoning is used for obtaining the conditions of the second commuting conversion.

#+name: fig:comm_conv
#+caption: Commuting conversions
#+begin_figure
\begin{align*}
\tenscase{s}{x}{y}{C[t]} &\equiv C[\tenscase{s}{x}{y}{t}] &&\text{when $x$ and $y$ are not free in $C[\_]$} \\
& &&\text{and when $C[\_]$ does not bind $x$ or $y$} \\
\bangcase{s}{x}{C[t]} &\equiv C[\bangcase{s}{x}{t}] &&\text{when $x$ is not free in $C[\_]$} \\
& &&\text{and when C[\_] does not bind $x$}
\end{align*}
#+end_figure

** Rationale for kinded assumptions
Equipped with an explicit notation for terms, representing proofs, we can see why this system distinguishes between intuitionistic assumptions and exponential types as two representations of free resources. While intuitionistic assumptions can be only /variables/, objects of exponential types can be entire programs, containing linear variables. One might be tempted to simplify LLC by removing intuitionistic assumptions and replacing them with assumptions of exponential types, for example giving rise to the alternative rule for contraction:
$$
\prftree[r]{$\bang{\text{Contraction}}$}
 {\Gamma, x: \bang{A}, y: \bang{A} \vdash s: B}
 {\Gamma, z: \bang{A} \vdash s[x:=z][y:=z]: B}
$$
but we will see that this system breaks linearity when applying \beta-reductions.

The best way to approach this topic is with an example. Let the type $W$ represent a proposition "I have a cup of water", and the type $G$ represent "I have a liter of gas in the tank of my car". Consequently, the types $\bang{W}$ represents a water source, because it can provide an unlimited amount of cups of water, and the type $G \loli \bang{W}$ represents a procedure for obtaining a water source using a moving car. Specifically, imagine the variable $c: G$ being a car with gas in the tank, and the function $f: A \loli \bang{W}$ being the ability to drive to a neighboring city, bringing back a water fountain. Then the program $(f\ c): \bang{W}$ represents the fountain obtained by going into the other city, consuming the gas in the process.

We can derive the following program, which says that water from one such fountain can be distributed into two fountains.
$$
\prftree[r]{$\Elim{\loli}$}
 {\prftree[r]{$\Intro{\loli}$}
   {\prftree[r]{$\bang{\text{Contraction}}$}
     {\prftree[r]{$\Intro{\tens}$}
       {\prftree[r]{$\Lin{\text{Id}}$}{\Lin{x: \bang{W}} \vdash x: \bang{W}}}
       {\prftree[r]{$\Lin{\text{Id}}$}{\Lin{y: \bang{W}} \vdash y: \bang{W}}}
       {\Lin{x: \bang{W}}, \Lin{y: \bang{W}} \vdash \tenstup{x}{y}: (\bang{W} \tens \bang{W})}}
     {\Lin{z: \bang{W}} \vdash \tenstup{z}{z}: (\bang{W} \tens \bang{W})}}
   {\vdash (\lambda z. \tenstup{z}{z}): (\bang{W} \loli (\bang{W} \tens \bang{W}))}}
 {\hspace{-3em}\prfsummary{\Lin{f: (G \loli \bang{W})}, \Lin{c: G} \vdash (f\ c): \bang{W}}}
 {\Lin{f: (G \loli \bang{W})}, \Lin{c: G} \vdash (\lambda z. \tenstup{z}{z})\ (f\ c): (\bang{W} \tens \bang{W})}
$$
This program is subject to \beta-reduction, because of the sequence of $\loli$ introduction and elimination. However, reducing the program leads to the typing
$$
\Lin{f: (G \loli \bang{W})}, \Lin{c: G} \vdash \tenstup{(f\ c)}{(f\ c)}: (\bang{W} \tens \bang{W})
$$
which clearly does not produce a well-typed term, because the linear variables $f$ and $c$ are used twice. This is akin to taking two trips in the car, but only having fuel for one trip. In other words, the system would not be locally sound.

The term assignment with intuitionistic assumptions solves this problem by not allowing multiple-use variables to be bound as arguments to lambdas. Instead, the argument must be always linear, and later consumed by exponential pattern matching. That is to say, the above program is written
$$
\Lin{f: (G \loli \bang{W})}, \Lin{c: G} \vdash \bangcase{(f\ c)}{z}{\tenstup{z}{z}}: (\bang{W} \tens \bang{W})
$$
where the linear variables are correctly used exactly once.

Flavors of linear logic and their term assignments without intuitionistic assumptions exist, for example \cite{Benton1993}. In general, these variants provide term assignments for the structural rules also. This thesis presents the variant with kinded assumptions, for its closer resemblance to the rules of STLC.

** Intuitionistic embedding, revisited

Type theory extends the proof-theoretical point of view through programs encoding proofs. Since we are able to embed intuitionistic logic into linear logic, we want to also embed STLC into LLC, and this embedding needs to follow two conditions. First, it needs to agree with the Curry-Howard correspondence \mdash that is to say, embedding of types must behave the same as embedding of propositions, embedding of programs must behave the same as embedding of proofs and embedding of computations must behave the same as embedding of proof reductions. Secondly, equivalent programs in STLC must translate to equivalent programs in LLC.

The first condition is satisfied rudimentarily \mdash we define the translations of types, programs and computations by adding terms to the proof trees used in defining their logical counterparts. Then, agreement with the Curry-Howard correspondence is reached by definition.

For the intuitionistic correspondence between types and propositions, we obtain the following type embedding
\begin{align*}
\LinTrans{X} &= X \\
\LinTrans{A \times B} &= \LinTrans{A} \with \LinTrans{B} \\
\LinTrans{A \to B} &= \bang{\LinTrans{A}} \loli \LinTrans{B}
\end{align*}

For translating programs, we find the terms corresponding to translating derivation rules of intuitionistic logic.
\begin{align*}
&\LinTrans{x} = x &\mathrm{Id} \\
&\LinTrans{(\lambda x. s)} = (\lambda y. \bangcase{y}{x}{\LinTrans{s}}) \text{ for $y$ not free in $s$} &\Intro{\to} \\
&\LinTrans{(s\ t)} = (\LinTrans{s}\ \bang{\LinTrans{t}}) &\Elim{\to} \\
&\LinTrans{\tuple{s}{t}} = \tuple{\LinTrans{s}}{\LinTrans{t}} &\Intro{\times} \\
&\LinTrans{\fst{s}} = \fst{\LinTrans{s}} &\Elim{\times}_1 \\
&\LinTrans{\snd{s}} = \snd{\LinTrans{s}} &\Elim{\times}_2
\end{align*}
for $x$ ranging over variables and $s$ and $t$ ranging over well-typed terms.

Showing commutativity of equations with translation first requires commutativity of substitution with translation. We need to show that for every well-typed term $s$ from STLC, the following equation holds
$$
\LinTrans{t[x:=s]} \equiv \LinTrans{t}[x:=\LinTrans{s}]
$$
This is accomplished using structural induction over terms of STLC.

This property trivially holds for variables. For $x$ and $y$ two distinct variables, we have the following equalities
\begin{align*}
\LinTrans{x[x:=s]} &= \LinTrans{s} = x[x:=\LinTrans{s}] = \LinTrans{x}[x:=\LinTrans{s}] \\
\LinTrans{y[x:=s]} &= \LinTrans{y} = y = y[x:=\LinTrans{s}] = \LinTrans{y}[x:=\LinTrans{s}]
\end{align*}

For the induction step, we assume that the property holds for every subterm, and we produce the following equalities, for $x$ and $y$ two distinct variables and $s$ and $t$ well-typed terms in STLC
\begin{align*}
\LinTrans{(\lambda x. t)[x:=s]}
&= \LinTrans{(\lambda x. t)} \\
&= (\lambda y. \bangcase{y}{x}{\LinTrans{t}}) \\
&= (\lambda y. \bangcase{y[x:=\LinTrans{s}]}{x}{\LinTrans{t}}) \\
&= (\lambda y. \bangcase{y}{x}{\LinTrans{t}}[x:=\LinTrans{s}]) \\
&= (\lambda y. \bangcase{y}{x}{\LinTrans{t}})[x:=\LinTrans{s}] \\
&= \LinTrans{(\lambda x. t)}[x:=\LinTrans{s}] \\
\LinTrans{(\lambda y. t)[x:=s]}
&= \LinTrans{(\lambda y. t[x:=s])} \\
&= (\lambda z. \bangcase{z}{y}{\LinTrans{t[x:=s]}}) \\
&= (\lambda z. \bangcase{z}{y}{\LinTrans{t}[x:=\LinTrans{s}]}) \\
&= (\lambda z. \bangcase{z[x:=\LinTrans{s}]}{y}{\LinTrans{t}[x:=\LinTrans{s}]}) \\
&= (\lambda z. \bangcase{z}{y}{\LinTrans{t}}[x:=\LinTrans{s}]) \\
&= (\lambda z. \bangcase{z}{y}{\LinTrans{t}})[x:=\LinTrans{s}] \\
&= \LinTrans{(\lambda y. t)}[x:=\LinTrans{s}] \\
\LinTrans{(t\ u)[x:=s]}
&= \LinTrans{(t[x:=s]\ u[x:=s])} \\
&= (\LinTrans{t[x:=s]}\ \bang{\LinTrans{u[x:=s]}}) \\
&= (\LinTrans{t}[x:=\LinTrans{s}]\ \bang{\LinTrans{u}[x:=\LinTrans{s}]}) \\
&= (\LinTrans{t}\ \bang{\LinTrans{u}})[x:=\LinTrans{s}] \\
&= \LinTrans{(t\ u)}[x:=\LinTrans{s}] \\
\LinTrans{\tuple{t}{u}[x:=s]}
&= \LinTrans{\tuple{t[x:=s]}{u[x:=s]}} \\
&= \tuple{\LinTrans{t[x:=s]}}{\LinTrans{u[x:=s]}} \\
&= \tuple{\LinTrans{t}[x:=\LinTrans{s}]}{\LinTrans{u}[x:=\LinTrans{s}]} \\
&= \tuple{\LinTrans{t}}{\LinTrans{u}}[x:=\LinTrans{s}] \\
&= \LinTrans{\tuple{t}{u}}[x:=\LinTrans{s}] \\
\LinTrans{\fst{t}[x:=s]}
&= \LinTrans{\fst{t[x:=s]}} \\
&= \fst{\LinTrans{t[x:=s]}} \\
&= \fst{\LinTrans{t}[x:=\LinTrans{s}]} \\
&= \fst{\LinTrans{t}}[x:=\LinTrans{s}] \\
&= \LinTrans{\fst{t}}[x:=\LinTrans{s}] \\
\LinTrans{\snd{t}[x:=s]}
&= \LinTrans{\snd{t[x:=s]}} \\
&= \snd{\LinTrans{t[x:=s]}} \\
&= \snd{\LinTrans{t}[x:=\LinTrans{s}]} \\
&= \snd{\LinTrans{t}}[x:=\LinTrans{s}] \\
&= \LinTrans{\snd{t}}[x:=\LinTrans{s}] \\
\end{align*}

# NOTE: Maybe unnecessary later
\pagebreak
Next, we need to show that equivalent STLC terms are translated to equivalent LLC terms. This is accomplished by showing that this property holds for every \beta-reduction and every \eta-conversion equation. That is, for every equation $s \equiv s'$, we need to show that $\LinTrans{s} \equiv \LinTrans{s}$.

Proofs of commutativity for the \beta-reductions are obtained by the corresponding proofs of commutativity for proof reductions.
\begin{align*}
\LinTrans{((\lambda x. s)\ t)}
&= (\LinTrans{(\lambda x. s)}\ \bang{\LinTrans{t}}) \\
&= ((\lambda y. \bangcase{y}{x}{\LinTrans{s}})\ \bang{\LinTrans{t}}) \\
&\equiv \bangcase{\bang{\LinTrans{t}}}{x}{\LinTrans{s}} \\
&\equiv \LinTrans{s}[x:=\LinTrans{t}] \\
&= \LinTrans{s[x:=t]} \\
\LinTrans{\fst{\tuple{s}{t}}}
&= \fst{\LinTrans{\tuple{s}{t}}} \\
&= \fst{\tuple{\LinTrans{s}}{\LinTrans{t}}} \\
&\equiv \LinTrans{s} \\
\LinTrans{\snd{\tuple{s}{t}}}
&= \snd{\LinTrans{\tuple{s}{t}}} \\
&= \snd{\tuple{\LinTrans{s}}{\LinTrans{t}}} \\
&\equiv \LinTrans{t} \\
\end{align*}

We have not specified what \eta-conversions correspond to in logic, so the verification is not as simple as following existing proofs, however it is still straight-forward.
\begin{align*}
\LinTrans{(\lambda x. (f\ x))}
&= (\lambda y. \bangcase{y}{x}{\LinTrans{(f\ x)}}) \\
&= (\lambda y. \bangcase{y}{x}{(\LinTrans{f}\ \bang{\LinTrans{x}})}) \\
&\equiv (\lambda y. (\LinTrans{f}\ \bangcase{y}{x}{\bang{x}})) \\
&\equiv (\lambda y. (\LinTrans{f}\ y)) \\
&\equiv \LinTrans{f} \\
\LinTrans{\tuple{\fst{s}}{\snd{s}}}
&= \tuple{\LinTrans{\fst{s}}}{\LinTrans{\snd{s}}} \\
&= \tuple{\fst{\LinTrans{s}}}{\snd{\LinTrans{s}}} \\
&\equiv \LinTrans{s}
\end{align*}

Since we covered all equations from STLC, we can conclude that equivalent programs translate to equivalent programs.

* TODO Category theory

Category theory is the study of categories, which allow for expressing many mathematical structures and properties via diagrams of arrows between abstract objects \cite{MacLane1998}. Categorical semantics, as opposed to the traditional set-theoretical interpretations, have the advantage of being defined with more general structures, not limiting oneself to sets specifically \cite[pp.129--132]{Crole1993}.

This chapter introduces only those categorical concepts that are necessary for giving categorical semantics to the linear lambda calculus, and it only gives a bird-eye view. For a broader introduction to category theory, see \cite{MacLane1998} or \cite{Adamek1990}. The ultimate objective is to represent types as objects in a category, programs as arrows from a context object to a target object, type constructors as functors and rules as natural transformations.

A *category* is composed of two collections, the *objects* and the *arrows*, two operations on any arrow, the *source* and the *target*, and additional structure, specifically the *arrow composition* operator and the existence of *identity arrows*, which are the left and right identity for the composition.

The source and target operations are written $\Src$ and $\Tar$, respectively, and each associates an object to an arrow \mdash an arrow $a$ with $\src{a} = X$ and $\tar{a} = Y$ is written graphically as $a: X \to Y$ or $X \xrightarrow{a} Y$. The collection of arrows between objects $X$ and $Y$ is written $\hom{X}{Y}$.

The composition operator asserts that for any two arrows with matching ends, for example $a: X \to Y$ and $b: Y \to Z$, there exists an arrow $b \comp a: X \to Z$. This composition is associative, that is, the following equality holds for any three arrows $a: X \to Y$, $b: Y \to Z$, $c: Z \to W$ for any four objects $X$, $Y$, $Z$ and $W$
$$
c \comp (b \comp a) = (c \comp b) \comp a
$$

Every object $X$ is equipped with the identity arrow $\id_X: X \to X$. The identity arrows serve as identities for composition, which is described by the equations
\begin{align*}
\id_{\tar{a}} \comp a &= a \\
a \comp \id_{\src{a}} &= a
\end{align*}
In situations where the object is apparent from context, we choose to omit the subscript and simply write $\id$.

Examples of categories include $\Set$, the category with all sets for objects and functions for arrows, or $\Grp$, the category of all groups and homomorphisms between them.

For every category $\cat{C}$, there exists the *opposite category* $\op{\cat{C}}$. The opposite category has the exact same objects and identity arrows as the original category, but the arrows are reversed \mdash the collection of arrows $\hom{X}{Y}$ in $\cat{C}$ corresponds to the collection of arrows $\hom{Y}{X}$ in $\op{\cat{C}}$, and the composition $a \comp b$ in $\op{\cat{C}}$ corresponds to the composition $b \comp a$ in $\cat{C}$.

An important tool of category theory are commutative diagrams. A commutative diagram is a diagram for which commutativity is either assumed or proven. Such a diagram contains nodes, which represent objects, and directed edges, which represent arrows. For example, we can construct a commutative diagram which represents the property of the identity arrow being the identity of arrow composition.
#+begin_center
\begin{tikzcd}
X \arrow[r, "a"] \arrow[dr, "a"] & Y \arrow[d, "\id_Y"] \arrow[rd, "b"] & \\
& Y \arrow[r, "b"] & Z
\end{tikzcd}
#+end_center
This diagram commutes when the two triangles commute, and we can see that the left triangle expresses the equality $\id_Y \comp a = a$, and the right one $b \comp \id_Y = b$.

An arrow $a: X \to Y$ is an *isomorphism* if there is another arrow $a^{-1}: Y \to X$ such that the equations $a^{-1} \comp a = \id_X$ and $a \comp a^{-1} = \id_Y$ hold. Isomorphisms are also called /invertible/ arrows.

In the remainder of this thesis, we will only consider *locally small categories*, which have the property that for every two objects $X$ and $Y$, the collection $\hom{X}{Y}$ is a set. For example the category *Set* is locally small, because although the objects form a proper class, there is only a set of functions between any two given sets.

We introduce the notion of structure-preserving mappings between categories, called *functors*. Given two categories $\cat{C}$ and $\cat{D}$, a functor $F$ from $\cat{C}$ to $\cat{D}$, denoted $F: \cat{C} \to \cat{D}$, needs to map contents of $\cat{C}$ to contents of $\cat{D}$ while preserving the categorical structure \mdash the contents of a category are its objects and arrows, and the structure is described by the source and target assignments, arrow composition and identity arrows.

A functor $F: \cat{C} \to \cat{D}$ consists of a mapping from objects of $\cat{C}$ to objects of $\cat{D}$ and another mapping from arrows of $\cat{C}$ to arrows of $\cat{D}$, the two of which interact in such a way that for any arrow $a$ in $\cat{C}$, its image $Fa$ is an arrow in $\cat{D}$ whose source and target are the images of the source and target of $a$, graphically the image of an arrow $a: X \to Y$ is $Fa: FX \to FY$. Additionally, the identity arrows in $\cat{C}$ map to identity arrows in $\cat{D}$, so that $F\id_X = \id_{FX}$ for all objects $X$ in $\cat{C}$, and the compositions of arrows in $\cat{C}$ maps to composition of arrows in $\cat{D}$, following the equation $F(b \comp a) = Fb \comp Fa$. These laws are represented by the commutative diagrams in Figure [[fig:functor_laws]].

#+name: fig:functor_laws
#+caption: Functor laws
#+begin_figure
#+begin_center
\hspace{2em}
\begin{tikzcd}
X \arrow[loop, "\id_X"', distance=2em, in=305, out=235]
\end{tikzcd}
$\Rightarrow$
\begin{tikzcd}
FX \arrow[d, "F\id_X", shift left] \arrow [d, "\id_{FX}"', shift right] \\
FX
\end{tikzcd}
\hspace{2em}
\begin{tikzcd}
X \arrow[r, "a"] \arrow[dr, "b \comp a"'] & Y \arrow[d, "b"] \\
& Z
\end{tikzcd}
$\Rightarrow$
\begin{tikzcd}
FX \arrow[r, "Fa"] \arrow[dr, "F(b \comp a)"'] & FY \arrow[d, "Fb"] \\
& FZ
\end{tikzcd}
#+end_center
#+end_figure

Because we are interested in functors with multiple parameters, we also need to describe the concept of a *product category*. Given two categories $\cat{C}$ and $\cat{D}$, their product is denoted by $\cat{C} \times \cat{D}$. Objects of this category are ordered pairs $\tuple{C}{D}$, where $C$ is an object of $\cat{C}$ and $D$ is an object of $\cat{D}$, and arrows of this category are ordered pairs $\tuple{f}{g}: \tuple{C}{D} \to~\tuple{C'}{D'}$, where $f: C \to C'$ and $g: D \to D'$ are arrows of $\cat{C}$ and $\cat{D}$, respectively. Identity and composition are defined in the obvious way, the identity arrow being $\id_{\tuple{C}{D}} = \tuple{\id_C}{\id_D}$ and composition being $\tuple{f'}{g'} \comp \tuple{f}{g} = \tuple{f' \comp f}{g' \comp g}$.

A functor $F: \cat{C} \times \cat{D} \to \cat{B}$ can be regarded as a mapping with two arguments, one from $\cat{C}$ and one from $\cat{D}$. For example, we can define the Cartesian product on sets as a functor $\_ \times \_: \Set \times \Set \to \Set$. It takes a pair of two sets, $\tuple{A}{B}$, and maps it to the Cartesian product $A \times B$, which is itself a set. The action on arrows is defined element-wise. When the domain of a functor is not a binary product category, but a more general \(n\)-ary product category, then we talk about \(n\)-ary functors.

In the context of \(n\)-ary functors, we can refer to the property of being *functorial in an argument*. When saying that a functor $F: \cat{C} \times \cat{D} \to \cat{B}$ is /functorial in $\cat{C}$/, we mean that fixing any object $D$ of $\cat{D}$ gives rise to a functor $\hat{F}: \cat{C} \to \cat{B}$, which is defined by its action on objects $C$ of $\cat{C}$ by $\hat{F}C = F\tuple{C}{D}$, and by its action on arrows $a: C \to C'$ by $\hat{F}a = F\tuple{a}{\id_D}: F\tuple{C}{D} \to F\tuple{C'}{D}$. This notion is extended from binary to \(n\)-ary functors, by implying that all arguments other than the one for which we check for functoriality are fixed. A functor is functorial in all its arguments, and conversely a mapping that is functorial in all its arguments is a functor.

An important functor we will use is the *$\Hom$ functor*, $\hom{\_}{\_}: \op{\cat{C}} \times \cat{C} \to \Set$, which to every pair of objects $X$ and $Y$ in $\cat{C}$ assigns their \(\Hom\)-set $\hom{X}{Y}$. Functoriality in the second argument means that for every arrow $a: Y \to Y'$, we can find an arrow $\hom{\id_X}{a}: \hom{X}{Y} \to \hom{X}{Y'}$. Since the objects of $\Set$ are sets, and the arrows are functions on sets, we can show this action on elements of the \(\Hom\)-sets. An element of the set $\hom{X}{Y}$ is an arrow $a: X \to Y$, and we need to transform it into an arrow $\hom{\id_X}{f}: X \to Y$, which is easily done by post-composition with $f$, because for every arrow $a: X \to Y$, there is an arrow $f \comp a: X \to Y'$.

Functoriality in the first argument requires that it be from the opposite category, $\op{\cat{C}}$. Only in this way can we define the action on arrows as pre-composition, analogously to functoriality in the second argument. We know that every arrow $f: X \to X'$ in $\op{\cat{C}}$ is the arrow $f: X' \to X$ in $\cat{C}$. Then, finding the arrow $\hom{f}{\id_Y}: \hom{X}{Y} \to \hom{X'}{Y}$ is as easy as taking an element of the set $\hom{X}{Y}$, which is an arrow $a: X \to Y$ in $\cat{C}$, and pre-composing it with the arrow $f: X' \to X$ in $\cat{C}$, to obtain the arrow $a \comp f: X' \to Y$, which is an element of the set $\hom{X'}{Y}$.

Furthermore, we will work with mappings between functors, called *natural transformations*. Given two categories $\cat{C}$ and $\cat{D}$, and two functors $F, G: \cat{C} \to \cat{D}$, a natural transformation $\varphi: \nat{F}{G}$ consists of a family of arrows $\varphi_C: FC \to GC$ (its *components*), indexed by objects of $\cat{C}$, such that the following diagram commutes for all objects $X$ and $Y$, all arrows $f: X \to Y$ from $\cat{C}$.
#+begin_center
\begin{tikzcd}
FX \arrow[r, "Ff"] \arrow[d, "\varphi_X"] & FY \arrow[d, "\varphi_Y"] \\
GX \arrow[r, "Gf"] & GY
\end{tikzcd}
#+end_center

When all the components of a natural transformation $\varphi$ are isomorphisms, we call $\varphi$ a *natural isomorphism*.

Similarly to functoriality in an argument, we can talk about *naturality in an argument* when \(n\)-ary functors are involved. Given two functors $F, G: \cat{C} \times \cat{D} \to \cat{B}$, a family of arrows $\varphi_{C, D}: F(C, D) \to G(C, D)$, indexed by objects of $\cat{C}$ and $\cat{D}$, is natural in $C$ for a fixed object $D$ in $\cat{D}$ if the family $\varphi_{C, D}: F(C, D) \to G(C, D)$, indexed by /only the objects/ $C$ of $\cat{C}$, is a natural transformation from $F(\_, D): \cat{C} \to \cat{B}$ to $G(\_, D): \cat{C} \to \cat{B}$, that is if the diagram
#+begin_center
\begin{tikzcd}
F\tuple{C}{D} \arrow[r, "F\tuple{f}{\id_D}"] \arrow[d, "\hat{\varphi}_C"] & F\tuple{C'}{D} \arrow[d, "\hat{\varphi}_{C'}"] \\
G\tuple{C}{D} \arrow[r, "G\tuple{f}{\id_D}"] & G\tuple{C'}{D}
\end{tikzcd}
#+end_center
commutes for all arrows $f: C \to C'$ in $\cat{C}$.

Naturality of natural transformations between \(n\)-ary functors is defined in the obvious way, by asserting that fixing all arguments except the one of interest yields a natural transformation. A natural transformation is natural in all its arguments, and a mapping between functors that is natural in all arguments is a natural transformation.

The last tool in the basic categorical toolbox are *adjunctions*. While natural transformations represented a mapping between two parallel functors, an adjunction describes a correspondence between functors going in opposing directions. Given two categories $\cat{C}$ and $\cat{D}$, and functors $F: \cat{C} \to \cat{D}$ and $G: \cat{D} \to \cat{C}$, we say that $F$ is *left adjoint* to $G$ (or equivalently that $G$ is *right adjoint* to $F$) if there is a bijection between \(\Hom\)-sets $\psi_{X, Y}: \hom{FX}{Y} \to \hom{X}{GY}$ for all objects $X$ in $\cat{C}$ and $Y$ in $\cat{D}$, such that it is natural in $X$ and $Y$. This relationship is written $F \dashv G$.

Naturality in $X$ means that for any arrow $a: X' \to X$ in $\cat{C}$, the following diagram
#+begin_center
\begin{tikzcd}
\hom{FX}{Y} \arrow[r, "\hom{Fa}{Y}"] \arrow[d, "\psi_{X, Y}"'] & \hom{FX'}{Y} \arrow[d, "\psi_{X', Y}"] \\
\hom{X}{GY} \arrow[r, "\hom{a}{GY}"] & \hom{X'}{GY}
\end{tikzcd}
#+end_center
commutes, while naturality in $Y$ means that for any arrow $a: Y \to Y'$ in $\cat{D}$, the diagram
#+begin_center
\begin{tikzcd}
\hom{FX}{Y} \arrow[r, "\hom{FX}{a}"] \arrow[d, "\psi_{X, Y}"] & \hom{FX}{Y'} \arrow[d, "\psi_{X, Y'}"] \\
\hom{X}{GY} \arrow[r, "\hom{X}{Ga}"] & \hom{X}{GY'}
\end{tikzcd}
#+end_center
commutes.

There are other, equivalent, definitions of adjoint functors. We present one of them here, because the text to follow uses properties of the definition above as well as the one listed below. We leave out the standard proof that these two definitions are equivalent, referring to \cite{MacLane1998}.

An adjunction is also determined by a functor $F: \cat{C} \to \cat{D}$ and by the existence of "cofree" objects: for every object $D$ in $\cat{D}$, there exists an object $G_0D$ in $\cat{C}$ together with an arrow $\epsilon_D: FG_0D \to D$, such that for every arrow $f: FC \to D$ in $\cat{D}$, there is a unique arrow $f^{\flat}: C \to G_0D$, making the following diagram commute
#+begin_center
\begin{tikzcd}
D & FG_0D \arrow[l, "\epsilon_D"'] \\
& FC \arrow[lu, "f"] \arrow[u, "Ff^{\flat}"']
\end{tikzcd}
#+end_center

** Models of intuitionistic programs

For modeling the simply typed \lambda-calculus, we need to interpret types, contexts and programs of STLC, in a way that the interpretation preserves \beta and \eta equivalences.

The types and contexts are represented by objects in a category, and programs are interpreted as arrows from context objects to target type objects \mdash thus to interpret a contextualized type judgment $\Gamma \vdash t: A$, we first need an object $\CatTrans{\Gamma}$, an object $\CatTrans{A}$, and then an appropriate arrow $\CatTrans{t}: \CatTrans{\Gamma} \to \CatTrans{A}$. Because the semantic brackets hinder readability of diagrams significantly, we choose to use the same name for a meta object in type theory and its categorical interpretation, so the above example will be henceforth written as $t: \Gamma \to A$ or @@latex:\begin{tikzcd}[cramped,column sep=normal]B \arrow[r, "t"] & A\end{tikzcd}@@.

The category for interpreting STLC needs to be equipped with additional structure. We begin by introducing the *product* in a category (which is different from a product of categories), a notion generalizing the Cartesian product from set theory. A product of two objects $X$ and $Y$, usually denoted $X \times Y$, is another object in the same category equipped with two arrows, $\pi_1: X \times Y \to X$ and $\pi_2: X \times Y \to Y$, such that for all other objects $W$ and arrows $p: W \to X$ and $q: W \to Y$, there is a unique arrow $m: W \to X \times Y$, called the *factoring arrow*, satisfying the equations $p = \pi_1 \comp m$ and $q: \pi_2 \comp m$. This property is captured in the commuting diagram
#+begin_center
\begin{tikzcd}
& W \arrow[ld, "p"'] \arrow[rd, "q"] \arrow[d, dashrightarrow, "m"] & \\
X & X \times Y \arrow[l, "\pi_1"] \arrow[r, "\pi_2"'] & Y
\end{tikzcd}
#+end_center
and the arrow $m$ is also denoted $\prodar{p}{q}$.

We focus on categories that have all binary products, which means that the product $X \times Y$ exists for all objects $X$ and $Y$ of the category. We also require presence of the product unit \mdash an object $\One$ in the same category, with the property that for every other object $Z$ in the category, there exists a unique arrow $\One_Z: Z \to \One$. With this property alone, we can show that there is an isomorphism between the objects $X \times \One$ and $X$, since $X$ can be shown to manifest the product property \mdash for every other object $W$, there is only one arrow from it to $\One$, specifically $\One_W$, and then given an arrow $a: W \to X$, there is indeed only one arrow that is equal to $\id \comp a$, and that is $a$ itself.
#+begin_center
\begin{tikzcd}
& W \arrow[ld, "\One_W"'] \arrow[rd, "a"] \arrow[d, dashrightarrow, "a"] & \\
\One & X \arrow[l, "\One_X"] \arrow[r, "\id"] & X
\end{tikzcd}
#+end_center
The symmetric isomorphism $\One \times X \cong X$ is provable in the same manner.

Categories with all binary products come equipped with a *product functor* $\_ \times \_: \cat{C} \times \cat{C} \to \cat{C}$, which to each pair of objects $\tuple{C}{D}$ associates their product $C \times D$, and acts on arrows in the obvious way.

This structure is enough for us to model contexts. Interpretation of the empty context is the unit $\One$, and context concatenation $\Gamma, A$ is interpreted as the product $\CatTrans{\Gamma} \times A$. Since the product is associative, we omit parentheses.

Composition of arrows reflects substitution \mdash two arrows $s: A \to B$ and $t: B \to C$ represent two programs, the first program $x: A \vdash s: B$ possibly having a free variable of type $A$, and the second program $y: B \vdash t: C$ possibly having a free variable of type $B$. The "possibly" indicates that these variables might not appear in the program if they were introduced via weakening. Upon carefully choosing variable names to prevent accidental captures and conflicts, we interpret the substitution $x: A \vdash t[y:=s]: C$ as the composition $t \comp s: A \to C$.

We proceed to give interpretations to the rules of STLC. The identity axiom requires that every object is equipped with an arrow going to itself, and this arrow is guaranteed to exist by the identity arrow property of all categories. Visually, we represent this rule as
$$
\prftree[r]{Id}
{}
{A \xrightarrow{\id_A} A}
$$

The exchange rule is modeled by the symmetry of the product \mdash there is an isomorphism $p_{X, Y}: X \times Y \to Y \times X$ for all objects $X$ and $Y$, satisfying the property $p_{Y, X} \comp p_{X, Y} = \id_{X \times Y}$.
$$
\prftree[r]{Exchange}
{\Gamma \times A \times B \times \Delta \xrightarrow{s} B}
{\Gamma \times B \times A \times \Delta \cong \Gamma \times A \times B \times \Delta \xrightarrow{s} B}
$$

The contraction rule is given interpretation via the arrow $\prodar{\id_A}{\id_A}: A \to A \times A$.
$$
\prftree[r]{Contraction}
{\Gamma \times A \times A \xrightarrow{s} B}
{\Gamma \times A \xrightarrow{\id_\Gamma \times \prodar{\id_A}{\id_A}} \Gamma \times A \times A \xrightarrow{s} B}
$$

We can represent contraction, as well as all other deduction rules, via a natural transformation \cite{Benton1993Re}. We choose to present the premises and the conclusion of a rule with appropriate functors, whose arguments are the parts of the rule that "do not change" during the application of the rule, and which are parameterized by the objects that change. Specifically for the contraction rule, we choose the family of functors $P_A, C_A: \op{\cat{C}} \times \cat{C} \to \Set$ (for /premise functor/ and /conclusion functor/), which are parameterized by the object $A$, and functorial in the objects $\Gamma$ and $B$. The premise functors map the tuple $\tuple{\Gamma}{B}$ to the \(\Hom\)-set $\hom{\Gamma \times A \times A}{B}$, and the conclusion functors map the tuple $\tuple{\Gamma}{B}$ to the \(\Hom\)-set $\hom{\Gamma \times A}{B}$. The family of natural transformations $\varphi^A$ have for components the arrows $\varphi^A_{\Gamma, B}: P_A\tuple{\Gamma}{B} \to C_A\tuple{\Gamma}{B}$.

This formulation gives rise to two naturality diagrams, one for the naturality in $\Gamma$ and one for the naturality in $B$. In all natural transformations describing derivation rules, the naturality in context objects corresponds to the ability of replacing variables with expressions of the same type. We give a worked example for the contraction rule, and we do not describe the naturality in context objects for other rules.

Naturality in $\Gamma$ is expressed by the following diagram
#+begin_center
\begin{tikzcd}[column sep=8em]
\hom{\Gamma \times A \times A}{B} \arrow[r, "P_A\tuple{f}{\id_B}"] \arrow[d, "\varphi^A_{\Gamma, B}"] & \hom{\Gamma' \times A}{B} \arrow[d, "\varphi^A_{\Gamma', B}"] \\
\hom{\Gamma \times A}{B} \arrow[r, "C_A\tuple{f}{\id_B}"] & \hom{\Gamma' \times A}{B}
\end{tikzcd}
#+end_center
Because the components of $\varphi^A$ are arrows in the category $\Set$, we know that they correspond to functions between sets, so we can observe their action on elements of the sets. This action on elements is described in an /element chasing/ diagram, shown in Figure [[fig:ccc_contr]].

#+name: fig:ccc_contr
#+caption: Element chasing for the naturality diagram of \(\varphi^A_{\_, B}\)
#+begin_sidewaysfigure
#+begin_center
\begin{tikzcd}
(\Gamma \times A \times A \xrightarrow{s} B) \arrow[r, mapsto, "P_A\tuple{f}{\id_B}"] \arrow[dd, mapsto, "\varphi^A_{\Gamma, B}"] & (\Gamma' \times A \times A \xrightarrow{f \times \id_{A \times A}} \Gamma \times A \times A \xrightarrow{s} B) \arrow[d, mapsto, "\varphi^A_{\Gamma', B}"] \\
& \Gamma' \times A \xrightarrow{\id_{\Gamma'} \times \prodar{\id_A}{\id_A}} \Gamma' \times A \times A \xrightarrow{f \times \id_{A \times A}} \Gamma \times A \times A \xrightarrow{s} B \arrow[d, phantom, "=" description] \\
(\Gamma \times A \xrightarrow{\id_\Gamma \times \prodar{\id_A}{\id_A}} \Gamma \times A \times A \xrightarrow{s} B) \arrow[r, mapsto, "C_A\tuple{f}{\id_B}"] & (\Gamma' \times A \xrightarrow{f \times \id_A} \Gamma \times A \xrightarrow{\id_\Gamma \times \prodar{\id_A}{\id_A}} \Gamma \times A \times A \xrightarrow{s} B)
\end{tikzcd}
#+end_center
#+end_sidewaysfigure

The arrow in the upper left corner represents a program with possibly three free variables, one of the type $\Gamma$, and two of the type $A$. For explanation's sake we assign names to the variables, but keep in mind that the names can always be chosen to avoid accidental capture and conflicts. We say that the arrow $s$ is an interpretation of a program $u: \Gamma, x: A, y: A \vdash s: B$. Following this program to the right, we pre-compose the arrow $f: \Gamma' \to \Gamma$, which represents a program $w: \Gamma' \vdash f: \Gamma$. Composition corresponds to substitution, so the program represented by the arrow in the upper right is $w: \Gamma', x: A, y: A \vdash s[u:=f]: B$. Chasing the programs vertically, we apply the contraction rule, so the bottom left arrow is the program $u: \Gamma, z: A \vdash s[x:=z][y:=z]$, and the bottom right arrow is the program $w: \Gamma', z: A \vdash (s[u:=f])[x:=z][y:=z]$. However, the importance of the naturality property is that the bottom right program must be the same irrespective of the choice of the path. Transporting the bottom left arrow to the right, we obtain the program $w: \Gamma', z: A \vdash (s[x:=z][y:=z])[u:=f]$, and the equation $(s[u:=f])[x:=z][y:=z] = (s[x:=z][y:=z])[u:=f]$. As mentioned, this equation corresponds to the fact variables in the context can be substituted.

Naturality in $B$ gives rise to a similar equation, namely $(f[w:=s])[x:=z][y:=z] = f[w:=s[x:=z][y:=z]]$, which also holds by appropriately choosing variable names.

Weakening is defined with the projection arrow $\pi_1: \Gamma \times B \to \Gamma$.
$$
\prftree[r]{Weakening}
{\Gamma \xrightarrow{s} A}
{\Gamma \times B \xrightarrow{\pi_1} \Gamma \xrightarrow{s} A}
$$
The premise and conclusion functors $P_B, C_B: \op{\cat{C}} \times \cat{C} \to \Set$ are connected via the natural transformations $\varphi^B: \nat{P_B}{C_B}$, and nothing of note is provided by the naturality.

The existence of all binary products plays the role of product type formation \mdash if $A$ and $B$ are objects in $\cat{C}$, then $A \times B$ is also an object in $\cat{C}$. The product projections give meaning to the $\Elim{\times}$ rules, specifically
$$
\prftree[r]{$\Elim{\times}_1$}
{\Gamma \xrightarrow{s} A \times B}
{\Gamma \xrightarrow{s} A \times B \xrightarrow{\pi_1} A}
\hspace{2em}
\prftree[r]{$\Elim{\times}_2$}
{\Gamma \xrightarrow{s} A \times B}
{\Gamma \xrightarrow{s} A \times B \xrightarrow{\pi_2} A}
$$
the family of functors indexed by $A$ and $B$ being $P_{A,B}, C_{A, B}: \op{\cat{C}} \to \Set$ defined by $P_{A, B}(\Gamma) = \hom{\Gamma}{A \times B}$ and $C_{A, B}(\Gamma) = \hom{\Gamma}{A}$, and analogously for the other component. The functors only take one argument, the context.

The product introduction is formulated in terms of the factoring arrow, with the deduction
$$
\prftree[r]{$\Intro{\times}$}
{\Gamma \xrightarrow{s} A}
{}
{\Gamma \xrightarrow{t} B}
{\Gamma \xrightarrow{\prodar{s}{t}} A \times B}
$$
and functors $P_{A, B}, C_{A, B}: \op{\cat{C}} \to \Set$ defined by $P_{A, B}(\Gamma) = \hom{\Gamma}{A} \times \hom{\Gamma}{B}$ and $C_{A, B}(\Gamma) = \hom{\Gamma}{A \times B}$.

Equations are modeled with commutative diagrams. The \beta-reductions correspond exactly to the product property
#+begin_center
\begin{tikzcd}
& \Gamma \arrow[ld, "s"'] \arrow[d, "\prodar{s}{t}"] \arrow[rd, "t"] \\
A & A \times B \arrow[l, "\pi_1"] \arrow[r, "\pi_2"] & B
\end{tikzcd}
#+end_center
with the left triangle describing the equation $\fst{\tuple{s}{t}} \equiv s$ and the right triangle describing the equation $\snd{\tuple{s}{t}} \equiv t$.

The \eta-conversion corresponds to the diagram
#+begin_center
\begin{tikzcd}
A \times B \arrow[d, "\pi_1"] & \Gamma \arrow[l, "s"'] \arrow[r, "s"] \arrow[d, dashed, "\prodar{\pi_1 \comp s}{\pi_2 \comp s}"] & A \times B \arrow[d, "\pi_2"] \\
A & A \times B \arrow[l, "\pi_1"] \arrow[r, "\pi_2"] & B
\end{tikzcd}
#+end_center
Because of the product property, the dashed arrow is a unique arrow that makes the two squares commute. However, the arrow $s$ also makes the squares commute, so the two arrows must be equal, making the terms they represent, $s$ and $\tuple{\fst{s}}{\snd{s}}$, equivalent.

To model function types, we introduce another categorical concept \mdash the *exponential*. The exponential of two objects $C$ and $D$ in a category $\cat{C}$ is another object in the same category, denoted $D^C$, equipped with an arrow $\epsilon: D^C \times C \to D$ (called the *evaluator*), such that for all other objects $W$ and arrows $h: W \times C \to D$, there is a unique arrow $h^{\flat}: W \to D^C$, satisfying the equation $\epsilon \comp (h^{\flat} \times \id_C) = h$. This property is captured in the commuting diagram
#+begin_center
\begin{tikzcd}
D & D^C \times C \arrow[l, "\epsilon"'] \\
& W \times C \arrow[lu, "h"] \arrow[u, "h^{\flat} \times \id"']
\end{tikzcd}
\hspace{2em}
\begin{tikzcd}
D^C \\
W \arrow[u, dashed, "h^{\flat}"]
\end{tikzcd}
#+end_center
If a category has an exponential $D^C$ for all objects $C$ and $D$, then it gives rise to a functor $\_^-: \op{\cat{C}} \times \cat{C} \to \cat{C}$, and the property corresponds to an adjunction with the product functor \mdash for every object $C$ in $\cat{C}$, the functor $\_ \times C$ is left adjoint to the functor $\_^C$. In other words, there is a bijection between the \(\Hom\)-sets $\hom{B \times C}{D}$ and $\hom{B}{D^C}$.

A category equipped with all binary products, a unit object for the product, and all exponentials, is called a *Cartesian closed category*. This is all the structure necessary for interpreting STLC, which we show by interpreting rules and equations for the function type via exponentials.

The $\Intro{\to}$ rule takes advantage of the bijection of \(\Hom\)-sets
$$
\prftree[r]{$\Intro{\to}$}
{\Gamma \times A \xrightarrow{s} B}
{\Gamma \xrightarrow{s^{\flat}} B^A}
$$
with the natural transformation between $P_{A,B}(\Gamma) = \hom{\Gamma \times A}{B}$ and $C_{A, B}(\Gamma) = \hom{\Gamma}{B^A}$ being the natural isomorphism with \(\Hom\)-set bijections for components.

The $\Elim{\to}$ rule uses the $\epsilon$ evaluator
$$
\prftree[r]{$\Elim{\to}$}
{\Gamma \xrightarrow{s} B^A}
{}
{\Delta \xrightarrow{t} A}
{\Gamma \times \Delta \xrightarrow{s \times t} B^A \times A \xrightarrow{\epsilon} B}
$$
with the obvious premise and conclusion functors taking for arguments $\Gamma$ and $\Delta$.

The \beta-reduction equation is expressed via the commutative diagram
#+begin_center
\begin{tikzcd}
\Gamma \times \Delta \arrow[r, "\id \times t"] \arrow[d, "s^{\flat} \times t"'] & \Gamma \times A \arrow[d, "s"] \\
B^A \times A \arrow[r, "\epsilon"'] & B
\end{tikzcd}
#+end_center
where we decompose the arrow $\Gamma \times \Delta \xrightarrow{s^{\flat} \times t} B^A \times A$ as $\Gamma \times \Delta \xrightarrow{\id \times t} \Gamma \times A \xrightarrow{s^{\flat} \times \id}$ and factor out $\id \times t$ to obtain the equivalent diagram
#+begin_center
\begin{tikzcd}
\Gamma \times \Delta \arrow[r, "\id \times t"] & \Gamma \times A \arrow[rd, "s"] \arrow[d, "s^{\flat} \times \id"'] \\
& B^A \times A \arrow[r, "\epsilon"'] & B
\end{tikzcd}
#+end_center
which commutes by definition, because the triangle is exactly the one described in the adjunction property.

For the \eta-conversion, we demand that the arrow $f: \Gamma \to B^A$ be equal to the arrow $(\epsilon \comp (f \times \id))^{\flat}: \Gamma \to B^A$. Writing out the adjunction triangle for the arrow $\epsilon \comp (f \times \id): \Gamma \times A \to B$, we obtain
#+begin_center
\begin{tikzcd}
B & B^A \times A \arrow[l, "\epsilon"'] \\
B^A \times A \arrow[u, "\epsilon"] & \Gamma \times A \arrow[u, "(\epsilon \comp (f \times \id))^{\flat} \times \id"'] \arrow[l, "f \times \id"]
\end{tikzcd}
#+end_center
By the adjunction property, the arrow $(\epsilon \comp (f \times \id))^{\flat}: \Gamma \to B^A$ is the unique arrow that makes the square commute. But since we can replace it with the arrow $f$ and obtain a commuting diagram, we conclude that the two arrows are equal.

As such, we are able to interpret types, programs and equations of the simply typed \lambda-calculus in Cartesian closed categories.

** Models of linear programs

When modeling linear programs, we limit ourselves to programs containing only linear variables \mdash that is, we do not attempt to give interpretations to the $\Intro{\bang}$ and $\Elim{\bang}$ rules, which only make sense in the presence of intuitionistic variables, as the difficulty of this problem is far beyond the level of a bachelor's thesis. An interested reader may take a look at the survey \cite{DePaiva2014}.

We stated multiple times that the intuitionistic product corresponds to the linear $\with$ product. However, modeling linear contexts as $\with$ products is not desirable, because that would require unnecessary structure \mdash specifically, there is no point in requiring arrows $\pi_1: A \times B \to A$ for all objects $A$ and $B$ when we cannot produce well-typed programs $\Lin{x: A}, \Lin{y: B} \vdash s: A$ for all types $A$ and $B$, and analogously for the second component. Instead, we introduce a more general concept of a *symmetrically monoidal category*, which is used for modeling context concatenation and the $\tens$ product.

A symmetrically monoidal category is a category $\cat{C}$ with a symmetric monoidal structure, represented by a functor $\tens: \cat{C} \times \cat{C} \to \cat{C}$, called the *tensor product*, a specific object $\One_{\tens}$ from $\cat{C}$, called the *unit object*, four natural isomorphisms and four kinds of commuting diagrams, listed below.

The first natural isomorphism is the *associator* $\assoc{X}{Y}{Z}: (X \tens Y) \tens Z \to X \tens (Y \tens Z)$, which represents associativity of the tensor product. Second and third are the *left* and *right unitor*, written $\lambda_X: \One_{\tens} \tens X \to X$ and $\rho_X: X \tens \One_{\tens} \to X$, respectively. These represent the fact that $\One_{\tens}$ is the left and right unit of the tensor product. The fourth isomorphism is the *braiding* $B_{X, Y}: X \tens Y \to Y \tens X$, which provides the symmetry of the tensor product.

The commuting diagrams enforce that the natural isomorphisms obey our intuitive expectations. The first one is the pentagon identity
#+begin_center
\begin{tikzcd}
((X \tens Y) \tens Z) \tens W \arrow[r, "\assoc{X \tens Y}{Z}{W}"] \arrow[d, "\assoc{X}{Y}{Z} \tens \id"] & (X \tens Y) \tens (Z \tens W) \arrow [r, "\assoc{X}{Y}{Z \tens W}"] & X \tens (Y \tens (Z \tens W)) \\
(X \tens (Y \tens Z)) \tens W \arrow [rr, "\assoc{X}{Y \tens Z}{W}"] & & X \tens ((Y \tens Z) \tens W) \arrow[u, "\id \tens \assoc{Y}{Z}{W}"]
\end{tikzcd}
#+end_center
followed by the triangle identity
#+begin_center
\begin{tikzcd}
(X \tens \One_{\tens}) \tens Y \arrow[rr, "\assoc{X}{\One_{\tens}}{Y}"] \arrow[rd, "\rho_X \tens \id"'] & & X \tens (\One_{\tens} \tens Y) \arrow[ld, "\id \tens \lambda_Y"] \\
& X \tens Y &
\end{tikzcd}
#+end_center
then the hexagon identity
#+begin_center
\begin{tikzcd}
(X \tens Y) \tens Z \arrow[r, "\assoc{X}{Y}{Z}"] \arrow[d, "B_{X, Y} \times Z"] & X \tens (Y \tens Z) \arrow[r, "B_{X, Y \tens Z}"] & (Y \tens Z) \tens X \arrow[d, "\assoc{Y}{Z}{X}"] \\
(Y \tens X) \tens Z \arrow[r, "\assoc{Y}{X}{Z}"] & Y \tens (X \tens Z) \arrow[r, "\id \tens B_{X, Z}"] & Y \tens (Z \tens X)
\end{tikzcd}
#+end_center
and finally the symmetry condition
#+begin_center
\begin{tikzcd}
X \tens Y \arrow[r, "B_{X, Y}"] \arrow[rd, "\id"'] & Y \tens X \arrow[d, "B_{Y, X}"] \\
& X \tens Y
\end{tikzcd}
#+end_center
holding for all objects $X$, $Y$, $Z$ and $W$ of $\cat{C}$.

This structure gives us enough power to express both context concatenation in LLC, in the same way that Cartesian products interpreted context concatenation in STLC, and the $\tens$ product.

We begin with the linear identity axiom, which is interpreted by the identity arrow
$$
\prftree[r]{$\Lin{\mathrm{Id}}$}
{A \xrightarrow{\id_A} A}
$$

Programs with only linear variables are closed under substitution, because there is no risk of performing a substitution resulting in a linear variable being used multiple times \mdash that would require substituting for intuitionistic variables, which are not present. Therefore, we can interpret substitution with composition, where the program $t[x:=s]$ is composed of a term $t: B$ with a free linear variable $x: A$ and possibly others, and a term $s: A$ with possibly free linear variables. The interpretation of $t$ is $\Gamma \tens A \tens \Delta \xrightarrow{t} B$, the program $s$ is interpreted as $\Theta \xrightarrow{s} A$, and their composition is $\Gamma \tens \Theta \tens \Delta \xrightarrow{\id_\Gamma \tens s \tens \id_\Delta} \Gamma \tens A \tens \Delta \xrightarrow{t} B$.

From the structural rules, only exchange can be interpreted without intuitionistic variables, and it is satisfied by the symmetry of the tensor product, provided by the isomorphism $B_{A, B}$
$$
\prftree[r]{Exchange}
{\Gamma \tens A \tens B \tens \Delta \xrightarrow{s} C}
{\Gamma \tens B \tens A \tens \Delta \underset{B_{A, B}}{\cong} \Gamma \tens A \tens B \tens \Delta \xrightarrow{s} C}
$$

The tensor product functor is used also as interpretation for the $\tens$ product. The $\Intro{\tens}$ rule is obvious from the functorial structure
$$
\prftree[r]{$\Intro{\tens}$}
{\Gamma \xrightarrow{s} A}
{\Delta \xrightarrow{t} B}
{\Gamma \tens \Delta \xrightarrow{s \tens t} A \tens B}
$$
where the premise functor $P_{A, B}: \op{\cat{C}} \times \op{\cat{C}} \to \Set$ is given by $P_{A, B}\tuple{\Gamma}{\Delta} = \hom{\Gamma}{A} \times \hom{\Delta}{B}$ and the conclusion functor is given by $C_{A, B}\tuple{\Gamma}{\Delta} = \hom{\Gamma \tens \Delta}{A \tens B}$. Naturality of the transformation from $P_{A, B}$ to $C_{A, B}$ again corresponds to the ability to do substitution in the contexts $\Gamma$ and $\Delta$.

The $\Elim{\tens}$ rule has a more interesting structure \mdash for once, the premise and conclusion functors take more than context arguments
$$
\prftree[r]{$\Elim{\tens}$}
{\Gamma \tens A \tens B \xrightarrow{s} C}
{\Delta \xrightarrow{t} A \tens B}
{\Gamma \tens \Delta \xrightarrow{\id_\Gamma \tens t} \Gamma \tens A \xrightarrow{s} C}
$$
The corresponding functors take $C$ as one of their parameters \mdash the premise functor is defined as $P_{A, B}: \op{\cat{C}} \times \op{\cat{C}} \times \cat{C} \to \Set$ by $P_{A, B}(\Gamma, \Delta, C) = \hom{\Gamma \tens A \tens B}{C} \times \hom{\Delta}{A \tens B}$, and the conclusion functor is $C_{A, B}(\Gamma, \Delta, C) = \hom{\Gamma \tens \Delta}{C}$. Chasing an element of $P_{A, B}(\Gamma, \Delta, C)$ through the diagram of naturality in $C$, we obtain the equation $\tenscase{t}{x}{y}{f[u:=s]} \equiv f[u:=\tenscase{t}{x}{y}{s}]$, which is one of the commuting conversions. Note that this substitution is performed only on programs that only contain linear variables, so the substitution corresponds to filling a term-with-holes that was used without the constructs $\bangcase{C[\_]}{x}{s}$ and $\bangcase{s}{x}{C[\_]}$. Therefore naturality in $C$ recovers the commuting conversion.

For interpreting linear functions, we need a similar structure to exponents in Cartesian closed categories. There, the exponent functors are right adjoint to the Cartesian product, because that is how context concatenation in STLC is interpreted. In LLC, context concatenation is interpreted with the tensor product, so we need a right adjoint of that.

We require a functor $\_ \loli \_: \op{\cat{C}} \times \cat{C} \to \cat{C}$, which for any object $A$ in $\cat{C}$ gives a right adjoint to the tensor product, written $\_ \tens A \dashv A \loli \_$.

Then, the $\loli$ rules and equations are interpreted exactly the same as the $\to$ rules and equations, except the Cartesian product is replaced with the tensor product. This is possible because none of the $\to$ rules and equations considered the projection arrows, instead they are purely based on the adjunction structure. For example, the $\Intro{\loli}$ rule is interpreted by the rule
$$
\prftree[r]{$\Intro{\loli}$}
{\Gamma \tens A \xrightarrow{s} B}
{\Gamma \xrightarrow{s^{\flat}} (A \loli B)}
$$

A symmetric monoidal category with the $\loli$ functor described above is called a *symmetric monoidal closed category*, and its structure reflects the multiplicative fragment of intuitionistic linear logic \mdash that is only the proofs (or programs) consisting of linear variables and the $\tens$ and $\loli$ types.

If we additionally equip the category with a Cartesian product, and include the interpretation of the $\times$ product from STLC, we obtain a Cartesian symmetric monoidal closed category, which is able to interpret all programs from LLC that only use linear variables.

#+begin_export latex
\chapter*{References}
\renewcommand{\refname}{}
\bibliographystyle{apacite}
\bibliography{ComputationalTrinitarianism}
#+end_export

* COMMENT Topic

Computational trinitarianism describes the intimate relationship between logic, category theory and type theory. This relationship identifies propositions of a logic with a type of a corresponding type system, and also establishes a correspondence between a proof of a proposition, a term (program) of a given type, and a generalized element of an object in a category.
A linear type system is a special kind of a substructural type system with important applications in computer science. An advantage of a linear type system resides in its ability to place constraints on the usage of (or access to) variables (resources).
The aim of the bachelor thesis is to describe linear logic as an example of a substructural logic, to construct a linear type system stemming from that logic, and to give their categorical semantics via categories with structure.
The style and presentation of the thesis will be theoretical.


* Footnotes

[fn:4] Barber and other authors call this concept "contexts", "term contexts" or "contexts-with-holes", but we prefer terms-with-holes to avoid overloading the word "context" 

[fn:3] Or /a generator of/, or /an infinite pocket of/

[fn:2] The judgment /$A$ prop/ (and subsequently /$A$ type/) is used more frequently in predicate logic and dependent type theories, which are out of scope for this thesis. The well-formed propositions of the relevant fragment can be described more easily with a simple grammar.

[fn:1] The notation is borrowed from Gentzen's other proof calculus, the sequent calculus. To prevent confusion of the two systems, we prefer the term /contextualized judgment/ to Gentzen's /sequent/.

# Local Variables:
# org-latex-classes: (("book" "\\documentclass[11pt,twoside,a4paper]{book}" ("\\chapter{%s}" . "\\chapter*{%s}") ("\\section{%s}" . "\\section*{%s}") ("\\paragraph{%s}" . "\\paragraph*{%s}")))
# End:
