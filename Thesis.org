#+OPTIONS: toc:nil ':t

#+latex_class: book
#+latex_header: \usepackage{fontspec}
#+latex_header: \usepackage{prftree}
#+latex_header: \usepackage{stmaryrd}
#+latex_header: \usepackage{mathtools}
#+latex_header: \usepackage{tikz-cd}
#+latex_header: \tikzcdset{every label/.append style={font=\small}}
#+latex_header: \tikzcdset{every diagram/.append style={row sep={4em}, column sep={4em}}}
#+latex_header: \usepackage{apacite}
#+latex_header: \usepackage{fancyhdr}
#+latex_header: \usepackage[english]{babel}
#+latex_header: \usepackage{./thesis_template/k336_thesis_macros}

# Binary or
#+latex_header: \newcommand{\binor}{\mathbin{|}}

# Introduction rule
#+latex_header: \newcommand{\Intro}[1]{#1\mathrm{I}}
# Elimination rule
#+latex_header: \newcommand{\Elim}[1]{#1\mathrm{E}}

# Proof reduction
#+latex_header: \newcommand{\prfRed}[1][1em]{\hspace{#1}\Rightarrow\hspace{#1}}
#+latex_header: \newcommand{\prfEq}[1][1em]{\hspace{#1}=\hspace{#1}}

# Linear assumption
#+latex_header: \newcommand{\Lin}[1]{\langle#1\rangle}
# Intuitionistic assumption
#+latex_header: \newcommand{\Int}[1]{[#1]}

# Tensor
#+latex_header: \newcommand{\tens}{\mathbin{\otimes}}
# With
#+latex_header: \newcommand{\with}{\mathbin{\&}}
# Lollipop
#+latex_header: \newcommand{\loli}{\multimap}
# Linear translation
#+latex_header: \newcommand{\LinTrans}[1]{\left\llbracket #1 \right\rrbracket_L}

# Product type
#+latex_header: \newcommand{\tuple}[2]{\left( #1, #2 \right)}
#+latex_header: \DeclareMathOperator{\Fst}{\mathsf{fst}}
#+latex_header: \DeclareMathOperator{\Snd}{\mathsf{snd}}
#+latex_header: \newcommand{\fst}[1]{\left(\Fst #1 \right)}
#+latex_header: \newcommand{\snd}[1]{\left(\Snd #1 \right)}

# Tensor Tuple
#+latex_header: \newcommand{\tenstup}[2]{\left| #1, #2 \right|}
#+latex_header: \DeclareMathOperator{\Case}{\mathsf{case}}
#+latex_header: \DeclareMathOperator{\Of}{\mathsf{of}}
#+latex_header: \DeclareMathOperator{\In}{\mathsf{in}}
#+latex_header: \newcommand{\tenscase}[4]{\left(\Case #1 \Of \tenstup{#2}{#3} \In #4 \right)}
# With Tuple
#+latex_header: \newcommand{\bang}{{!}}
#+latex_header: \newcommand{\bangcase}[3]{\left(\Case #1 \Of \bang{#2} \In #3 \right)}

# Categories
# Axioms
#+latex_header: \DeclareMathOperator{\Src}{\mathsf{src}}
#+latex_header: \DeclareMathOperator{\Tar}{\mathsf{tar}}
#+latex_header: \DeclareMathOperator{\Hom}{\mathsf{hom}}
#+latex_header: \DeclareMathOperator{\Id}{\mathsf{id}}
#+latex_header: \DeclareMathOperator{\Comp}{\circ}
#+latex_header: \newcommand{\src}[1]{\Src(#1)}
#+latex_header: \newcommand{\tar}[1]{\Tar(#1)}
#+latex_header: \renewcommand{\hom}[2]{\Hom(#1, #2)}
#+latex_header: \newcommand{\id}[0]{\Id}
#+latex_header: \newcommand{\comp}[0]{\Comp}
#+latex_header: \newcommand{\cat}[1]{\mathcal{#1}}

# Examples
#+latex_header: \newcommand{\Set}{\mathbf{Set}}
#+latex_header: \newcommand{\Grp}{\mathbf{Grp}}

# Natural transformations
#+latex_header: \newcommand{\nat}[2]{#1 \Rightarrow #2}

#+latex_header: \newcommand\WorkTitle{Computational trinitarianism and Linear types}
#+latex_header: \newcommand\FirstandFamilyName{Vojtěch Štěpančík}
#+latex_header: \newcommand\Supervisor{Ing. Matěj Dostál, Ph.D.}
#+latex_header: \newcommand\TypeOfWork{Bachelor's Thesis}
#+latex_header: \newcommand\StudProgram{Otevřená informatika, Bakalářský}
#+latex_header: \newcommand\StudBranch{Software}

#+begin_export latex
\graphicspath{{thesis_template/}}
\selectlanguage{english}
\translate
\coverpagestarts
\acknowledgements
...
\declaration{In Prague on ... ... 2021}
\abstractpage
\vglue60mm
\noindent{\Huge \textbf{Abstrakt}}

\tableofcontents
\listoffigures
\mainbodystarts
#+end_export

* COMMENT Topic

Computational trinitarianism describes the intimate relationship between logic, category theory and type theory. This relationship identifies propositions of a logic with a type of a corresponding type system, and also establishes a correspondence between a proof of a proposition, a term (program) of a given type, and a generalized element of an object in a category.
A linear type system is a special kind of a substructural type system with important applications in computer science. An advantage of a linear type system resides in its ability to place constraints on the usage of (or access to) variables (resources).
The aim of the bachelor thesis is to describe linear logic as an example of a substructural logic, to construct a linear type system stemming from that logic, and to give their categorical semantics via categories with structure.
The style and presentation of the thesis will be theoretical.

* TODO Introduction

* Logic

Mathematical logic is logic treated by mathematical methods. However, such studies of different kinds of logic often use logical and deductive thinking themselves. To separate the logic observed from the logic used to make the observations, we consider them to be two separate systems. The language of the logic studied is referred to as the *object language*, while the language of the logic used for doing the observing is called the *metalanguage* \cite{Kleene1966}.

These metalanguages are then used to reason about formal composition of proofs \mdash therefore we call them *proof systems*, or *proof calculi*.

The proof system used in this paper stems from Gentzen's natural deduction \cite{Gentzen1935}. Natural deduction builds proofs on *judgements* and *propositions*.

A proposition is a formula of the object language, and a judgement is a knowable fact. For example in traditional logic (that is to say, a /truth-oriented/ logic), one might take "It is raining today" for a proposition $A$, and a judgement is the statement /$A$ is true/, or /$A$ true/ for short.

Another judgment that often arises in various logics is identifying propositions themselves \mdash one can only make judgments about a proposition $A$ if $A$ is a proposition, which is represented by the judgment /$A$ is a proposition/, abbreviated to /$A$ prop/.

We will later see that, without delving into the philosophy of mathematics, the exact nature of propositions and judgements depends on the object language.

The basis for the metalanguage is the *deduction rules*. A deduction rule consists of a collection of judgements, called the *premises*, and a single judgement, called the *conclusion*. To be able to refer to the rule in proofs, it is assigned a semantically significant name. Graphically, it is represented by drawing a horizontal line (the *derivation line*), placing the premises above it, the conclusion below, and writing the name of the rule to the right.

To illustrate, if we wanted to show the rule expressing that given two propositions, $A$ and $B$, and the judgements /$A$ true/ and /$B$ true/, one can obtain the judgement /$A \land B$ true/, we could write it as

$$
\prftree[r]{$\Intro{\land}$}
 {A\ true}
 {A\ prop}
 {B\ true}
 {B\ prop}
 {(A \land B)\ true}
$$
where the label $\Intro{\land}$ is an abbreviation for "conjunction introduction".

Gentzen used the concept of assumptions to formulate the rules for implication. If, given that /$A$ true/, we could sequence the deduction rules in such a way that we get the judgement /$B$ true/, we can abstract this dependency on a hypothetical $A$ into an implication. Gentzen used $[A]$ to denote the *assumption* of the judgement /$A$ true/, and this assumption needs to be later *discharged* by abstracting it into an appropriate implication via a corresponding implication introduction. The formulation of the $\Intro{\to}$ rule can be seen in Figure\nbsp[[fig:localized_hyp]]\nbsp(left). The symbol $\vdots$ stands for a sequence of deduction rules that can derive the judgement /$B$ true/ from the judgement /$A$ true/.

A proof in natural deduction is tree-like, with the judgement to be proven at the root, assumptions at the leaves, and deduction rules between the nodes. It is not a proper tree, because it needs to keep track of which implication introductions discharge which assumptions, so additional structure to manage backreferences is necessary.

In this notation, assumptions are /global/ to the proof. We can change the notation to be able to reason about assumptions locally, allowing us to degenerate the proof structure to a proper tree. We say that a *contextualized judgement*[fn:1] has the form $\Gamma \vdash J$, where \Gamma is a sequence of zero or more assumptions, called the *context*, and $J$ is the judgement. An example of rewriting a proof from Gentzen's notation to the context notation is shown in Figure\nbsp[[fig:localized_hyp]]. Note that the context can also be empty. Assumptions can be added to the context via a comma: $\Gamma, S$ is a new context, which includes all the assumptions from \Gamma, and the assumption $S$. This concatenation is extended in the obvious way to merging of two contexts, so $\Gamma, \Delta$ is a context that includes all the assumptions from \Gamma, and all the assumptions from \Delta \cite{Pfenning2004}. In this new notation, deduction rules have contextualized judgements for premises and conclusion.

#+name: fig:localized_hyp
#+caption: Gentzen's assumption notation (left) and notation for localized assumptions (right)
#+begin_figure
$$
\prftree[r]{$(\Intro{\to})_{\prfref<A>}$}
 {\prfsummary
   {\prfboundedassumption<A>{A}}
   {B}}
 {A \to B}
\hspace{2em}
\prftree[r]{$\Intro{\to}$}
 {A\ prop, A\ true, B\ prop \vdash B\ true}
 {A\ prop, B\ prop \vdash (A \to B)\ true}
$$
#+end_figure

The behaviour of the context is specified in the metalanguage, using deduction rules. These rules are called *structural rules*, and usually include Weakening, Contraction, and Exchange, which are listed in Figure\nbsp[[fig:structural]]. These three rules encode semantics similar to those of a finite set.

Weakening allows one to add arbitrary assumptions to the context without invalidating the derived judgement. Contraction states that assumptions may be used multiple times. Exchange asserts that the order in which assumptions appear in the context is irrelevant.

A logic which constrains one or more of these structural rules is called *substructural* \cite{Paoli2013}.

#+name: fig:structural
#+caption: Structural rules
#+begin_figure
$$
\prftree[r]{Weakening}
 {\Gamma \vdash A\ true}
 {\Gamma, B\ true \vdash A\ true}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, A\ true, A\ true \vdash B\ true}
 {\Gamma, A\ true \vdash B\ true}
$$

$$
\prftree[r]{Exchange}
 {\Gamma, A\ true, B\ true, \Delta \vdash C\ true}
 {\Gamma, B\ true, A\ true, \Delta \vdash C\ true}
$$
#+end_figure

Apart from the structural rules, the logic also specifies *logical rules*. These describe how the logical connectives participate in derivations. Conventionally, they come in pairs of introduction and elimination, the former defining how a proposition containing the connective is created, and the latter defining how such a proposition is "used" and split apart.

Just as there can be zero assumptions in a contextualized judgement, there can be zero premises in a deduction rule. Such rules are called *axioms*, and the judgments in their conclusions are always derivable.

A proof in this updated notation is now a proper tree, with a contextualized judgement at the root, contextualized judgements in the inner nodes, axioms at the leaves, and deduction rules connecting the nodes.

When composing deductions, we sometimes produce redundancies. Namely when a rule for introducing a connective is immediately followed by a rule for eliminating it, the proof can be simplified via rewriting rules called *proof-reductions*. These rules must preserve well-formedness of the proof, meaning that the proof after a reduction must still consist only of derivations specified for the logic. This condition is called /local soundness/ \cite{Pfenning2004}.

** Intuitionistic logic

Intuitionistic logic is the logic of constructive mathematics \mdash the only axiom in the system is $A\ true \vdash A\ true$, in other words, any judgement can be made assuming itself. This is in contrast with classical logic, which also axiomatizes the law of excluded middle, $\vdash (A \lor \lnot A)\ true$. The philosophical difference between classical and intuitionistic logic is that classical logic is content with knowing whether a proposition is true or whether it is false. After all, those are the only options. Intuitionistic logic, on the other hand, requires a constructive proof \mdash a "recipe", turning the assumptions into the conclusion. The law of excluded middle allows for proofs where one judges a proposition to be true, just because it cannot be false. This goes against the intuitionistic line of reasoning, because merely showing that something has to exists doesn't provide the mathematician with a way to construct it. In intuitionistic logic, the judgement /$(A \lor \lnot A)$ true/ can still be made, but it needs to be accompanied with either a proof of /$A$ true/ or /$\lnot A$ true/ \cite{Sorensen2006}.

Since intuitionistic logic is an example of a traditional logic, the basic judgement that can be made about a proposition stays the same, /$A$ true/. Because this is the only judgment we will be using in the proofs[fn:2], we define a shorthand notation, $\Gamma \vdash_T A$, where \Gamma is a list of /propositions/, and $A$ is a proposition, and we take it to mean the contextualized judgment where the context is a list of judgments /$P$ true/ for every proposition $P$ in \Gamma, and where the conclusion is the judgment /$A$ true/ (the index $T$ stands for "truth"). For example, the formula $A, B \vdash_T C$ is short for $A\ true, B\ true \vdash C\ true$. This notation will be used exclusively in the diagrams to prevent them from spreading too wide, and we will use the full form in the body of the thesis.

The logic studied in this section is the meet-implicative fragment of propositional intuitionistic logic \mdash that is to say, we only concern ourselves with propositions created using the connectives $\land$ and $\to$. The propositions of this fragment can be described by the following Backus-Naur form:
$$
A, B ::= X \binor (A \to B) \binor (A \land B)
$$
for X ranging over atomic propositions. The rules of this fragment are given in Figure\nbsp[[fig:intuit_deduct]].

#+name: fig:intuit_deduct
#+caption: Deduction rules for the meet-implicative fragment of propositional intuitionistic logic
#+begin_figure
$$
\prftree[r]{Id}
 {A \vdash_T A}
\hspace{2em}
\prftree[r]{Weakening}
 {\Gamma \vdash_T A}
 {\Gamma, B \vdash_T A}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, A, A \vdash_T B}
 {\Gamma, A \vdash_T B}
\hspace{2em}
\prftree[r]{Exchange}
 {\Gamma, A, B, \Delta \vdash_T C}
 {\Gamma, B, A, \Delta \vdash_T C}
$$

$$
\prftree[r]{$\Intro{\land}$}
 {\Gamma \vdash_T A}
 {}
 {\Gamma \vdash_T B}
 {\Gamma \vdash_T A \land B}
$$

$$
\prftree[r]{$\Elim{\land}_1$}
 {\Gamma \vdash_T A \land B}
 {\Gamma \vdash_T A}
\hspace{2em}
\prftree[r]{$\Elim{\land}_2$}
 {\Gamma \vdash_T A \land B}
 {\Gamma \vdash_T B}
$$

$$
\prftree[r]{$\Intro{\to}$}
 {\Gamma, A \vdash_T B}
 {\Gamma \vdash_T A \to B}
\hspace{2em}
\prftree[r]{$\Elim{\to}$}
 {\Gamma \vdash_T A \to B}
 {}
 {\Delta \vdash_T A}
 {\Gamma, \Delta \vdash_T B}
$$
#+end_figure

The rules consist of the one axiom Id mentioned above, the three structural rules, Weakening, Contraction, and Exchange, and introduction and elimination rules for the two connectives, $\Intro{\land}$, $\Elim{\land}_1$, $\Elim{\land}_2$, $\Intro{\to}$ and $\Elim{\to}$.

/Conjunction introduction/, labeled $\Intro{\land}$ in the deduction rules, states that given a proof of /$A$ true/ and a proof of /$B$ true/, the two proofs combined give a proof of /$(A \land B$) true/. The respective elimination rules allow one to extract one of the proofs of /$A$ true/ or /$B$ true/ from /$(A \land B)$ true/, even after they were combined.

When formulating the proof reduction rule for a particular connective, one needs to look at a generic example of a reducible proof. For sequencing a conjunction introduction and a conjunction elimination, we need to represent generic proofs of the premises, then apply the two rules in succession, and finally justify an alternative path to reach the conclusion. We can represent the generic proofs with the symbol $\vdots$, much like how Gentzen formulated assumptions. For the conjunction reduction, the generic schema would look like the following tree, with the subproofs labeled $s$ and $t$.
$$
\prftree[r]{$\Elim{\land}_1$}
 {\prftree[r]{$\Intro{\land}$}
   {\prfsummary[$s$]{\Gamma \vdash_T A}}
   {}
   {\prfsummary[$t$]{\Gamma \vdash_T B}}
   {\Gamma \vdash_T A \land B}}
 {\Gamma \vdash_T A}
$$

It is easy to see that the conclusion $\Gamma \vdash A\ true$ could have been reached earlier with the $s$ subproof. The full rule is shown in Figure\nbsp[[fig:intuit_conj_red]]. The rule for the other elimination rule is not shown, as it is trivially symmetrical.

#+name: fig:intuit_conj_red
#+caption: Conjunction proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\land}_1$}
 {\prftree[r]{$\Intro{\land}$}
   {\prfsummary[$s$]{\Gamma \vdash_T A}}
   {}
   {\prfsummary[$t$]{\Gamma \vdash_T B}}
   {\Gamma \vdash_T A \land B}}
 {\Gamma \vdash_T A}}
\prfRed
\vcenter{\prfsummary[$s$]{\Gamma \vdash_T A}}
$$
#+end_figure

/Implication introduction/, labeled $\Intro{\to}$, once again builds on abstracting away an assumption. If a judgement /$B$ true/ can be made under an assumption /$A$ true/, then the proof tree can be seen as a way of turning a proof of /$A$ true/ (or multiple proofs of /$A$ true/) into a proof of /$B$ true/. The implication elimination is then a method for providing such a proof of $A$.

The proof reduction rule must take into account that the judgment /\(A\)\nbsp{}true/ might have been assumed zero or multiple times in the proof of /$B$ true/, and the context later modified with contractions or weakenings to reach the contextualized judgment $\Gamma, A\ true \vdash B\ true$. Every assumption of /$A$ true/ that is used in the proof must have been introduced by the identity rule, and the ones that aren't used were introduced by weakening. As shown in \cite{Wadler1993}, applications of structural and logic rules commute, so for every proof where contraction and weakening are used, there is an equivalent proof with all the contractions and weakenings pushed to the root of the proof tree. In other words, for every proof of $\Gamma, J_1 \vdash J_2$, where $J_1$ and $J_2$ stand for arbitrary judgments, there is an equivalent proof which consists of a contraction- and weakening-less subproof of $\Gamma, J_1 \cdots \vdash J_2$, followed by applications of contraction and weakening to accommodate the context, where the ellipsis indicates zero or more assumptions of $J_1$. The final applications of contraction and weakening are represented by a doubled derivation line, to indicate that it's multiple steps shown as one.

(TODO: wording) The role of the proof reduction is then to take the proof of $\Delta \vdash A\ true$, and replace with it the instances of $A\ true \vdash A\ true$ in the proof of $\Gamma, A\ true \vdash B\ true$. The full proof reduction rule is shown in Figure\nbsp[[fig:intuit_impl_red]].

#+name: fig:intuit_impl_red
#+caption: Implication proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\to}$}
 {\prftree[r]{$\Intro{\to}$}
   {\prftree[double]
     {\prfsummary[$s$]
       {\left(\vcenter{\prftree[r]{Id}{A \vdash_T A}}\right) \cdots}
       {\Gamma, A \cdots \vdash_T B}}
     {\Gamma, A \vdash_T B}}
   {\Gamma \vdash_T A \to B}}
 {\prfsummary[$t$]{\Delta \vdash_T A}}
 {\Gamma, \Delta \vdash_T B}}
\prfRed
\vcenter{\prftree[double]
 {\prfStackPremises
   {\left(\vcenter{\prfsummary[$t$]{\Delta \vdash_T A}}\right) \cdots}
   {\prfsummary[$s$]{\Gamma, \Delta \cdots \vdash_T B}}}
 {\Gamma, \Delta \vdash_T B}}
$$
#+end_figure

** Linear logic

In contrast to intuitionistic logic, linear logic considers propositions to be a form of resource \mdash they should not be subject to duplication or discard. When looking at intuitionistic proofs, such as the ones listed in Figure [[fig:intuit_duplic]], we can see that intuitionistic logic has no problem with duplicating propositions (from a single $A$ one might obtain multiple \(A\)'s) or discarding propositions (the $B$ is unnecessary in the proof of $A$, so it is thrown away).

#+name: fig:intuit_duplic
#+caption: Duplication and discard of truth
#+begin_figure
$$
\prftree[r]{$\Intro{\to}$}
 {\prftree[r]{Contr}
   {\prftree[r]{$\Intro{\land}$}
     {\prftree[r]{Id}
       {A \vdash_T A}}
     {\prftree[r]{Id}
       {A \vdash_T A}}
     {A, A \vdash_T A \land A}}
   {A \vdash_T A \land A}}
 {\vdash_T A \to (A \land A)}
\hspace{2em}
\prftree[r]{$\Intro{\to}$}
 {\prftree[r]{$\Intro{\to}$}
  {\prftree[r]{Weak}
    {\prftree[r]{Id}
      {A \vdash_T A}}
    {A, B \vdash_T A}}
  {A \vdash_T B \to A}}
 {\vdash_T A \to (B \to A)}
$$
#+end_figure

In intuitionistic logic, we judged a proposition to be true, and the judgment had the form /$A$ true/. In linear logic, we focus on /availability/. We can judge a proposition $A$ to be available, written /$A$ avail/, if there is a proof that "consumes" some assumptions, "producing" the proposition $A$. The semantics of consumption are embedded in the deduction rules, explained below.

One simple way to prevent "invalid" usage of resources is to remove the contraction and weakening rules altogether. However, this approach severely limits the expressivity of the language. We might still want to model "free" resources, meaning resources that can be used any number of times, even zero, but conveying this information would not be possible in such a system. Instead, we introduce an annotation for unbound resources, and limit contraction and weakening so that they can only be used on these "intuitionistic" resources. This alternative gives us strictly greater expressivity than intuitionistic logic, as we will see that every intuitionistic proof can be translated to an equivalent linear proof.

The introduction of unbound resources necessitates differentiating between two kinds of assumptions in contextualized judgments \mdash a /linear/ assumption of the judgment /$A$ avail/ is written $\Lin{A\ avail}$, and indicates that the conclusion uses the fact that $A$ is available /exactly once/. An /intuitionistic/ assumption of the judgment /$A$ avail/, written $\Int{A\ avail}$, makes no guarantees about its usage in the conclusion \mdash it may be used zero, one, or even more times. It is important to emphasize that these glyphs are not a part of the object language \mdash neither $\Lin{A}$ nor $\Int{A}$ are well-formed propositions, and the bracket notation can only appear on the left side of a turnstile.

Contraction and weakening are now limited to only intuitionistic assumptions, meaning that judgments can be linearly assumed multiple times. These new rules lead to a general context $\Gamma$ behaving like a multiset. Every intuitionistic judgment can be made to have a multiplicity of one (using the new contraction and weakening), and multiplicity of linear assumptions is given by their usage in the conclusion.

Similarly to the intuitionistic case, a shorthand notation for contextualized judgments is used \mdash writing $\Gamma \vdash_R A$, the context \Gamma is a list of /propositions/ in square or angle brackets, such as $\Lin{B}$ or $\Int{C \loli D}$, and $A$ is a proposition. This is shorthand for a contextualized judgment whose context is a list containing one occurrence of the judgment $\Lin{B\ avail}$ for every proposition $B$ in angle brackets in \Gamma, and one occurrence of the judgment $\Int{C\ avail}$ for every proposition $C$ in square brackets in \Gamma. The conclusion of this contextualized judgment is the judgment $A\ avail$, where $A$ is the proposition on the right of the turnstile in the shorthand.

A general context \Gamma can contain assumptions of both kinds, linear and intuitionistic, but an /intuitionistic context/, denoted by $\Int{\Gamma}$, is a context that only contains intuitionistic assumptions, if any.

The focus of this chapter is a fragment of propositional intuitionistic linear logic. It bears similarity to the intuitionistic logic described in the last chapter, specifically it provides tools for representing implication and conjunction, in addition to the linear-logic-specific exponentiation.

The new implication connective is historically called "lollipop", and it's written $A \loli B$. The proposition is read "produce $B$ consuming $A$".

Interestingly, there are two conjunction connectives \mdash the "tensor", written $A \tens B$, and the "with", written $A \with B$. The tensor represents a conjunction "containing" /both/ resources $A$ and $B$, while the "with" lists two resources that are both available, but not at the same time \mdash the recipient of such a resource needs to choose either $A$ or $B$.

The last connective is a new concept entirely. The exponential operator $\bang{A}$, pronounced "of course", allows one to represent an infinite amount of a resource. We will see how this connective differs from the intuitionistic assumption $\Int{A\ avail}$ and why they are both necessary once we take a look at program evaluation in [[*Type theory][Part III]].

The propositions of this logic can also be described by the simple grammar
$$
A, B ::= X \binor (A \loli B) \binor (A \tens B) \binor (A \with B) \  \binor \  \bang{A}
$$
for X ranging over atomic propositions. The deduction rules are listed in Figure [[fig:linear_deduct]].

#+name: fig:linear_deduct
#+caption: Deduction rules for the fragment of intuitionistic linear logic
#+begin_figure
$$
\prftree[r]{$\Lin{\text{Id}}$}
 {\Lin{A} \vdash_R A}
\hspace{2em}
\prftree[r]{$\Int{\text{Id}}$}
 {\Int{A} \vdash_R A}
$$

$$
\prftree[r]{Exchange}
 {\Gamma, S, T, \Delta \vdash_R A}
 {\Gamma, T, S, \Delta \vdash_R A}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, \Int{A}, \Int{A} \vdash_R B}
 {\Gamma, \Int{A} \vdash_R B}
\hspace{2em}
\prftree[r]{Weakening}
 {\Gamma \vdash_R B}
 {\Gamma, \Int{A} \vdash_R B}
$$

$$
\prftree[r]{$\Intro{\loli}$}
 {\Gamma, \Lin{A} \vdash_R B}
 {\Gamma \vdash_R (A \loli B)}
\hspace{2em}
\prftree[r]{$\Elim{\loli}$}
 {\Gamma \vdash_R (A \loli B)}
 {}
 {\Delta \vdash_R A}
 {\Gamma, \Delta \vdash_R B}
$$

$$
\prftree[r]{$\Intro{\with}$}
 {\Gamma \vdash_R A}
 {}
 {\Gamma \vdash_R B}
 {\Gamma \vdash_R A \with B}
$$

$$
\prftree[r]{$\Elim{\with}_1$}
 {\Gamma \vdash_R A \with B}
 {\Gamma \vdash_R A}
\hspace{2em}
\prftree[r]{$\Elim{\with}_2$}
 {\Gamma \vdash_R A \with B}
 {\Gamma \vdash_R B}
$$

$$
\prftree[r]{$\Intro{\tens}$}
 {\Gamma \vdash_R A}
 {}
 {\Delta \vdash_R B}
 {\Gamma, \Delta \vdash_R A \tens B}
\hspace{2em}
\prftree[r]{$\Elim{\tens}$}
 {\Gamma, \Lin{A}, \Lin{B} \vdash_R C}
 {}
 {\Delta \vdash_R A \tens B}
 {\Gamma, \Delta \vdash_R C}
$$

$$
\prftree[r]{$\Intro{\bang}$}
 {\Int{\Gamma} \vdash_R A}
 {\Int{\Gamma} \vdash_R \bang{A}}
\hspace{2em}
\prftree[r]{$\Elim{\bang}$}
 {\Gamma, \Int{A} \vdash_R B}
 {}
 {\Delta \vdash_R \bang{A}}
 {\Gamma, \Delta \vdash_R B}
$$
#+end_figure

There are now two axioms, one for each kind of assumption. The /linear identity/ $\Lin{\text{Id}}$ says that one can conclude the availability of a resource if one such resource is available. The /intuitionistic identity/ expresses the very same concept, except with one caveat \mdash the proof says nothing about how many times the resource was used in the reasoning.

The exchange rule stays unchanged, only $S$ and $T$ stand for any two propositions with square or angle brackets \mdash we are free to rearrange and intermix linear and intuitionistic assumptions.

The contraction and weakening rules are limited to intuitionistic assumptions, as mentioned in the introduction.

The $\loli$ ("lollipop") introduction rule in linear logic also abstracts an assumption, but it is limited only to linear ones. The proposition $A \loli B$ represents an action of "consuming" a resource $A$ to "produce" a resource $B$. We choose the word "consuming", because when introducing the lollipop, the resource $A$ is removed from the context. In other words, the subsequent deductions loose access to it. Because the deduction sequence leading to the judgment /\(B\)\nbsp{}avail/  was using the assumption $\Lin{A\ avail}$, we can imagine a proof of the judgment /$(A \loli B)$ avail/ to contain a hole, waiting for an $A$.

The corresponding elimination rule fills such a hole with a resource obtained from a different context. Emphasis is put on the contexts being different \mdash the context \Gamma contains other resources that are also consumed during the process of turning an $A$ into a $B$, therefore the resources cannot be shared with the context used for filling the hole.

Proof reduction for the lollipop is similar in spirit to the intuitionistic implication, except there is no need to worry about the assumption /$A$ avail/ being used multiple times. This is apparent from the fact that linear assumptions cannot be contracted. Therefore, the resulting reduction rule is simpler, as shown in Figure [[fig:lin_impl_red]].

#+name: fig:lin_impl_red
#+caption: Lollipop proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\loli}$}
 {\prftree[r]{$\Intro{\loli}$}
   {\prfsummary[$s$]
     {\prftree[r]{$\Lin{\text{Id}}$}
       {\Lin{A} \vdash_R A}}
     {\Gamma, \Lin{A} \vdash_R B}}
   {\Gamma \vdash_R A \loli B}}
 {\prfsummary[$t$]{\Delta \vdash_R A}}
 {\Gamma, \Delta \vdash_R B}}
\prfRed
\vcenter{\prfStackPremises
 {\prfsummary[$t$]{\Delta \vdash_R A}}
 {\prfsummary[$s$]{\Gamma, \Delta \vdash_R B}}}
$$
#+end_figure

The $\with$ ("with") deduction rules exactly mirror the intuitionistic conjunction rules. This connective is also called the /additive conjunction/, because the introduction rule shares the resources used for producing the individual components. This sharing of resources prevents a consumer from extracting both of the components \mdash the resources are all used once one of the components is extracted. The proof reduction is also analogous, and presented in Figure [[fig:lin_with_red]].

#+name: fig:lin_with_red
#+caption: With conjunction proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\with}_1$}
 {\prftree[r]{$\Intro{\with}$}
   {\prfsummary[$s$]{\Gamma \vdash_R A}}
   {}
   {\prfsummary[$t$]{\Gamma \vdash_R B}}
   {\Gamma \vdash_T A \with B}}
 {\Gamma \vdash_T A}}
\prfRed
\vcenter{\prfsummary[$s$]{\Gamma \vdash_R A}}
$$
#+end_figure

The $\tens$ ("tensor") conjunction represents a pair of resources, both of which have to be consumed, due to the requirement of not discarding resources. The introduction rule looks almost exactly the same as the one for the $\with$ conjunction, however in this case, the two parts of the tensor conjunction are produced in different contexts. It is this difference that makes the two connectives have different semantics \mdash while the $\with$ conjunction offers two different possible results from the same resources, the $\tens$ conjunction combines two sets of resources into a pair of two results, and provides both for later consumption.

The elimination rule says that a $\tens$ resource can be used to complete a proof that contains a linear assumption of each of its constituents. In other words, to consume a $\tens$ resource, one must consume both of its parts.

The reduction rule, shown in Figure [[fig:lin_tens_red]], describes how to perform such a completion. If the conjunction is constructed using two proofs $t$ and $u$ of the judgments /$A$ avail/ and /$B$ avail/, respectively, then these proofs can replace the assumptions $\Lin{A\ avail}$ and $\Lin{B\ avail}$ in another proof $s$.

#+name: fig:lin_tens_red
#+caption: Tensor conjunction proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\tens}$}
 {\prfsummary[$s$]
   {\prftree[r]{$\Lin{\text{Id}}$}
     {\Lin{A} \vdash_R A}}
   {}
   {\prftree[r]{$\Lin{\text{Id}}$}
     {\Lin{B} \vdash_R B}}
   {\Gamma, \Lin{A}, \Lin{B} \vdash_R C}}
 {\prftree[r]{$\Intro{\tens}$}
   {\prfsummary[$t$]{\Delta \vdash_R A}}
   {}
   {\prfsummary[$u$]{\Theta \vdash_R B}}
   {\Delta, \Theta \vdash_R A \tens B}}
 {\Gamma, \Delta, \Theta \vdash_R C}}
\prfRed
\vcenter{\prfsummary[$s$]
 {\prfsummary[$t$]{\Delta \vdash_R A}}
 {}
 {\prfsummary[$u$]{\Theta \vdash_R B}}
 {\Gamma, \Delta, \Theta \vdash_R C}}
$$

#+end_figure

The $\bang$ ("of course") connective is supposed to extend the expressive power of linear logic to reason about free resources. A judgment of the form /$\bang{A}$ avail/ does not represent an instance of the resource $A$, but rather /a source of/[fn:3] these resources. The idea is that a resource $A$ can be pulled out from this source at any time, or even never at all, allowing us to model free resources \mdash the judgment /$\bang{A}$ avail/ serves as a statement that $A$ is a free resource.

To produce one of these sources, the introduction rule provides us with a way of extending proofs based on only intuitionistic assumptions. Intuitionistic assumptions are another way of modeling free resources, so the essence of the introduction rule is an observation that, given a recipe of creating one unit of a resource $A$ from free ingredients $\Int{\Gamma}$, we can duplicate those free ingredients however many times is necessary to supply more instances of the resource, and that we don't mind throwing the ingredients away in the case that there is no demand for it.

Dually to the introduction rule, which relays how to create a source from free ingredients, the elimination rule describes how a source can satiate an undisclosed demand. A proof built on an intuitionistic assumption gives no guarantees about the number of times it uses the associated resource $A$. To satisfy this assumption, we can provide the proof with a source $\bang{A}$, which can adapt to its requirements.

Reducing a sequence of $\bang$ introduction and elimination looks similar to reducing an implication in intuitionistic logic, because it operates on the same principle \mdash replacing assumptions with auxiliary proofs, while acknowledging the fact that the assumptions might appear zero or more times. In the Figure [[fig:lin_exp_red]], the expression $\Int{A} \cdots$ represents zero or more intuitionistic assumptions of the judgment /$A$ avail/, and the proof tree $s$ is devoid of contraction and weakening on the judgment /$A$ avail/. Instead, these are all applied in the step represented by the double derivation line. The reduction then replaces each instance of the intuitionistic assumption /$A$ avail/ with a derivation tree $t$, which produces a resource $A$ from other intuitionistic assumptions. The double line in the reduced proof signifies applications of contraction and weakening to the assumptions $\Int{\Delta}$, corresponding to the double line in the non-reduced proof.

#+name: fig:lin_exp_red
#+caption: Exponential proof reduction
#+begin_figure
$$
\vcenter{\prftree[r]{$\Elim{\bang}$}
 {\prftree[double]
   {\prfsummary[$s$]
     {\left(\vcenter{\prftree[r]{$\Int{\text{Id}}$}{\Int{A} \vdash_R A}}\right) \cdots}
     {\Gamma, \Int{A} \cdots \vdash_R B}}
   {\Gamma, \Int{A} \vdash_R B}}
 {\prftree[r]{$\Intro{\bang}$}
   {\prfsummary[$t$]{\Int{\Delta} \vdash_R A}}
   {\Int{\Delta} \vdash_R \bang{A}}}
 {\Gamma, \Int{\Delta} \vdash_R B}}
\prfRed
\vcenter{\prftree[double]
 {\prfStackPremises
   {\left(\vcenter{\prfsummary[$t$]{\Int{\Delta} \vdash_R A}}\right) \cdots}
   {\prfsummary[s]{\Gamma, \Int{\Delta} \cdots \vdash_R B}}}
 {\Gamma, \Int{\Delta} \vdash_R B}}
$$
#+end_figure

** Intuitionistic embedding

We claimed that every intuitionistic proof can be translated to an equivalent linear proof. To verify this statement, two steps are necessary. First, we need to show how to translate the three primitive constructs: propositions, judgments, and contextualized judgments. Secondly, we need to show that this translation preserves deduction rules and proof reductions. That is to say, for every intuitionistic deduction rule or proof reduction, there is a corresponding linear deduction or reduction taking the translated premises to the translated conclusion.

The intuitionistic propositions come in three flavors: base propositions, conjunctions and implications. We define a translation operator $\LinTrans{\_}$, and its action on propositions is given by the equations
\begin{align*}
  \LinTrans{X} & = X \\
  \LinTrans{A \land B} & = \LinTrans{A} \with \LinTrans{B} \\
  \LinTrans{A \to B} & = \bang{\LinTrans{A}} \loli \LinTrans{B} \\
\end{align*}
where $X$ stands for an atomic proposition, and $A$ and $B$ stand for arbitrary intuitionistic propositions.

On a formal level, this mapping is justified by showing that it preserves deduction and reduction, which is done later in the chapter. On an intuitive level, we appeal to the interpretation of the connectives. When looking at an atomic proposition in isolation, the intuitionistic and linear interpretation is the same, because differences arise only when talking about more complex propositions, and how they relate to each other, for example how are the two sides of a conjunction used, or how is the input to an implication used. The intuitionistic conjunction gives access to each of its constituents, but only one can be extracted, behaving the same as the $\with$ conjunction. Finally, the intuitionistic implication gives no guarantees about the use of its hypothesis, therefore it is necessary to mark the hypothesis with a bang, and promote it to a source in the linear interpretation.

There are only two judgments in intuitionistic logic, and these are /$A$ prop/ and /$A$ true/ for an intuitionistic proposition $A$. These are interpreted as /$A$ prop/ and /$A$ avail/, respectively, defining the action of the translation operator on judgments.
\begin{align*}
  \LinTrans{A\ prop} &= A\ prop \\
  \LinTrans{A\ true} &= A\ avail
\end{align*}

To give a translation of a contextualized judgment, we need to describe how to translate the context. This action is defined with an equation for the empty context, labeled '$\cdot$', and an equation for a concatenation of an arbitrary context $\Gamma$ with an arbitrary assumption $J$.
\begin{align*}
  \LinTrans{\cdot} &= \cdot \\
  \LinTrans{\Gamma, J} &= \LinTrans{\Gamma}, \Int{\LinTrans{J}}
\end{align*}

Verbally, the translation preserves the empty context, and it maps every judgment $J$ in $\Gamma$ (since assumptions in intuitionistic logic are simply judgments) to an intuitionistic assumption of the translation of the judgment. As a consequence, all the assumptions in a translated context are intuitionistic. The contextualized judgment translation is then given by the equation
$$
\LinTrans{\Gamma \vdash J} = \LinTrans{\Gamma} \vdash \LinTrans{J}
$$

It is easy to see that by also defining the action of the translation on lists of propositions as $\LinTrans{(\Gamma_i)_{i=0}^n} = (\Int{\LinTrans{\Gamma_i}})_{i=0}^n$, we can recover a relationship between the shorthand notations:
$$
\LinTrans{\Gamma \vdash_T A} = \LinTrans{\Gamma} \vdash_R \LinTrans{A}
$$

Having defined the translation of contextualized judgments, we continue by defining how their relationships are translated \mdash that is, how to translate deduction rules.

The axiom of intuitionistic logic is translated into the intuitionistic axiom of linear logic, and the structural rules correspond to their respective counterparts, as shown in the following equations:
\begin{align*}
\LinTrans{\vcenter{\prftree[r]{Id}
    {A \vdash_T A}}}
&\prfEq
\vcenter{\prftree[r]{$\Int{\text{Id}}$}{\Int{\LinTrans{A}} \vdash_R \LinTrans{A}}}
\\[1ex]
\LinTrans{\vcenter{\prftree[r]{Weakening}
    {\Gamma \vdash_T A}
    {\Gamma, B \vdash_T A}}}
&\prfEq
\vcenter{\prftree[r]{Weakening}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A}}
  {\LinTrans{\Gamma}, \Int{\LinTrans{B}} \vdash_R \LinTrans{A}}}
\\[1ex]
\LinTrans{\vcenter{\prftree[r]{Contraction}
  {\Gamma, A, A \vdash_T B}
  {\Gamma, A \vdash_T B}}}
&\prfEq
\vcenter{\prftree[r]{Contraction}
  {\LinTrans{\Gamma}, \Int{\LinTrans{A}}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}
  {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}}
\\[1ex]
\LinTrans{\vcenter{\prftree[r]{Exchange}
    {\Gamma, A, B, \Delta \vdash_T C}
    {\Gamma, B, A, \Delta \vdash_T C}}}
&\prfEq
\vcenter{\prftree[r]{Exchange}
  {\LinTrans{\Gamma}, \Int{\LinTrans{A}}, \Int{\LinTrans{B}}, \LinTrans{\Delta} \vdash_R \LinTrans{C}}
  {\LinTrans{\Gamma}, \Int{\LinTrans{B}}, \Int{\LinTrans{A}}, \LinTrans{\Delta} \vdash_R \LinTrans{C}}}
\end{align*}
\newpage

Translation of the intuitionistic conjunction is defined in terms of the $\with$ conjunction, so it is expected that the deduction rules of one will correspond to the deduction rules of the other. That is indeed the case, as the translation is given below. It uses the equality $\LinTrans{A \land B} = \LinTrans{A} \with \LinTrans{B}$.
\begin{align*}
\LinTrans{\vcenter{\prftree[r]{$\Intro{\land}$}
    {\Gamma \vdash_T A}
    {\Gamma \vdash_T B}
    {\Gamma \vdash_T A \land B}}}
&\prfEq
\vcenter{\prftree[r]{$\Intro{\with}$}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A}}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{B}}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A} \with \LinTrans{B}}}
\\[1ex]
\LinTrans{\vcenter{\prftree[r]{$\Elim{\land}_1$}
    {\Gamma \vdash_T A \land B}
    {\Gamma \vdash_T A}}}
&\prfEq
\vcenter{\prftree[r]{$\Elim{\with}_1$}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A} \with \LinTrans{B}}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A}}}
\end{align*}

The intuitionistic implication is translated with the $\bang$ and $\loli$ connectives, and the translation of the $\Intro{\to}$ rule, stated below, demonstrates why. The linear implication cannot be introduced from an intuitionistic assumption, so it necessitates an intermediary step which replaces it with a linear assumption, through $\bang$ elimination.
$$
\LinTrans{\vcenter{\prftree[r]{$\Intro{\to}$}
    {\Gamma, A \vdash_T B}
    {\Gamma \vdash_T A \to B}}}
\prfEq
\vcenter{\prftree[r]{$\Intro{\loli}$}
  {\prftree[r]{$\Elim{\bang}$}
    {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}
    {\prftree[r]{$\Lin{\text{Id}}$}
      {\Lin{\bang{\LinTrans{A}}} \vdash_R \bang{\LinTrans{A}}}}
    {\LinTrans{\Gamma}, \Lin{\bang{\LinTrans{A}}} \vdash_T \LinTrans{B}}}
  {\LinTrans{\Gamma} \vdash_R \bang{\LinTrans{A}} \loli \LinTrans{B}}}
$$

The translation for the $\Elim{\to}$ rule takes advantage of the fact that for any intuitionistic context $\Delta$, its translation $\LinTrans{\Delta}$ consists only of intuitionistic assumptions, therefore it is a valid target for applying $\bang$ introduction. Producing a $\bang$ proposition is required for the input of the translated implication proposition.
$$
\LinTrans{\vcenter{\prftree[r]{$\Elim{\to}$}
    {\Gamma \vdash_T A \to B}
    {\Delta \vdash_T A}
    {\Gamma, \Delta \vdash_T B}}}
\prfEq
\vcenter{\prftree[r]{$\Elim{\loli}$}
  {\LinTrans{\Gamma} \vdash_R \bang{\LinTrans{A}} \loli \LinTrans{B}}
  {\prftree[r]{$\Intro{\bang}$}
    {\LinTrans{\Delta} \vdash_R \LinTrans{A}}
    {\LinTrans{\Delta} \vdash_R \bang{\LinTrans{A}}}}
  {\LinTrans{\Gamma}, \LinTrans{\Delta} \vdash_R \LinTrans{B}}}
$$

We can extend the notion of translating deduction rules into translating entire proof trees. The linear translation of an intuitionistic proof tree $p$ is denoted $\LinTrans{p}$, and it is constructed by replacing the intuitionistic contextualized judgments and deduction rules by their linear translations. Because the deduction rules are translated into well-formed linear deductions, and because the premises and conclusions are consistently translated, we can be certain that the new deduction tree is correctly constructed and represents a well-formed linear proof.

Finally, we need to show that reduction and translation commute. That is, given an intuitionistic proof $p$ and its reduction $p \prfRed[0em] p'$, there is an equivalent reduction $\LinTrans{p} \LinTrans{\prfRed[0em]} \LinTrans{p}'$ such that its result is the same as translating $p'$. This condition is represented by the following diagram:
#+begin_center
\begin{tikzcd}
p \arrow[r, maps to, "\Rightarrow"] \arrow[d, maps to, "\LinTrans{\_}"] & p' \arrow[d, maps to, "\LinTrans{\_}"] \\
\LinTrans{p} \arrow[r, maps to, "\LinTrans{\Rightarrow}"] & \LinTrans{p}' = \LinTrans{p'}
\end{tikzcd}
#+end_center

To prove this commutativity, it suffices to prove it for the two intuitionistic reductions individually.

For conjunction reduction, we take a general reducible proof
$$
p \prfEq \vcenter{\prftree[r]{$\Elim{\land}_1$}
  {\prftree[r]{$\Intro{\land}$}
    {\prfsummary[$s$]{\Gamma \vdash_T A}}
    {\prfsummary[$t$]{\Gamma \vdash_T B}}
    {\Gamma \vdash_T A \land B}}
  {\Gamma \vdash_T A}}
$$
its reduced form
$$
p' \prfEq \vcenter{\prfsummary[$s$]{\Gamma \vdash_T A}}
$$
and its translation
$$
\LinTrans{p} \prfEq \vcenter{\prftree[r]{$\Elim{\with}_1$}
  {\prftree[r]{$\Intro{\with}$}
    {\prfsummary[$\LinTrans{s}$]{\LinTrans{\Gamma} \vdash_R \LinTrans{A}}}
    {\prfsummary[$\LinTrans{t}$]{\LinTrans{\Gamma} \vdash_R \LinTrans{B}}}
    {\LinTrans{\Gamma} \vdash_R \LinTrans{A} \with \LinTrans{B}}}
  {\LinTrans{\Gamma} \vdash_R \LinTrans{A}}}
$$

We can verify that indeed
\begin{align*}
\LinTrans{p}'
&\prfEq \vcenter{\prfsummary[$\LinTrans{s}$]{\LinTrans{\Gamma} \vdash_R \LinTrans{A}}}
\\[1ex]
&\prfEq \LinTrans{p'}
\end{align*}

The proof for implication reduction involves both lollipop and exponential reductions. First, take a general reducible proof
$$
p \prfEq \vcenter{\prftree[r]{$\Elim{\to}$}
  {\prftree[r]{$\Intro{\to}$}
    {\prftree[double]
      {\prfsummary[$s$]
        {\left(\vcenter{\prftree[r]{Id}{A \vdash_T A}}\right) \cdots}
        {\Gamma, A \cdots \vdash_T B}}
      {\Gamma, A \vdash_T B}}
    {\Gamma \vdash_T A \to B}}
  {\prfsummary[$t$]{\Delta \vdash_T A}}
  {\Gamma, \Delta \vdash_T B}}
$$
its reduced form
$$
p' \prfEq \vcenter{\prftree[double]
  {\prfStackPremises
    {\left(\vcenter{\prfsummary[$t$]{\Delta \vdash_T A}}\right) \cdots}
    {\prfsummary[$s$]{\Gamma, \Delta \cdots \vdash_T B}}}
  {\Gamma, \Delta \vdash_T B}}
$$
and its translation
$$
\LinTrans{p} \prfEq \vcenter{\prftree[r]{$\Elim{\loli}$}
  {\prftree[r]{$\Intro{\loli}$}
    {\prftree[r]{$\Elim{\bang}$}
      {\prftree[double]
        {\prfsummary[$\LinTrans{s}$]
          {\left(\vcenter{\prftree[r]{$\Int{\text{Id}}$}{\Int{\LinTrans{A}} \vdash_R \LinTrans{A}}}\right) \cdots}
          {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \cdots \vdash_R \LinTrans{B}}}
        {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}}
      {\prftree[r]{$\Lin{\text{Id}}$}{\Lin{\bang{\LinTrans{A}}} \vdash_R \bang{\LinTrans{A}}}}
      {\LinTrans{\Gamma}, \Lin{\bang{\LinTrans{A}}} \vdash_R \LinTrans{B}}}
    {\LinTrans{\Gamma} \vdash_R \bang{\LinTrans{A}} \loli \LinTrans{B}}}
  {\prftree[r]{$\Intro{\bang}$}
    {\prfsummary[$\LinTrans{t}$]{\LinTrans{\Delta} \vdash_R \LinTrans{A}}}
    {\LinTrans{\Delta} \vdash_R \bang{\LinTrans{A}}}}
  {\LinTrans{\Gamma}, \LinTrans{\Delta} \vdash_R \LinTrans{B}}}
$$

We can define the translation of the implication reduction as first reducing the lollipop, and subsequently reducing the exponential, as in the sequence
\begin{align*}
\LinTrans{p}
&\underset{\loli}{\prfRed}
\vcenter{\prftree[r]{$\Elim{\bang}$}
  {\prftree[double]
    {\prfsummary[$\LinTrans{s}$]
      {\left(\vcenter{\prftree[r]{$\Int{\text{Id}}$}{\Int{\LinTrans{A}} \vdash_R \LinTrans{A}}}\right) \cdots}
      {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \cdots \vdash_R \LinTrans{B}}}
    {\LinTrans{\Gamma}, \Int{\LinTrans{A}} \vdash_R \LinTrans{B}}}
  {\prftree[r]{$\Intro{\bang}$}
    {\prfsummary[$\LinTrans{t}$]{\LinTrans{\Delta} \vdash_R \LinTrans{A}}}
    {\LinTrans{\Delta} \vdash_R \bang{\LinTrans{A}}}}
  {\LinTrans{\Gamma}, \LinTrans{\Delta} \vdash_R \LinTrans{B}}}
\\[2ex]
&\underset{\bang}{\prfRed}
\vcenter{\prftree[double]
  {\prfStackPremises
    {\left(\vcenter{\prfsummary[$\LinTrans{t}$]{\LinTrans{\Delta} \vdash_R \LinTrans{A}}}\right) \cdots}
    {\prfsummary[$\LinTrans{s}$]{\LinTrans{\Gamma}, \LinTrans{\Delta} \cdots \vdash_R \LinTrans{B}}}}
  {\LinTrans{\Gamma}, \LinTrans{\Delta} \vdash_R \LinTrans{B}}}
\\[2ex]
&\prfEq
\LinTrans{p}'
\end{align*}

Writing down the translation of $p'$, we can see that the two conclusions are equal.
\begin{align*}
  \LinTrans{p'}
  &\prfEq
  \vcenter{\prftree[double]
    {\prfStackPremises
      {\left(\vcenter{\prfsummary[$\LinTrans{t}$]{\LinTrans{\Delta} \vdash_R \LinTrans{A}}}\right) \cdots}
      {\prfsummary[$\LinTrans{s}$]{\LinTrans{\Gamma}, \LinTrans{\Delta} \cdots \vdash_R \LinTrans{B}}}}
    {\LinTrans{\Gamma}, \LinTrans{\Delta} \vdash_R \LinTrans{B}}}
  \\[2ex]
  &\prfEq \LinTrans{p}'
\end{align*}

Because all proof reductions are composed of sequenced implication and conjunction reductions, it follows that the defined translation commutes with every proof reduction.

* Type theory

Type theory is the study of types, and it serves as a constructive way of organizing mathematical objects. Types are descriptions of constructions, and in a constructive system, every existing object needs a recipe for how it can be constructed. It follows that every mathematical object has an associated type.

To assert that a mathematical object $a$ is of a certain type $T$, we write $a : T$, and this statement is called a *typing judgment*, or sometimes simply a *typing*. Analogously to judgments in logic, a typing judgment might be valid only in a certain context, so we introduce a notion of *contextualized typing judgments*, which have the form $\Gamma \vdash a : T$, meaning that $a$ is of type $T$ in the context $\Gamma$.

Traditionally, defining a type is a procedure consisting of fours steps \cite{Bauer2019}. First, the *formation* rules are given, which describe the conditions for a mathematical object $T$ to be called a type. Then, the *introduction* rules specify how objects of this type are constructed. After an object is constructed, the *elimination* rules give ways of taking it apart. Lastly, objects that have type $T$ may relate to each other in some ways, and these relationships are described by additional *equations*.

A collection of types is called a *type system*. One such type system is the /simply typed \lambda-calculus/, or STLC for short. It uses syntax of the untyped \lambda-calculus, and a metalanguage similar to natural deduction to describe its types. The version of STLC used in this thesis is the traditional simply typed \lambda-calculus, extended with product types.

Given a collection of base types, an STLC system is generated by introducing function and product types.

The formation rules of function types and product types are almost identical, so we present them both at the same time.

/If $A$ and $B$ are types, then $(A \to B)$ is a type, and $(A \times B)$ is a type./

\noindent Types in STLC are then described by the grammar
$$
A, B ::= X \binor (A \to B) \binor (A \times B)
$$
for X ranging over base types.

Objects of STLC are *well-typed* terms of the untyped \lambda-calculus. A well-typed term is a term that is obtainable by deductions of the type system. A well-typed term is also called a *program*. A context of a contextualized typing judgment in STLC is a list of typing judgments, where the terms being typed are variables, and every variable appears in the context at most once. When concatenating contexts, it is implicitly assumed that they don't share any variables.

Analogues to the structural rules from intuitionistic logic exist for STLC. The only difference is that the type-theoretical variants provide additional information on their action on terms. All three rules are listed in Figure [[fig:stlc_struct]], along with the identity axiom.

The exchange rules remains mostly unchanged. It asserts that changing the order of variable typings in the context has no effect on neither the typed term nor its type.

The weakening rule plays the same role as logical weakening, but it also states that the conclusion deduces the same term of the same type as the premise. As stated above, there is an implicit assumption that the variable $x$ is not contained in the context $\Gamma$.

The contraction rule expresses that the type of an expression does not depend on specific values of its free variables, only their types. That is to say, any two variables $x$ and $y$ of the same type may be replaced by a new variable $z$ without changing the resulting type. It employs capture-avoiding variable substitution as defined in \cite{Sorensen2006}, which is a metaoperation \mdash the symbols '$[$', '$:=$' and '$]$' are not part of the language of lambda calculus. The metaterm $s[x:=z]$ stands for the term $s$ with free occurrences of the variable $x$ replaced by the term $z$.

The identity axiom claims that every variable from the context can be derived.

#+name: fig:stlc_struct
#+caption: Structural rules and the identity axiom for STLC
#+begin_figure
$$
\prftree[r]{Exchange}
 {\Gamma, x: A, y: B, \Delta \vdash t: C}
 {\Gamma, y: B, x: A, \Delta \vdash t: C}
$$

$$
\prftree[r]{Weakening}
 {\Gamma \vdash s: A}
 {\Gamma, x: B \vdash s: A}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, x: A, y: A \vdash s: B}
 {\Gamma, z: A \vdash s[x:=z][y:=z]: B}
$$

$$
\prftree[r]{Id}
 {x: A \vdash x: A}
$$
#+end_figure

The introduction and elimination rules for function types mirror the structure of implication deduction rules in intuitionistic logic. Whereas the logical interpretation relied on hypotheses, the type-theoretical interpretation is given in terms of binding variables and applying abstractions. The premise of the introduction rule presents a term $t$, and among its free variables might be the variable $x$ ($x$ is free in $t$ if it was derived using the identity axiom, or it might not be referenced in $t$ if it was derived using weakening). The conclusion then produces a \lambda-term which explicitly binds this variable.

The elimination rule introduces an application term, and together with \beta-reduction it gives a notion of "computation", which corresponds to the implication proof reduction rule. As a consequence of the \eta-conversion, we know that every object of the function type is equivalent to a \lambda-term. The rules and equations are listed in Figure [[fig:stlc_fun]].

The condition of $x$ not being free in $f$ for the \eta-conversion can be justified by looking at the expanded form of the equality, which is obtained by annotating the terms with their proof trees.
$$
\vcenter{\prfsummary{\Gamma \vdash f: (A \to B)}}
\hspace{2em}\equiv\hspace{2em}
\vcenter{\prftree[r]{$\Intro{\to}$}
 {\prftree[r]{$\Elim{\to}$}
   {\prfsummary{\Gamma \vdash f: (A \to B)}}
   {\prftree[r]{Id}{x: A \vdash x: A}}
   {\Gamma, x: A \vdash (f\ x): B}}
 {\Gamma \vdash (\lambda x. (f\ x)): (A \to B)}}
$$
We see that the tree contains a typing in the context $\Gamma, x: A$. If $x$ was free in $f$, then the list $\Gamma$ would already contain a typing of the variable $x$, leading to a proof that is not well-formed.

#+name: fig:stlc_fun
#+caption: Rules and equations of the function type
#+begin_figure
$$
\prftree[r]{$\Intro{\to}$}
 {\Gamma, x: A \vdash s: B}
 {\Gamma \vdash (\lambda x.s): (A \to B)}
\hspace{2em}
\prftree[r]{$\Elim{\to}$}
 {\Gamma \vdash f: (A \to B)}
 {\Delta \vdash s: A}
 {\Gamma, \Delta \vdash (f\ s): B}
$$
\begin{align*}
&\beta\text{-reduction: } ((\lambda x. s)\ t) \equiv s[x := t]
\\
&\eta\text{-conversion: } (\lambda x. (f\ x)) \equiv f \text{ when $x$ is not free in $f$}
\end{align*}
#+end_figure

On the other hand, the induction and elimination rules for product types looks exactly like logical conjunction. Previously, we saw that conjunction in intuitionistic logic encodes the availability of proofs of both of its constituents, and this notion is made explicit as the product involves storing both terms. The elimination rules with \beta-reduction say that either of the two original terms may be recovered, and the reductions correspond to proof reduction of intuitionistic conjunction. The \eta-conversion for product types fulfills the same role as the one for function types \mdash we see that every object of a product type is equivalent to a term constructed with the $\Intro{\times}$ rule. The rules and equations are listed in Figure [[fig:stlc_prod]].

#+name: fig:stlc_prod
#+caption: Rules and equations of the product type
#+begin_figure
$$
\prftree[r]{$\Intro{\times}$}
 {\Gamma \vdash s: A}
 {\Gamma \vdash t: B}
 {\Gamma \vdash \tuple{s}{t}: (A \times B)}
\hspace{2em}
\prftree[r]{$\Elim{\times}_1$}
 {\Gamma \vdash s: (A \times B)}
 {\Gamma \vdash \fst{s} : A}
\hspace{2em}
\prftree[r]{$\Elim{\times}_2$}
 {\Gamma \vdash s: (A \times B)}
 {\Gamma \vdash \snd{s} : B}
$$
\begin{align*}
&\beta\text{-reduction: } \fst{\tuple{s}{t}} \equiv s
\\
&\beta\text{-reduction: } \snd{\tuple{s}{t}} \equiv t
\\
&\eta\text{-conversion: } \tuple{\fst{s}}{\snd{s}} \equiv s
\end{align*}
#+end_figure

The syntax of the terms of STLC is generated by the following grammar:
\begin{alignat*}{3}
s, t &::=\ &&x \\
&\binor &&(\lambda x. s) &&\binor (s\ t) \\
&\binor &&\tuple{s}{t} &&\binor \fst{s} \binor \snd{s}
\end{alignat*}
for $x$ ranging over variables.

The resemblance between STLC and intuitionistic logic is striking, and it has a name: the /Curry-Howard correspondence/. We can see a correspondence on three different levels.

First, the propositions from intuitionistic logic correspond to types. The judgment /$A$ true/ amounts to having an appropriate term $t$ for which we can make the typing judgment $t: A$.

Second, every logical rule in intuitionistic logic has an equivalent in STLC, and every rule has an associated syntactic construct. Consequently, the term encodes the deduction tree that led to its construction, up to commuting structural rules. In other words, programs are proofs.

Lastly, the \beta-reduction rules, which are computational in nature, correspond to proof reductions. Therefore, computation is proof reduction.

** Linear types

Given the correspondence between intuitionistic logic and the simply typed \lambda-calculus, we might wonder if there is a programming language corresponding to linear logic, and indeed there is.

In this section, we introduce a programming language called /linear \lambda-calculus/, or LLC. Its form is given by assigning terms to the logical deduction rules of linear logic.

The context of contextualized judgments in LLC is a list of type judgments, each of which is enclosed in either square brackets $\Int{\_}$, indicating an intuitionistic assumption, or angle brackets $\Lin{\_}$, indicating a linear assumption. As with STLC, the terms typed in assumptions must be variables, and each variable can appear in the context at most once.

The type system includes two axioms, one for every assumption kind, and they are used for introducing variables. The structural rules are almost identical to the ones of STLC, with the exception that they only act on intuitionistic assumptions. The axioms and structural rules are listed in Figure [[fig:llc_struct]]. The Exchange rule does not show any brackets around its assumptions, which is done to indicate that any two assumptions can be exchanged. This syntactic deviation is made in the name of not having to specify four separate exchange rules, one for each combination of an intuitionistic/linear pair.

We can see how the contraction rule allows an intuitionistic variable to be used more than once in a proof \mdash instances of two separate intuitionistic assumptions of the same type can be replaced by one variable.

#+name: fig:llc_struct
#+caption: Structural rules and identity axioms for LLC
#+begin_figure
$$
\prftree[r]{$\Lin{\text{Id}}$}
 {\Lin{x: A} \vdash x: A}
\hspace{2em}
\prftree[r]{$\Int{\text{Id}}$}
 {\Int{x: A} \vdash x: A}
$$

$$
\prftree[r]{Exchange}
 {\Gamma, x: A, y: B, \Delta \vdash s: C}
 {\Gamma, y: B, x: A, \Delta \vdash s: C}
\hspace{2em}
\prftree[r]{Weakening}
 {\Gamma \vdash s: B}
 {\Gamma, \Int{x: A} \vdash s: B}
$$

$$
\prftree[r]{Contraction}
 {\Gamma, \Int{x: A}, \Int{y: A} \vdash s: B}
 {\Gamma, \Int{z: A} \vdash s[x:=z][y:=z]: B}
$$
#+end_figure

The rules of linear functions assign terms to $\loli$ introduction and elimination, producing linear abstraction and linear application. As a consequence of $\lambda$ expression being formed strictly by binding linear variables, we can conclude that every variable bound by a $\lambda$ expression is used exactly once in its body. Therefore, the \beta-reduction is correct, meaning that the expression $t$ being substituted will end up being used exactly once. Both rules and equations for linear functions are listed in Figure [[fig:llc_fun]].

#+name: fig:llc_fun
#+caption: Rules and equations for the $\loli$ function type
#+begin_figure
$$
\prftree[r]{$\Intro{\loli}$}
 {\Gamma, \Lin{x: A} \vdash s: B}
 {\Gamma \vdash (\lambda x. s): (A \loli B)}
\hspace{2em}
\prftree[r]{$\Elim{\loli}$}
 {\Gamma \vdash f: (A \loli B)}
 {\Delta \vdash s: A}
 {\Gamma, \Delta \vdash (f\ s): B}
$$
\begin{align*}
&\beta\text{-reduction: } ((\lambda x. s)\ t) \equiv s[x:=t]
\\
&\eta\text{-conversion: } (\lambda x. (f\ x)) \equiv f \text{ when $x$ is not free in $f$}
\end{align*}
#+end_figure

The $\with$ product's terms and equations correspond to the $\times$ product from STLC. The introduction rule is used for forming a tuple of two resources, each of which references the exact same context, and the elimination rules allow the consumer to pick which component they want. The reduction equations identify components of a tuple with the objects extracted using the eliminators, and the conversion equation identifies every object of a $\with$ type with one constructed using the introduction rule. The rules and equations are listed in Figure [[fig:llc_with]].

The $\with$ product is the reason why we differentiate between "using" a variable exactly once in a program, and having the variable "appear" exactly once in a program. In the contextualized typing judgment
$$
x: A \vdash \tuple{x}{x}: (A \with A)
$$
the variable $x$ is /used/ exactly once, because a $\with$ product can only be used by extracting one of its components, even though $x$ /appears/ twice in the program.

#+name: fig:llc_with
#+caption: Rules and equations for the $\with$ product type
#+begin_figure
$$
\prftree[r]{$\Intro{\with}$}
 {\Gamma \vdash s: A}
 {\Gamma \vdash t: B}
 {\Gamma \vdash \tuple{s}{t}: (A \with B)}
\hspace{2em}
\prftree[r]{$\Elim{\with}_1$}
 {\Gamma \vdash s: (A \with B)}
 {\Gamma \vdash \fst{s}: A}
\hspace{2em}
\prftree[r]{$\Elim{\with}_2$}
 {\Gamma \vdash s: (A \with B)}
 {\Gamma \vdash \snd{s}: B}
$$
\begin{align*}
&\beta\text{-reduction: } \fst{\tuple{s}{t}} \equiv s
\\
&\beta\text{-reduction: } \snd{\tuple{s}{t}} \equiv t
\\
&\eta\text{-conversion: } \tuple{\fst{s}}{\snd{s}} \equiv s
\end{align*}
#+end_figure

A tuple of two independent resources $s$ and $t$ is an instance of a $\tens$ product, and it is written $\tenstup{x}{y}$. This syntax was chosen to indicate that the two resources exist "in parallel", in contrast to the $\with$ product. The elimination rule specifies a syntactic construction known in functional languages as /pattern matching/ \mdash if the term $s$ assumes two linear variables $x$ and $y$, then a value of the appropriate $\tens$ type can be deconstructed into its parts, and the construct binds each part to the corresponding variable. Since they are both linear variables, the \beta-reduction once again preserves the property of using the components of a $\tens$ product exactly once. The rules and equations are listed in Figure [[fig:llc_tens]].

The \eta-conversion for the $\tens$ product differs from what we have seen so far \mdash past conversion were, in some sense, direct opposites of the corresponding \beta-reductions. Where \beta-reductions allowed to remove an introduction followed by an elimination, the \eta-conversions allowed wrapping a proof into an elimination followed by an introduction. On the other hand, annotating the terms of the \eta-conversion for $\tens$ products gives the following diagram.
$$
\vcenter{\prfsummary{\Gamma \vdash t: (A \tens B)}}
\hspace{2em}\equiv\hspace{2em}
\vcenter{\prftree[r]{$\Elim{\tens}$}
 {\prfsummary{\Gamma \vdash t: (A \tens B)}}
 {\prftree[r]{$\Intro{\tens}$}
   {\prftree[r]{$\Lin{\text{Id}}$}{\Lin{x: A} \vdash x: A}}
   {\prftree[r]{$\Lin{\text{Id}}$}{\Lin{y: B} \vdash y: B}}
   {\Lin{x: A}, \Lin{y: B} \vdash \tenstup{x}{y}: (A \tens B)}}
 {\Gamma \vdash \tenscase{t}{x}{y}{\tenstup{x}{y}}: (A \tens B)}}
$$
We can see that the order of introduction/elimination is reversed for this conversion. Note, however, that this proof tree is not subject to \beta-reduction, because the introduction and elimination rules act on different instances of the connective \mdash we introduce the term $\tenstup{x}{y}$, but eliminate the term $t$. The reason for this pattern change is that the $\tens$ product has a different /polarity/. While all the previous types were negative types, the $\tens$ product is positive. Intuitively, positive types encode structure, while negative types encode behavior. Exploration of type polarity is, however, out of scope for this thesis, therefore the interested reader may consult \cite{Zeilberger2009}.

#+name: fig:llc_tens
#+caption: Rules and equations for the $\tens$ product type
#+begin_figure
$$
\prftree[r]{$\Intro{\tens}$}
 {\Gamma \vdash s: A}
 {\Delta \vdash t: B}
 {\Gamma, \Delta \vdash \tenstup{s}{t}: (A \tens B)}
\hspace{2em}
\prftree[r]{$\Elim{\tens}$}
 {\Gamma, \Lin{x: A}, \Lin{y: B} \vdash s: C}
 {\Delta \vdash t: (A \tens B)}
 {\Gamma, \Delta \vdash \tenscase{t}{x}{y}{s}: C}
$$
\begin{align*}
&\beta\text{-reduction: } \tenscase{\tenstup{s}{t}}{x}{y}{u} \equiv u[x:=s][y:=t]
\\
&\eta\text{-conversion: } \tenscase{s}{x}{y}{\tenstup{x}{y}} \equiv s
\end{align*}
#+end_figure

The $\bang$ exponential is another example of a positive type. The pattern matching term $\bangcase{t}{x}{s}$ decomposes a source $t$ into its "template" $x$, which can then be used however many times is necessary in the program $s$. The rules and equations are listed in Figure [[fig:llc_bang]].

At first sight, it might not be obvious why the \beta-reduction holds. After all, the substitution might result in $t$ being used non-linearly. Upon further inspection, we see that since $t$ was promoted to $\bang{t}$ using the $\Intro{\bang}$ rule, it can only use intuitionistic variables. Therefore, the substitution cannot break any linearity contracts.

#+name: fig:llc_bang
#+caption: Rules and equations for the $\bang$ exponential type
#+begin_figure
$$
\prftree[r]{$\Intro{\bang}$}
 {\Int{\Gamma} \vdash s: A}
 {\Int{\Gamma} \vdash \bang{s}: \bang{A}}
\hspace{2em}
\prftree[r]{$\Elim{\bang}$}
 {\Gamma, \Int{x: A} \vdash s: B}
 {\Delta \vdash t: \bang{A}}
 {\Gamma, \Delta \vdash \bangcase{t}{x}{s}}
$$
\begin{align*}
&\beta\text{-reduction: } \bangcase{\bang{t}}{x}{s} \equiv s[x:=t]
\\
&\eta\text{-conversion: } \bangcase{s}{x}{\bang{x}} \equiv s
\end{align*}
#+end_figure
\newpage

The syntax of LLC is generated by the following grammar:
\begin{alignat*}{3}
s, t &::=\ &&x \\
&\binor &&(\lambda x. s) &&\binor (s\ t) \\
&\binor &&\tuple{s}{t} &&\binor \fst{s} \binor \snd{s} \\
&\binor &&\tenstup{s}{t} &&\binor \tenscase{s}{x}{y}{t} \\
&\binor &&\bang{s} &&\binor \bangcase{s}{x}{t}
\end{alignat*}
for $x$ and $y$ ranging over variables.

Substitution of LLC terms is defined analogously to substitution of STLC terms. The rules are listed in Figure [[fig:llc_subst]], and they are defined for distinct variables $x$, $y$ and $z$, and for LLC terms $s$, $t$ and $u$. The substitution avoids variable capture \mdash if a free variable in $s$ would become bound following the substitution, then the substitution is not defined, and renaming variables must precede.

#+name: fig:llc_subst
#+caption: Substitution of LLC terms
#+begin_figure
\begin{align*}
x[x:=s] &= s \\
y[x:=s] &= y \\
(\lambda x. t)[x:=s] &= (\lambda x. t) \\
(\lambda y. t)[x:=s] &= (\lambda y. t[x:=s]) \\
(t\ u)[x:=s] &= (t[x:=s]\ u[x:=s]) \\
\tuple{t}{u}[x:=s] &= \tuple{t[x:=s]}{u[x:=s]} \\
\fst{t}[x:=s] &= \fst{t[x:=s]} \\
\snd{t}[x:=s] &= \snd{t[x:=s]} \\
\tenstup{t}{u}[x:=s] &= \tenstup{t[x:=s]}{u[x:=s]} \\
\tenscase{t}{x}{y}{u}[x:=s] &= \tenscase{t[x:=s]}{x}{y}{u} \\
\tenscase{t}{y}{x}{u}[x:=s] &= \tenscase{t[x:=s]}{y}{x}{u[x:=s]} \\
\tenscase{t}{y}{z}{u}[x:=s] &= \tenscase{t[x:=s]}{y}{z}{u[x:=s]} \\
(\bang{t})[x:=s] &= \bang{(t[x:=s])} \\
\bangcase{t}{x}{u}[x:=s] &= \bangcase{t[x:=s]}{x}{u} \\
\bangcase{t}{y}{u}[x:=s] &= \bangcase{t[x:=s]}{y}{u[x:=s]}
\end{align*}
#+end_figure

** Commuting conversions
In addition to equations describing relationships between introductions and eliminations of the same type, there are also equations relating rules of different types. These equations, called *commuting conversions*, describe valid ways of moving pattern matching terms through the program. For example, when given the program
$$
\tenscase{t}{x}{y}{(f\ {\tenstup{x}{y}})}
$$
one might feel that it ought to be equivalent to the program
$$
(f\ t)
$$
because the object $t$ is being deconstructed only to be reconstructed in the same manner later, exactly like in the \eta-conversion rule for the $\tens$ product. Alas, \eta-conversion is not applicable in this case, because the $\tens$ elimination-introduction pair is interleaved with function application. Commuting conversions give us a framework for "tunneling" pattern matching, so that the first program can be rewritten to
$$
(f\ \tenscase{t}{x}{y}{\tenstup{x}{y}})
$$
where \eta-conversion is applicable.

We describe commuting conversions using terms-with-holes à la Barber[fn:4] \cite{Barber1996}. A term-with-holes is a mathematical object defined by the grammar
\begin{alignat*}{4}
C[\_], D[\_] &::=\ &&\_ \\
&\binor &&(\lambda x. C[\_]) &&\binor (C[\_]\ s) &&\binor (s\ C[\_]) \\
&\binor &&\tuple{C[\_]}{D[\_]} &&\binor \fst{C[\_]} &&\binor \snd{C[\_]} \\
&\binor &&\tenstup{C[\_]}{s} &&\binor \tenstup{s}{C[\_]} &&\binor \tenscase{C[\_]}{x}{y}{s} \binor \tenscase{s}{x}{y}{C[\_]} \\
&\binor &&\mathrlap{\bangcase{C[\_]}{x}{s}} && &&\binor \bangcase{s}{x}{C[\_]}
\end{alignat*}
for $x$ and $y$ ranging over variables and $s$ ranging over terms of LLC.

In effect, a term-with-holes is a program constructed without the use of the $\Intro{\bang}$ rule, with a subexpression replaced by '_', called a hole. Note that a term-with-holes $C[\_]$ uses exactly one hole. Emphasis is once again on the terminology "uses", because multiple holes may appear in a term-with-holes, if it was constructed using the $\tuple{C[\_]}{D[\_]}$ rule. Next, we provide a way to fill the hole \mdash $C[t]$ is defined as the term $C[\_]$ with the hole replaced by the term $t$.

The commuting conversions are listed in Figure [[fig:comm_conv]]. The motivating example mentioned above is an application of the first commuting conversion, with the term-with-holes $C[\_]$ being equal to $(f\ \_)$, and the term $t$ being equal to $\tenstup{x}{y}$.

The requirements on the bindings of variables arise naturally when one writes down the proof trees for the terms on the two sides of the equations. When considering the first commuting conversion, the proof tree of the left side has the form
$$
\prftree[r]{$\Elim{\tens}$}
 {\prfsummary
   {\Theta, \Lin{x: A}, \Lin{y: B} \vdash u: E}
   {\Gamma, \Theta, \Lin{x: A}, \Lin{y: B} \vdash C[u]: D}}
 {}
 {\Delta \vdash t: (A \tens B)}
 {\Gamma, \Theta, \Delta \vdash \tenscase{t}{x}{y}{C[u]}: D}
$$
and the right side has the form
$$
\prfsummary
 {\prftree[r]{$\Elim{\tens}$}
   {\Theta, \Lin{x: A}, \Lin{y: B} \vdash u: E}
   {}
   {\Delta \vdash t: (A \tens B)}
   {\Theta, \Delta \vdash \tenscase{t}{x}{y}{u}: E}}
 {\Gamma, \Theta, \Delta \vdash C[\tenscase{t}{x}{y}{u}]: D}
$$
Since the first derivation contains the context $\Gamma, \Theta, \Lin{x: A}, \Lin{y: B}$, we know that neither $x$ or $y$ may appear in $\Gamma$, which contains the free variables of $C[\_]$. The second condition specifies that the $x$ and $y$ that are free in $u$ are the same $x$ and $y$ that are bound by the pattern matching in the final term. The only way to make them differ would be if the proof tree of $C[\_]$ first bound them for $u$, and then introduced them as new variables. For example, the term-with-hole
$$
(((\lambda y. (\lambda x. \_))\ x)\ y)
$$
is not eligible for the commuting conversion, because moving the pattern matching into the hole would swap the values of $x$ and $y$.

Similar reasoning is used for obtaining the conditions of the second commuting conversion.

#+name: fig:comm_conv
#+caption: Commuting conversions
#+begin_figure
\begin{align*}
\tenscase{s}{x}{y}{C[t]} &\equiv C[\tenscase{s}{x}{y}{t}] &&\text{when $x$ and $y$ are not free in $C[\_]$} \\
& &&\text{and when $C[\_]$ does not bind $x$ or $y$} \\
\bangcase{s}{x}{C[t]} &\equiv C[\bangcase{s}{x}{u}] &&\text{when $x$ is not free in $C[\_]$} \\
& &&\text{and when C[\_] does not bind $x$}
\end{align*}
#+end_figure

** Rationale for kinded assumptions
Equipped with an explicit notation for proof terms, we can see why this system distinguishes between intuitionistic assumptions and exponential types as two representations of free resources. While intuitionistic assumptions can only represent /variables/, objects of exponential types can be entire programs, containing linear variables. One might be tempted to simplify LLC by removing intuitionistic assumptions and replacing them with assumptions of exponential types, for example giving rise to the alternative rule for contraction:
$$
\prftree[r]{$\bang{\text{Contraction}}$}
 {\Gamma, x: \bang{A}, y: \bang{A} \vdash s: B}
 {\Gamma, z: \bang{A} \vdash s[x:=z][y:=z]: B}
$$
but we will see that this system breaks linearity when applying \beta-reductions.

The best way to approach this topic is with an example. Let the type $W$ represent a proposition "I have a cup of water", and the type $G$ represent "I have a liter of gas in the tank of my car". Consequently, the types $\bang{W}$ represents a water source, because it can provide an unlimited amount of cups of water, and the type $G \loli \bang{W}$ represents a procedure for obtaining a water source using a moving car. Specifically, imagine the variable $c: G$ being a car with gas in the tank, and the function $f: A \loli \bang{W}$ being the ability to drive to a neighboring city, bringing back a water fountain. Then the program $(f\ c): \bang{W}$ represents the fountain obtained by going into the other city, consuming the gas in the process.

We can derive the following program, which says that water from one such fountain can be distributed into two fountains.
$$
\prftree[r]{$\Elim{\loli}$}
 {\prftree[r]{$\Intro{\loli}$}
   {\prftree[r]{$\bang{\text{Contraction}}$}
     {\prftree[r]{$\Intro{\tens}$}
       {\prftree[r]{$\Lin{\text{Id}}$}{\Lin{x: \bang{W}} \vdash x: \bang{W}}}
       {\prftree[r]{$\Lin{\text{Id}}$}{\Lin{y: \bang{W}} \vdash y: \bang{W}}}
       {\Lin{x: \bang{W}}, \Lin{y: \bang{W}} \vdash \tenstup{x}{y}: (\bang{W} \tens \bang{W})}}
     {\Lin{z: \bang{W}} \vdash \tenstup{z}{z}: (\bang{W} \tens \bang{W})}}
   {\vdash (\lambda z. \tenstup{z}{z}): (\bang{W} \loli (\bang{W} \tens \bang{W}))}}
 {\hspace{-3em}\prfsummary{\Lin{f: (G \loli \bang{W})}, \Lin{c: G} \vdash (f\ c): \bang{W}}}
 {\Lin{f: (G \loli \bang{W})}, \Lin{c: G} \vdash (\lambda z. \tenstup{z}{z})\ (f\ c): (\bang{W} \tens \bang{W})}
$$
This program is subject to \beta-reduction, because of the sequence of $\loli$ introduction and elimination. However, reducing the program leads to the typing
$$
\Lin{f: (G \loli \bang{W})}, \Lin{c: G} \vdash \tenstup{(f\ c)}{(f\ c)}: (\bang{W} \tens \bang{W})
$$
which clearly does not produce a well-typed term, because the linear variables $f$ and $c$ are used twice. This is akin to taking two trips in the car, but only having fuel for one trip. In other words, the system would not be locally sound.

The term assignment with intuitionistic assumptions solves this problem by not allowing multiple-use variables to be bound as arguments to lambdas. Instead, the argument must be always linear, and later consumed by exponential pattern matching. That is to say, the above program is written
$$
\Lin{f: (G \loli \bang{W})}, \Lin{c: G} \vdash \bangcase{(f\ c)}{z}{\tenstup{z}{z}}: (\bang{W} \tens \bang{W})
$$
where the linear variables are correctly used exactly once.

Flavors of linear logic and their term assignments without intuitionistic assumptions exist, for example \cite{Benton1993}. In general, these variants provide term assignments for the structural rules also. This thesis presents the variant with kinded assumptions, for its closer resemblance to the rules of STLC.

** Intuitionistic embedding, revisited

Type theory extends the proof-theoretical point of view through programs encoding proofs. Since we are able to embed intuitionistic logic into linear logic, we want to also embed STLC into LLC, and this embedding needs to follow two conditions. First, it needs to agree with the Curry-Howard correspondence \mdash that is to say, embedding of types must behave the same as embedding of propositions, embedding of programs must behave the same as embedding of proofs and embedding of computations must behave the same as embedding of proof reductions. Secondly, equivalent programs in STLC must translate to equivalent programs in LLC.

The first condition is satisfied rudimentarily \mdash we define the translations of types, programs and computations by adding terms to the proof trees used in defining their logical counterparts. Then, agreement with the Curry-Howard correspondence is reached by definition.

For the intuitionistic correspondence between types and propositions, we obtain the following type embedding
\begin{align*}
\LinTrans{X} &= X \\
\LinTrans{A \times B} &= \LinTrans{A} \with \LinTrans{B} \\
\LinTrans{A \to B} &= \bang{\LinTrans{A}} \loli \LinTrans{B}
\end{align*}

For translating programs, we find the terms corresponding to translating derivation rules of intuitionistic logic.
\begin{align*}
&\LinTrans{x} = x &\mathrm{Id} \\
&\LinTrans{(\lambda x. s)} = (\lambda y. \bangcase{y}{x}{\LinTrans{s}}) \text{ for $y$ not free in $s$} &\Intro{\to} \\
&\LinTrans{(s\ t)} = (\LinTrans{s}\ \bang{\LinTrans{t}}) &\Elim{\to} \\
&\LinTrans{\tuple{s}{t}} = \tuple{\LinTrans{s}}{\LinTrans{t}} &\Intro{\times} \\
&\LinTrans{\fst{s}} = \fst{\LinTrans{s}} &\Elim{\times}_1 \\
&\LinTrans{\snd{s}} = \snd{\LinTrans{s}} &\Elim{\times}_2
\end{align*}
for $x$ ranging over variables and $s$ and $t$ ranging over well-typed terms.

Showing commutativity of equations with translation first requires commutativity of substitution with translation. We need to show that for every well-typed term $s$ from STLC, the following equation holds
$$
\LinTrans{t[x:=s]} \equiv \LinTrans{t}[x:=\LinTrans{s}]
$$
This is accomplished using structural induction over terms of STLC.

This property trivially holds for variables. For $x$ and $y$ two distinct variables, we have the following equalities
\begin{align*}
\LinTrans{x[x:=s]} &= \LinTrans{s} = x[x:=\LinTrans{s}] = \LinTrans{x}[x:=\LinTrans{s}] \\
\LinTrans{y[x:=s]} &= \LinTrans{y} = y = y[x:=\LinTrans{s}] = \LinTrans{y}[x:=\LinTrans{s}]
\end{align*}

For the induction step, we assume that the property holds for every subterm, and we produce the following equalities, for $x$ and $y$ two distinct variables and $s$ and $t$ well-typed terms in STLC
\begin{align*}
\LinTrans{(\lambda x. t)[x:=s]}
&= \LinTrans{(\lambda x. t)} \\
&= (\lambda y. \bangcase{y}{x}{\LinTrans{t}}) \\
&= (\lambda y. \bangcase{y[x:=\LinTrans{s}]}{x}{\LinTrans{t}}) \\
&= (\lambda y. \bangcase{y}{x}{\LinTrans{t}}[x:=\LinTrans{s}]) \\
&= (\lambda y. \bangcase{y}{x}{\LinTrans{t}})[x:=\LinTrans{s}] \\
&= \LinTrans{(\lambda x. t)}[x:=\LinTrans{s}] \\
\LinTrans{(\lambda y. t)[x:=s]}
&= \LinTrans{(\lambda y. t[x:=s])} \\
&= (\lambda z. \bangcase{z}{y}{\LinTrans{t[x:=s]}}) \\
&= (\lambda z. \bangcase{z}{y}{\LinTrans{t}[x:=\LinTrans{s}]}) \\
&= (\lambda z. \bangcase{z[x:=\LinTrans{s}]}{y}{\LinTrans{t}[x:=\LinTrans{s}]}) \\
&= (\lambda z. \bangcase{z}{y}{\LinTrans{t}}[x:=\LinTrans{s}]) \\
&= (\lambda z. \bangcase{z}{y}{\LinTrans{t}})[x:=\LinTrans{s}] \\
&= \LinTrans{(\lambda y. t)}[x:=\LinTrans{s}] \\
\LinTrans{(t\ u)[x:=s]}
&= \LinTrans{(t[x:=s]\ u[x:=s])} \\
&= \LinTrans{t[x:=s]}\ \bang{\LinTrans{u[x:=s]}} \\
&= \LinTrans{t}[x:=\LinTrans{s}]\ \bang{\LinTrans{u}[x:=\LinTrans{s}]} \\
&= \LinTrans{t}\ \bang{\LinTrans{u}}[x:=\LinTrans{s}] \\
&= \LinTrans{(t\ u)}[x:=\LinTrans{s}] \\
\LinTrans{\tuple{t}{u}[x:=s]}
&= \LinTrans{\tuple{t[x:=s]}{u[x:=s]}} \\
&= \tuple{\LinTrans{t[x:=s]}}{\LinTrans{u[x:=s]}} \\
&= \tuple{\LinTrans{t}[x:=\LinTrans{s}]}{\LinTrans{u}[x:=\LinTrans{s}]} \\
&= \tuple{\LinTrans{t}}{\LinTrans{u}}[x:=\LinTrans{s}] \\
&= \LinTrans{\tuple{t}{u}}[x:=\LinTrans{s}] \\
\LinTrans{\fst{t}[x:=s]}
&= \LinTrans{\fst{t[x:=s]}} \\
&= \fst{\LinTrans{t[x:=s]}} \\
&= \fst{\LinTrans{t}[x:=\LinTrans{s}]} \\
&= \fst{\LinTrans{t}}[x:=\LinTrans{s}] \\
&= \LinTrans{\fst{t}}[x:=\LinTrans{x}] \\
\LinTrans{\snd{t}[x:=s]}
&= \LinTrans{\snd{t[x:=s]}} \\
&= \snd{\LinTrans{t[x:=s]}} \\
&= \snd{\LinTrans{t}[x:=\LinTrans{s}]} \\
&= \snd{\LinTrans{t}}[x:=\LinTrans{s}] \\
&= \LinTrans{\snd{t}}[x:=\LinTrans{s}] \\
\end{align*}

Next, we need to show that equivalent STLC terms are translated to equivalent LLC terms. This is accomplished by showing that this property holds for every \beta-reduction and every \eta-conversion equation. That is, for every equation $s \equiv s'$, we need to show that $\LinTrans{s} \equiv \LinTrans{s}$.

Proofs of commutativity for the \beta-reductions are obtained by the corresponding proofs of commutativity for proof reductions.
\begin{align*}
\LinTrans{((\lambda x. s)\ t)}
&= (\LinTrans{(\lambda x. s)}\ \bang{\LinTrans{t}}) \\
&= ((\lambda y. \bangcase{y}{x}{\LinTrans{s}})\ \bang{\LinTrans{t}}) \\
&\equiv \bangcase{\bang{\LinTrans{t}}}{x}{\LinTrans{s}} \\
&\equiv \LinTrans{s}[x:=\LinTrans{t}] \\
&= \LinTrans{s[x:=t]} \\
\LinTrans{\fst{\tuple{s}{t}}}
&= \fst{\LinTrans{\tuple{s}{t}}} \\
&= \fst{\tuple{\LinTrans{s}}{\LinTrans{t}}} \\
&\equiv \LinTrans{s} \\
\LinTrans{\snd{\tuple{s}{t}}}
&= \snd{\LinTrans{\tuple{s}{t}}} \\
&= \snd{\tuple{\LinTrans{s}}{\LinTrans{t}}} \\
&\equiv \LinTrans{t} \\
\end{align*}

We have not specified what \eta-conversions correspond to in logic, so the verification is not as simple as following existing proofs, however it is still straight-forward.
\begin{align*}
\LinTrans{(\lambda x. (f\ x))}
&= (\lambda y. \bangcase{y}{x}{\LinTrans{(f\ x)}}) \\
&= (\lambda y. \bangcase{y}{x}{(\LinTrans{f}\ \bang{\LinTrans{x}})}) \\
&\equiv (\lambda y. (\LinTrans{f}\ \bangcase{y}{x}{\bang{x}})) \\
&\equiv (\lambda y. (\LinTrans{f}\ y)) \\
&\equiv \LinTrans{f} \\
\LinTrans{\tuple{\fst{s}}{\snd{s}}}
&= \tuple{\LinTrans{\fst{s}}}{\LinTrans{\snd{s}}} \\
&= \tuple{\fst{\LinTrans{s}}}{\snd{\LinTrans{s}}} \\
&\equiv \LinTrans{s}
\end{align*}

Since we covered all equations from STLC, we can conclude that equivalent programs translate to equivalent programs.

* TODO Category theory

Category theory is the study of categories, which allow for expressing many mathematical structures and properties via diagrams of arrows between abstract objects \cite{MacLane1998}. Categorical semantics, as opposed to the traditional set-theoretical interpretations, have the advantage of being defined with more general structures, not limiting oneself to sets specifically \cite[pp.129--132]{Crole1993}.

This chapter introduces only those categorical concepts that are necessary for giving categorical semantics to the linear lambda calculus. For a broader introduction to category theory, see \cite{MacLane1998} or \cite{Adamek1990}. The ultimate objective is to represent types as objects in a category, programs as arrows from a context object to a target object, and type constructors as functors.

A *category* is composed of two collections, the *objects* and the *arrows*, two operations on any arrow, the *source* and the *target*, and two properties, the *arrow composition* operator and the existence of *identity arrows*, which are the left and right identity for the composition.

The source and target operations, written $\Src$ and $\Tar$, respectively, each associates an object to an arrow \mdash an arrow $a$ with $\src{a} = X$ and $\tar{a} = Y$ is written graphically as $a: X \to Y$. The collection of arrows between objects $X$ and $Y$ is written $\hom{X}{Y}$.

The composition operator asserts that for any two arrows with matching ends, for example $a: X \to Y$ and $b: Y \to Z$, there exists an arrow $b \comp a: X \to Z$. This composition is associative, that is the following equality holds for any three arrows $a: X \to Y$, $b: Y \to Z$, $c: Z \to W$ for any four objects $X$, $Y$, $Z$ and $W$
$$
c \comp (b \comp a) = (c \comp b) \comp a
$$

The property of identity arrows asserts that every object $X$ is equipped with the identity arrow $\id_X: X \to X$. In situations where the object is apparent from context, we choose to omit the subscript and simply write $\id$. The identity arrows serve as identities for composition, which is described by the equations
\begin{align*}
\id_{\tar{a}} \comp a &= a \\
a \comp \id_{\src{a}} &= a
\end{align*}

Examples of categories include $\Set$, which is the category with all sets for objects and functions for arrows, or $\Grp$, the category of all groups and homomorphisms between them.

An important tool of category theory are commutative diagrams. A commutative diagram is a diagram for which commutativity is either assumed or proven. Such a diagram contains nodes, which represent objects, and directed edges, which represent arrows. For example, we can construct a commutative diagram which represents the property of the identity arrow being the identity of arrow composition.
#+begin_center
\begin{tikzcd}
X \arrow[r, "a"] \arrow[rd, "a"] & Y \arrow[d, "\id_Y"] \arrow[rd, "b"] & \\
& Y \arrow[r, "b"] & Z
\end{tikzcd}
#+end_center
This diagram commutes when the two triangles commute, and we can see that the left triangle expresses the equality $\id_Y \comp a = a$, and the right one $b \comp \id_Y = b$.

An arrow $a: X \to Y$ is an *isomorphism* if there is another arrow $a^{-1}: Y \to X$ such that the diagram
#+begin_center
\begin{tikzcd}
X \arrow[loop, "\id_X"', distance=2em, in=215, out=145] \arrow[r, shift left, "a"] & Y \arrow[loop, "\id_Y"', distance=2em, in=35, out=325] \arrow[l, shift left, "a^{-1}"]
\end{tikzcd}
#+end_center
commutes. Commutativity with the identity arrows is automatic, so the information provided by the diagram are the equations $a^{-1} \comp a = \id_X$ and $a \comp a^{-1} = \id_Y$. Isomorphisms are also called invertible arrows.

In the remainder of this thesis, we will only consider *locally small categories*, which have the property that for every two objects $X$ and $Y$, the collection $\hom{X}{Y}$ is a set. For example the category *Set* is locally small, because although the objects form a proper class, there is only a set of functions between any two given sets.

We introduce the notion of structure-preserving mappings between categories, called *functors*. Given two categories $\cat{C}$ and $\cat{D}$, a functor $F$ from $\cat{C}$ to $\cat{D}$, denoted $F: \cat{C} \to \cat{D}$, needs to map contents of $\cat{C}$ to contents of $\cat{D}$ while preserving the categorical structure \mdash the contents of a category are its objects and arrows, and the structure is described by the source and target assignments, arrow composition and identity arrows.

In other words, a functor $F: \cat{C} \to \cat{D}$ consists of a mapping from objects of $\cat{C}$ to objects of $\cat{D}$ and another mapping from arrows of $\cat{C}$ to arrows of $\cat{D}$, the two of which interact in such a way that for any arrow $a$ in $\cat{C}$, its image $Fa$ is an arrow in $\cat{D}$ whose source and target are the images of the source and target of $a$, graphically the image of an arrow $a: X \to Y$ is $Fa: FX \to FY$. Additionally, the identity arrows in $\cat{C}$ map to identity arrows in $\cat{D}$, so that $F\id_X = \id_{FX}$ for all objects $X$ in $\cat{C}$, and the compositions of arrows in $\cat{C}$ maps to composition of arrows in $\cat{D}$, following the equation $F(b \comp a) = Fb \comp Fa$. These laws are represented by the commutative diagrams in Figure [[fig:functor_laws]].

#+name: fig:functor_laws
#+caption: Functor laws
#+begin_figure
#+begin_center
\begin{tikzcd}
X \arrow[d, "a"] \\
Y
\end{tikzcd}
$\Rightarrow$
\begin{tikzcd}
FX \arrow[d, "Fa"] \\
FY
\end{tikzcd}
\hspace{2em}
\begin{tikzcd}
X \arrow[loop, "\id_X"', distance=2em, in=305, out=235]
\end{tikzcd}
$\Rightarrow$
\begin{tikzcd}
FX \arrow[loop, "F\id_X"', distance=2em, in=305, out=235] \arrow[loop, "\id_{FX}", distance=2em, in=55, out=125]
\end{tikzcd}
\hspace{2em}
\begin{tikzcd}
X \arrow[r, "a"] \arrow[dr, "b \comp a"'] & Y \arrow[d, "b"] \\
& Z
\end{tikzcd}
$\Rightarrow$
\begin{tikzcd}
FX \arrow[r, "Fa"] \arrow[dr, "F(b \comp a)"'] & FY \arrow[d, "Fb"] \\
& FZ
\end{tikzcd}
#+end_center
#+end_figure

Because we are interested in functors with multiple parameters, we also need to describe the concept of a *product category*. Given two categories $\cat{C}$ and $\cat{D}$, their product is the category $\cat{C} \times \cat{D}$. Objects of this category are ordered pairs $\tuple{C}{D}$, where $C$ is an object of $\cat{C}$ and $D$ is an object of $\cat{D}$, and arrows of this category are ordered pairs $\tuple{f}{g}: \tuple{C}{D} \to~\tuple{C'}{D'}$, where $f: C \to C'$ and $g: D \to D'$ are arrows of $\cat{C}$ and $\cat{D}$, respectively. Identity and composition are defined in the obvious way, the identity arrow being $\id_{\tuple{C}{D}} = \tuple{\id_C}{\id_D}$ and composition being $\tuple{f'}{g'} \comp \tuple{f}{g} = \tuple{f' \comp f}{g' \comp g}$.

A functor $F: \cat{C} \times \cat{D} \to \cat{B}$ can be regarded as a mapping with two arguments, one from $\cat{C}$ and one from $\cat{D}$. For example, we can define the Cartesian product on sets as a functor $\_ \times \_: \Set \times \Set \to \Set$. It takes a pair of two sets, $\tuple{A}{B}$, and maps it to the Cartesian product $A \times B$, which is itself a set. The action on arrows is defined element-wise. When the domain of a functor is not a binary product category, but a more general n-ary product category, then we talk about n-ary functors.

In the context of n-ary functors, we can refer to the property of being *functorial in an argument*. When saying that a functor $F: \cat{C} \times \cat{D} \to \cat{B}$ is /functorial in $\cat{C}$/, we mean that fixing an object $D$ of $\cat{D}$ gives rise to a functor $\hat{F}: \cat{C} \to \cat{B}$, which is defined by its action on objects $C$ of $\cat{C}$ by $\hat{F}C = F\tuple{C}{D}$, and by its action on arrows $a: C \to C'$ by $\hat{F}a = F\tuple{a}{\id_D}: F\tuple{C}{D} \to F\tuple{C'}{D}$. This notion is extended from binary to n-ary functors, by implying that all arguments other than the one for which we study functoriality are fixed. A functor is functorial in all its arguments, and conversely a mapping that is functorial in all its arguments is a functor.

Furthermore, we will work with mappings between functors, called *natural transformations*. Given two categories $\cat{C}$ and $\cat{D}$, and two functors $F, G: \cat{C} \to \cat{D}$, a natural transformation $\varphi: \nat{F}{G}$ consists of a family of arrows $\varphi_C: FC \to GC$ (its *components*), indexed by objects of $\cat{C}$, such that the following diagram commutes for all objects $X$ and $Y$, all arrows $f: X \to Y$ from $\cat{C}$.
#+begin_center
\begin{tikzcd}
FX \arrow[r, "Ff"] \arrow[d, "\varphi_X"] & FY \arrow[d, "\varphi_Y"] \\
GX \arrow[r, "Gf"] & GY
\end{tikzcd}
#+end_center

When all the components of a natural transformation are isomorphisms, we talk call it a *natural isomorphism*.

Similarly to functoriality in an argument, we can talk about *naturality in an argument* when n-ary functors are involved. The definition is also analogous \mdash provided two functors $F, G: \cat{C} \times \cat{D} \to \cat{B}$, a natural transformation $\varphi: \nat{F}{G}$ is /natural in $\cat{C}$/ if fixing an object $D$ in $\cat{D}$ gives rise to a natural transformation $\hat{\varphi}: \nat{F}{G}$, which is given by its components $\hat{\varphi}_C = \varphi_{\tuple{C}{D}}$. The naturality property is expressed by the following diagram, which uses functoriality of $F$ and $G$ in $\cat{C}$
#+begin_center
\begin{tikzcd}
F\tuple{C}{D} \arrow[r, "F\tuple{f}{\id_D}"] \arrow[d, "\hat{\varphi}_C"] & F\tuple{C'}{D} \arrow[d, "\hat{\varphi}_{C'}"] \\
G\tuple{C}{D} \arrow[r, "G\tuple{f}{\id_D}"] & G\tuple{C'}{D}
\end{tikzcd}
#+end_center
Naturality of natural transformations between n-ary functors is defined in the obvious way, by asserting that fixing all arguments except the one of interest yields a natural transformation. A natural transformation is natural in all its arguments, and a mapping between functors that is natural in all arguments is a natural transformation.

* References
#+begin_export latex
\renewcommand*{\refname}{\vspace*{-1em}}
\bibliographystyle{apacite}
\bibliography{ComputationalTrinitarianism}
#+end_export

* Footnotes

[fn:4] Barber and other authors call this concept "contexts", "term contexts" or "contexts-with-holes", but we prefer terms-with-holes to avoid overloading the word "context" 

[fn:3] Or /a generator of/, or /an infinite pocket of/

[fn:2] The judgment /$A$ prop/ (and subsequently /$A$ type/) is used more frequently in predicate logic and dependent type theories, which are out of scope for this thesis. The well-formed propositions of the relevant fragment can be described more easily with a simple grammar.

[fn:1] The notation is borrowed from Gentzen's other proof calculus, the sequent calculus. To prevent confusion of the two systems, we prefer the term /contextualized judgment/ to Gentzen's /sequent/.

# Local Variables:
# org-latex-classes: (("book" "\\documentclass[11pt,twoside,a4paper]{book}" ("\\chapter{%s}" . "\\chapter*{%s}") ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")))
# End:
