#+TITLE: Computational trinitarianism and Linear types
#+AUTHOR: Vojtěch Štěpančík
#+OPTIONS: toc:nil ':t

#+latex_header: \usepackage{fontspec}
#+latex_header: \usepackage{prftree}
#+latex_header: \usepackage{apacite}
#+latex_header: \usepackage{framed}

#+begin_export latex
% Introduction rule
\newcommand{\Intro}[1]{#1\mathrm{I}}
% Elimination rule
\newcommand{\Elim}[1]{#1\mathrm{E}}

% Lambda calculus
\newcommand{\stl}{\lambda^{\to}_{\ProdTypeCon}}

% Product type
\newcommand{\ProdTypeCon}{\land}
\newcommand{\ProdType}[2]{#1 \ProdTypeCon #2}
\newcommand{\ProdTypeFst}[1]{fst(#1)}
\newcommand{\ProdTypeSnd}[1]{snd(#1)}

% Tuple
\newcommand{\tuple}[2]{(#1, #2)}
#+end_export

* COMMENT Topic

Computational trinitarianism describes the intimate relationship between logic, category theory and type theory. This relationship identifies propositions of a logic with a type of a corresponding type system, and also establishes a correspondence between a proof of a proposition, a term (program) of a given type, and a generalized element of an object in a category.
A linear type system is a special kind of a substructural type system with important applications in computer science. An advantage of a linear type system resides in its ability to place constraints on the usage of (or access to) variables (resources).
The aim of the bachelor thesis is to describe linear logic as an example of a substructural logic, to construct a linear type system stemming from that logic, and to give their categorical semantics via categories with structure.
The style and presentation of the thesis will be theoretical.

* TODO Introduction

* Logic

Mathematical logic is logic treated by mathematical methods. However, such studies of different kinds of logic often use logical and deductive thinking themselves. To separate the logic observed from the logic used to make the observations, we consider them to be two separate systems. The language of the logic studied is referred to as the *object language*, while the language of the logic used for doing the observations is called the *metalanguage* \cite{Kleene1966}.

These metalanguages are then used to reason about formal composition of proofs \mdash therefore we call them *proof systems*, or *proof calculi*.

The proof system used in this paper stems from Gentzen's natural deduction \cite{Gentzen1934}. Natural deduction builds proofs on *judgements* and *propositions*.

A proposition is a formula of the object language, and a judgement is a knowable fact about a proposition. For example in traditional logic (that is to say, a /truth-oriented/ logic), one might take "It is raining today" for a proposition $A$, and a judgement is the statement /$A$ is true/. In this context, the judgement /$A$ is true/ is shortened to simply $A$ if it is evident from the usage site that a judgement is expected to appear, and not a proposition. We will later see that, without delving into the philosophy of mathematics, the exact nature of propositions and judgements depends on the object language.

The basis for the metalanguage is the *deduction rules*. A deduction rule consists of a collection of judgements, called the *premises*, and a single judgement, called the *conclusion*. To be able to refer to the rule in proofs, it is assigned a semantically significant name. Graphically, it is represented by drawing a horizontal line (the *derivation line*), placing the premises above it, the conclusion below, and writing the name of the rule to the right.

To illustrate, if we wanted to express the rule expressing that given two propositions, $A$ and $B$, and the judgements /$A$ is true/ and /$B$ is true/, one can obtain the judgement /$A \land B$ is true/, we can write it as

$$
\prftree[r]{$\Intro{\land}$}
{A}
{B}
{A \land B}
$$

\noindent where the judgements $A$, $B$ and $A \land B$ are shorthand forms of /$A$ is true/, /$B$ is true/ and /$A \land B$ is true/, respectively. The label $\Intro{\land}$ is an abbreviation for "conjunction introduction".

Gentzen used the concept of assumptions to formulate implication. If, given that /$A$ is true/, we can sequence the derivation rules in such a way that we get the judgement /$B$ is true/, we can abstract this dependency on a hypothetical $A$ into an implication. Gentzen used $[A]$ to denote the *assumption* of the judgement /$A$ is true/, and this assumption needs to be later *discharged* by abstracting it into an appropriate implication via a corresponding implication introduction. The formulation of the $\Intro{\to}$ rule can be seen in Figure [[fig:localized_hyp]] (left). The $\vdots$ stands for a sequence of derivation rules that can derive the judgement /$B$ is true/ from the judgement /$A$ is true/.

A proof in natural deduction is tree-like, with the judgement to be proven at the root, assumptions at the leaves, and derivation rules between the nodes. It is not a proper tree, because it needs to keep track of which implication introductions discharge which assumptions, so additional structure to manage backreferences is necessary.

In this notation, assumptions are /global/ to the proof. We can change the notation to be able to reason about assumptions locally, allowing us to degenerate the proof structure to a proper tree. We say that a *contextualized judgement*[fn:1] has the form $\Gamma \vdash A$, where \Gamma is a sequence of zero or more assumptions, called the *context*, and $A$ is the judgement \mdash once again, in traditional logic, this is shorthand for /$A$ is true/. An example of rewriting a proof from Gentzen's notation to the context notation is shown in Figure [[fig:localized_hyp]]. Note that the context can also be empty. Assumptions can be added to the context via a comma: $\Gamma, A$ is a new context, which includes all the assumptions from \Gamma, and the assumption $A$. This concatenation is intuitively extended to merging of two contexts, so $\Gamma, \Delta$ is a context that includes all the assumptions from \Gamma, and all the assumptions from \Delta \cite{Pfenning2004}. In this new notation, deduction rules have contextualized judgements for premises and conclusion.

#+begin_figure
#+name: fig:localized_hyp
#+caption: Gentzen's assumption notation (left) and notation for localized assumptions (right)
#+begin_framed
$$
\prftree[r]{$(\Intro{\to})_{\prfref<A>}$}
{\prfsummary
{\prfboundedassumption<A>{A}}
{B}}
{A \to B}
\hspace{1cm}
\prftree[r]{$\Intro{\to}$}
{A \vdash B}
{\vdash A \to B}
$$
#+end_framed
#+end_figure

The behaviour of the context is specified in the metalanguage, using derivation rules. These rules are called *structural rules*, and usually include Weakening, Contraction, and Exchange, which are listed in Figure [[fig:structural]]. These three rules encode semantics similar to those of a finite set.

Weakening allows one to add arbitrary assumptions to the context without invalidating the derived judgement. Contraction states that assumptions may be used multiple times. Exchange asserts that the order in which assumptions appear in the context is irrelevant.

A logic which constrains one or more of these structural rules is called *substructural* \cite{Paoli2013}.

#+begin_figure
#+name: fig:structural
#+caption: Structural rules
#+begin_framed
$$
\prftree[r]{Weakening}
{\Gamma \vdash A}
{\Gamma, B \vdash A}
\hspace{1cm}
\prftree[r]{Contraction}
{\Gamma, A, A \vdash B}
{\Gamma, A \vdash B}
$$

$$
\prftree[r]{Exchange}
{\Gamma, A, B, \Delta \vdash C}
{\Gamma, B, A, \Delta \vdash C}
$$
#+end_framed
#+end_figure

Just as there can be zero assumptions in a contextualized judgement, there can be zero premises in a deduction rule. Such rules are called *axioms*, and their conclusions are judgement that can always be made.

A proof in this updated notation is again a tree, with a contextualized judgement at the root, contextualized judgements in the inner nodes, axioms at the leaves, and derivation rules connecting the nodes.

When composing existing proofs together, we sometimes produce redundancies. Namely when a rule for introducing a connective is immediately followed by a rule for eliminating that connective, the proof can be simplified via rewriting rules called *proof-reductions*, specific to the object language.

** Intuitionistic logic

Intuitionistic logic is the logic of constructive mathematics \mdash the only axiom in the system is $A \vdash A$, in other words, any judgement can be made assuming itself. This is in contrast with classical logic, which also axiomatizes the law of excluded middle, $\vdash A \lor \lnot A$. The philosophical difference between classical and intuitionistic logic is that classical logic is content with knowing whether a formula is true or false, as these are the only options, while intuitionistic logic demands one to construct a proof of such a judgement. By rejecting the law of excluded middle, logical connectives can only be introduced via their corresponding introduction rules. Thus, to give a proof of the judgement /$A \lor \lnot A$ is true/, one would need to present either a proof of /$A$ is true/, or /$\lnot A$ is true/ \cite{Sorensen2006}.

The basic judgement that can be made about a proposition $A$ in intuitionistic logic is /$A$ is derivable/. Given that the only way to introduce a connective into a formula is via its unique introduction rule, such a judgement is only possible to be made given a proof of $A$.

The logic studied in this section is the meet-implicative fragment of propositional intuitionistic logic \mdash that is to say, we only concern ourselves with propositions created using the connectives $\land$ and $\to$. The formulas of this fragment can be described in the following Backus-Naur form:

$$
A, B ::= X \; | \; (A \to B) \; | \; (A \land B)
$$


\noindent for X ranging over atomic formulas. The rules of this fragment are given in Figure [[fig:intuit_deduct]].

#+begin_figure
#+name: fig:intuit_deduct
#+caption: Deduction rules for the meet-implicative fragment of propositional intuitionistic logic
#+begin_framed
$$
\prftree[r]{Id}
{A \vdash A}
$$

$$
\prftree[r]{Weakening}
{\Gamma \vdash A}
{\Gamma, B \vdash A}
\hspace{1cm}
\prftree[r]{Contraction}
{\Gamma, A, A \vdash B}
{\Gamma, A \vdash B}
\hspace{1cm}
$$

$$
\prftree[r]{Exchange}
{\Gamma, A, B, \Delta \vdash C}
{\Gamma, B, A, \Delta \vdash C}
$$

$$
\prftree[r]{$\Intro{\land}$}
{\Gamma \vdash A}
{}
{\Gamma \vdash B}
{\Gamma \vdash A \land B}
\hspace{1cm}
\prftree[r]{$\Elim{\land}_1$}
{\Gamma \vdash A \land B}
{\Gamma \vdash A}
\hspace{1cm}
\prftree[r]{$\Elim{\land}_2$}
{\Gamma \vdash A \land B}
{\Gamma \vdash B}
$$

$$
\prftree[r]{$\Intro{\to}$}
{\Gamma, A \vdash B}
{\Gamma \vdash A \to B}
\hspace{1cm}
\prftree[r]{$\Elim{\to}$}
{\Gamma \vdash A \to B}
{}
{\Delta \vdash A}
{\Gamma, \Delta \vdash B}
$$
#+end_framed
#+end_figure

The rules consist of the one axiom Id mentioned above, the three structural rules, Weakening, Contraction, and Exchange, and introduction and elimination rules for the two connective, $\Intro{\land}$, $\Elim{\land}_1$, $\Elim{\land}_2$, $\Intro{\to}$ and $\Elim{\to}$.

/Conjunction introduction/, labeled $\Intro{\land}$ in the deduction rules, states that given a proof of $A$ and a proof of $B$, then the two proofs combined give a proof of $A \land B$. The respective elimination rules allow one to re-extract one of the proofs of $A$ or $B$ from $A \land B$, even after they were combined. The corresponding proof reduction rule for eliminating a sequence of conjunction introduction and conjunction elimination is shown in Figure [[fig:intuit_conj_red]]. The rule for eliminating a sequence of $\Intro{\land}$ and $\Elim{\land}_2$ is not shown, as it is trivially symmetrical.

#+begin_figure
#+name: fig:intuit_conj_red
#+caption: Conjunction proof reduction
#+begin_framed
$$
\prftree[r]{$\Elim{\land}_1$}
{\prftree[r]{$\Intro{\land}$}
{\prfsummary[s]{\Gamma \vdash A}}
{}
{\prfsummary[t]{\Gamma \vdash B}}
{\Gamma \vdash A \land B}}
{\Gamma \vdash A}
\hspace{1cm}\Rightarrow\hspace{1cm}
\prfsummary[s]{\Gamma \vdash A}
$$
#+end_framed
#+end_figure

/Implication introduction/, labeled $\Intro{\to}$, once again builds on abstracting away an assumption. If a judgement can be made under an assumption, then the proof tree can be seen as a way of turning a proof of $A$ into a proof of $B$. The implication elimination is then a method for providing such a proof of $A$. The respective proof reduction is shown in Figure [[fig:intuit_impl_red]].

#+name: fig:intuit_impl_red
#+begin_figure
#+caption: Implication proof reduction
#+begin_framed
$$
\prftree[r]{$\Elim{\to}$}
{\prftree[r]{$\Intro{\to}$}
{\prfsummary[s]{\Gamma, A \vdash B}}
{\Gamma \vdash A \to B}}
{}
{\prfsummary[t]{\Delta \vdash A}}
{\Gamma, \Delta \vdash B}
\hspace{1cm}\Rightarrow\hspace{1cm}
\prfStackPremises
{\prfsummary[t]{\Delta \vdash A}}
{\prfsummary[s]{\Gamma, \Delta \vdash B}}
$$
#+end_framed
#+end_figure

** TODO Linear logic

#+begin_figure
#+name: fig:intuit_duplic
#+caption: Duplication of truth
#+begin_framed
$$
\prftree[r]{Contr}
{\prftree[r]{$\Intro{\land}$}
{\prftree[r]{Id}
{A \vdash A}}
{\prftree[r]{Id}
{A \vdash A}}
{A, A \vdash A \land A}}
{A \vdash A \land A}
$$
#+end_framed
#+end_figure

As can be seen from Figure [[fig:intuit_duplic]], intuitionistic logic has no problem with "duplicating" propositions \mdash it considers truth and proofs to be "free". Linear logic attempts to formalize a system where such mathematical objects are /not/ free, and resources that can be freely duplicated or discarded have to be annotated. This approach was chosen so that intuitionistic logic can be fully embedded in linear logic, therefore adding expressivity instead of limiting it, which would be the case if Contraction and Weakening were simply not allowed.

To this extent, the intuition behind several connectives, as well as their nomenclature, must be adjusted.

The intuitionistic implication $A \to B$ becomes linear $A \multimap B$, and it carries the idea of "consuming" A to "produce" B.

* TODO Type theory

Type theory is the study of formal systems in which terms have an associated label called /type/, and rules for constructing the terms include the description of their behavior on the types. For more information on the subject, see \cite{Thompson1991} and \cite{PerLof1980}.

More precisely, in constructive mathematics, a mathematical object is created by construction, and the type of an object is the type of construction used to create it \cite{Bauer2018}.

One such type system is the simply typed \lambda-calculus, or STLC, which extends the untyped \lambda-calculus by introducing a set of /base types/, and inductively generates all its types with the $\to$ binary type operator, where the type $A \to B$ is the type of functions from type $A$ to type $B$. A term $t$ of type $A$ is expressed as $t: A$.

The STLC recognizes three forms for its terms, very much like the untyped \lambda-calculus. These are /variables/, of the form $x: A$, where $x$ is an atom and $A$ is a type, then /abstractions/, which represent functions, and have the form $\lambda x.t: A \to B$, where $x: A$, $t: B$, and $x$ is a free variable in $t$, becoming bound by the abstraction. Finally, abstractions can be used in an /application/, which, given the terms $f: A \to B$ and $t: A$, yields the term $f(t): B$. Application forms can be further simplified by performing /\(\beta\)-reduction/, defined using term substitution as $(\lambda x.t)(s) \to t[s/x]$, where free occurrences of $x$ in $t$ are rewritten to $s$. Performing a reduction is synonymous with /evaluating/ a program.

We define an extension of the simply typed \lambda-calculus by introducing the binary product type operator $\ProdTypeCon$, producing types of the form $\ProdType{A}{B}$, which represent tuples of one object of type $A$ and one object of type $B$. We call this extension the \(\stl\)-calculus, and the construction rules are listed in Figure [[fig:type_derivation]].

#+begin_figure
#+name: fig:type_deduction
#+caption: Deduction rules for the \(\stl\)-calculus
#+begin_framed
$$
\prftree[r]{Id}
{x: A \vdash x: A}
$$

$$
\prftree[r]{Weakening}
{\Gamma \vdash t: A}
{\Gamma, x: B \vdash t: A}
\hspace{1cm}
\prftree[r]{Contraction}
{\Gamma, x: A, y: A \vdash t: B}
{\Gamma, z: A \vdash t[z/x][z/y]: B}
$$

$$
\prftree[r]{Exchange}
{\Gamma, x: A, y: B, \Delta \vdash t: C}
{\Gamma, y: B, x: A, \Delta \vdash t: C}
$$

$$
\prftree[r]{$\Intro{\land}$}
{\Gamma \vdash x: A}
{}
{\Delta \vdash y: B}
{\Gamma, \Delta \vdash \tuple{x}{y}: \ProdType{A}{B}}
\hspace{0.5cm}
\prftree[r]{$\Elim{\ProdTypeCon}_1$}
{\Gamma \vdash t: \ProdType{A}{B}}
{\Gamma \vdash \ProdTypeFst{t}: A}
\hspace{0.5cm}
\prftree[r]{$\Elim{\ProdTypeCon}_2$}
{\Gamma \vdash t: \ProdType{A}{B}}
{\Gamma \vdash \ProdTypeSnd{t}: B}
$$

$$
\prftree[r]{$\Intro{\to}$}
{\Gamma, x: A \vdash t: B}
{\Gamma \vdash \lambda x.t: A \to B}
\hspace{0.5cm}
\prftree[r]{$\Elim{\to}$}
{\Gamma \vdash f: A \to B}
{\Delta \vdash t: A}
{\Gamma, \Delta \vdash f(t): B}
$$
#+end_framed
#+end_figure

The new forms introduced are /tuples/, written as $\tuple{x}{y}: \ProdType{A}{B}$, which represent a pair of terms, and left and right /projections/, written as $\ProdTypeFst{t}: A$ and $\ProdTypeSnd{t}: B$, respectively, assuming a term $t: \ProdType{A}{B}$. This new syntax allows for more redundant forms of terms, which can be simplified using /\(\pi\)-reduction/ via the evaluation steps $\ProdTypeFst{\tuple{x}{y}} \to x$ and $\ProdTypeSnd{\tuple{x}{y}} \to y$.

The language is once again described with derivation rules, with zero or more premises above and one conclusion below the line. The context in a judgment now stands for a collection of typed variables, and contains the variables that are free in the term on the right side of the turnstile. In this way, the $\Intro{\to}$ rule can be intuitively interpreted by taking a variable $x: A$, and instead of treating it as free, we remove it from the context and bind it with an abstraction.
#+begin_export latex
\bibliography{ComputationalTrinitarianism}
\bibliographystyle{apacite}
#+end_export

* Footnotes

[fn:1] TODO: Should we use Gentzen's term /sequent/?
